<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ryosuke Yoneda&#39;s Homepage</title>
    <link>https://yonesuke.github.io/</link>
    <description>Recent content on Ryosuke Yoneda&#39;s Homepage</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Ryosuke Yoneda</copyright>
    <lastBuildDate>Sun, 18 Apr 2021 21:20:05 +0900</lastBuildDate><atom:link href="https://yonesuke.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>MNISTをMLPで推論(Julia/Flux実装)</title>
      <link>https://yonesuke.github.io/posts/mnist_mlp/</link>
      <pubDate>Mon, 23 Aug 2021 16:38:35 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/mnist_mlp/</guid>
      <description>Juliaで機械学習をするための有名なライブラリにFluxがあります。Fluxを使ってMNISTの手書き数字の推論を行ったのでその方法をまとめておきます。 コードは次のようになります。これを参考に書きました。
 パッケージ 基本的にFluxさえあれば良いです。今回はMNISTデータを用いるのでMLDatasetsというパッケージを用いてデータを読み込みます。これらのパッケージは事前にインストールしておく必要があります。JuliaのREPLやnotebook上で次を入力してください。
julia&amp;gt; import Pkg; Pkg.add([&amp;#34;Flux&amp;#34;, &amp;#34;MLDatasets&amp;#34;]) これでパッケージを読み込むことができます。
using Flux using Flux.Data: DataLoader using Flux: onehotbatch, onecold using Flux.Losses: logitcrossentropy using MLDatasets データの読み込み MNISTデータを読み込みます。
x_train, y_train = MLDatasets.MNIST.traindata(Float32) x_test, y_test = MLDatasets.MNIST.testdata(Float32) MNISTの画像はサイズ(28,28,1)になっていますが、MLPには1次元の配列として渡したいのでflattenで各データを1次元に落とします。
x_train = Flux.flatten(x_train) # 784×60000 x_test = Flux.flatten(x_test) # 784×10000 また、各画像の数字(0~9)はone-hotにしておきたいのでそちらはonehotbatchという関数で変換しておきます。
y_train = onehotbatch(y_train, 0:9) # 10×60000 y_test = onehotbatch(y_test, 0:9) # 10×10000 モデルの定義 いよいよモデルの定義です。今回は一番簡単なMLPで実装していきます。
img_size = (28,28,1) input_size = prod(img_size) # 784 nclasses = 10 # 0~9 # Define model model = Chain( Dense(input_size, 32, relu), Dense(32, nclasses) ) Dense(input_size, output_size, f)という関数$F\colon\mathbb{R}^{\mathrm{inputsize}}\to\mathbb{R}^{\mathrm{outputsize}}$は $$ F(x) = f(Wx+b) $$ になります。$f$は活性化関数です。$W,b$は内部で勝手に定義されます。デフォルトでは初期値$W,b$はGlorotの一様分布に従ってランダムに選ばれます。 また、$f$を指定しなければ活性化関数は恒等関数になります。すなわち非線形変換は行われません。 今回は活性化関数にReLU関数を用いました。 Chainは合成関数を作ります。すなわち、Chain(F,G)は$G\circ F$という関数に対応します。 今回定義したモデルは784次元の入力から10次元の出力を返します。出力の10次元の中で一番大きい要素のindexが推定される数字とします。</description>
    </item>
    
    <item>
      <title>Gram行列の固有値の数値計算</title>
      <link>https://yonesuke.github.io/posts/gram_eigen/</link>
      <pubDate>Mon, 02 Aug 2021 16:59:14 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/gram_eigen/</guid>
      <description>カーネル関数$k(\cdot,\cdot)$が与えられたとき、データ点$\{x_{i}\}_{i=1}^{n}$に対するGram行列(グラム行列)は $$ K=\begin{pmatrix}k(x_{1},x_{1}) &amp;amp; \cdots &amp;amp; k(x_{1},x_{n})\\\vdots &amp;amp; \ddots &amp;amp; \vdots\\ k(x_{n},x_{1}) &amp;amp; \cdots &amp;amp; k(x_{n},x_{n})\end{pmatrix} $$ で与えられます。色々な場面に登場するのですが、RBFカーネルからガウス過程を生成する際にその固有値計算で詰まったところがあったのでかんたんにまとめておきます。
Gram行列の性質 カーネル関数の定義は対称性$k(x,y)=k(y,x)$が成り立ってグラム行列が(半)正定値行列になることです。すなわち、任意のベクトル$\bm{c}$に対して、 $$ \bm{c}^{\top}K\bm{c}\geq0 $$ が成り立つことです。なので、カーネル関数から生成されたGram行列は常に半正定値行列になります。 半正定値性と固有値が非負であることは同値なのでGram行列の固有値は常に非負です。
ガウス過程に従う関数の生成 次にガウス過程から関数をサンプルすることを考えましょう。$f\sim\mathcal{GP}(m,k)$について、特にかんたんのために平均を$0$としておきましょう。 このとき、$\{x_{i}\}_{i=1}^{n}$上で関数$f$のベクトル$\bm{f}$は $$ \bm{f}\sim\mathcal{N}(\bm{0},K) $$ という多次元ガウス分布に従うものになります。これはGram行列を分散共分散行列とするガウス分布で、代表的なサンプル方法は$K$のコレスキー分解行列と$n$次元の独立標準正規分布に従う乱数との行列・ベクトル積を計算することで得られます。
固有値分布とガウス過程の関係 RBFカーネルからGram行列を生成し、コレスキー分解することを考えましょう。ここでRBFカーネルは $$ k(x,y)=\exp\left(-\frac{(x-y)^{2}}{2l^{2}}\right) $$ で与えられます。Julia実装をしてみると、
using LinearAlgebra xs = -2:0.02:2 K = [exp(-(x-y)^2) for x in xs, y in xs] cholesky(K) となりますが、これを実行すると、
ERROR: PosDefException: matrix is not positive definite; Cholesky factorization failed. と帰ってきて、Gram行列が正定値でないと言われてしまいます。理論ではGram行列は正定値であるはずなので、これは数値的な誤差に起因していると考えられます。 そこでGram行列の固有値分布を確認してみることにします。この際、RBFカーネルだけでなく周期カーネルとMatérnカーネルについても固有値分布と対応するガウス過程のサンプルをプロットしました。この結果が次のようになります。
 固有値分布については降順にソートしたものの絶対値をとったものをプロットしています。ガウス過程のサンプル方法については後述します。
RBFカーネルと周期カーネルの固有値分布に着目すると、指数的な減衰の後、途中で$0$を横切って負の値をとっていることが確認できます。これは固有値が非常に小さく、数値的な誤差によって負だと出力してしまったケースだと考えることができます。一方で、Matérnカーネルの固有値分布に着目すると、ベキ的な減衰が起こっており、負の値を取る等の誤差が見られるわけではありません。</description>
    </item>
    
    <item>
      <title>Laplacianの積分表現</title>
      <link>https://yonesuke.github.io/posts/laplacian_integral/</link>
      <pubDate>Fri, 30 Jul 2021 18:34:51 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/laplacian_integral/</guid>
      <description>領域$\Omega\subset\mathbb{R}^{n}$上で定義された関数$u\in C^{2}(\Omega)$についてLaplacian(ラプラシアン)は $$ \Delta u(x)=\sum_{i=1}^{n}\frac{\partial^{2}u}{\partial x_{i}^{2}}(x) $$ で表されます。このとき、$\partial B(x,r)=\{y\in\mathbb{R}^{n}\mid |x-y|=r\}$とおくと、 $$ \Delta u(x)=\lim_{r\to+0}\frac{2n}{r^{2}|\partial B(x,r)|}\int_{\partial B(x,r)}u(y)-u(x)d\sigma_{y} $$ が成り立ちます。$d\sigma_{y}$は$\partial B(x,r)$上の面積要素です。 この表現を得るには$u(y)$を$x$まわりでTaylor展開することが大事になるのですが、 その際、平均値の定理によって得られるTaylor展開だと剰余項の評価が難しくなります。 積分型のTaylor展開を用いることでこの問題を解決することができます。
積分型のTaylor展開 はじめに多重指数$\alpha\in\mathbb{N}_{\geq 0}^{n}$を導入します。 $$ |\alpha|=\alpha_{1}+\cdots+\alpha_{n},\quad x^{\alpha}=x_{1}^{\alpha_{1}}\cdots x_{n}^{\alpha_{n}} $$ とし、多重指数による微分を $$ D^{\alpha}f=\frac{\partial^{|\alpha|}f}{\partial x_{1}^{\alpha_{1}}\cdots\partial x_{n}^{\alpha_{n}}} $$ とします。 このとき、関数$f$が$C^{k+1}$級であるとき、点$x$まわりの積分型のTaylor展開は $$\begin{aligned}&amp;amp;f(y)=\sum_{|\alpha|\leq k}\frac{D^{\alpha}f(x)}{\alpha!}(y-x)^{\alpha}+\sum_{|\beta|=k+1}R_{\beta}(y)(y-x)^{\beta},\\&amp;amp;R_{\beta}(y)=\frac{k+1}{\beta!}\int_{0}^{1}(1-t)^{k}D^{\beta}f(x+t(y-x))dt\end{aligned}$$ で与えられます。このように剰余項$R_{\beta}$が積分の形で明示的に与えられるのが特徴です。 授業ではよく平均値の定理を用いた証明を習うと思うのですが、意外にこの積分型のTaylor展開が役に立つことがあるので覚えておいて損はないと思います。
証明 $u$を$x$周りで展開しましょう。$u\in C^{2}(\Omega)$なので、 $$\begin{aligned}u(y)-u(x)=&amp;amp;\sum_{i=1}^{n}\frac{\partial u}{\partial x_{i}}(x)(y_{i}-x_{i})+2\sum_{i\ne j}(y_{i}-x_{i})(y_{j}-x_{j})\int_{0}^{1}(1-t)\frac{\partial^{2}u}{\partial x_{i}\partial x_{j}}(x+t(y-x))dt\\&amp;amp;+\sum_{i=1}^{n}(y_{i}-x_{i})^{2}\int_{0}^{1}(1-t)\frac{\partial^{2}u}{\partial x_{i}^{2}}(x+t(y-x))dt\end{aligned}$$ です。これを積分すると、 $$\begin{aligned}\int_{\partial B(x,r)}u(y)-u(x)d\sigma_{y}=&amp;amp;\sum_{i=1}^{n}\frac{\partial u}{\partial x_{i}}(x)\int_{\partial B(x,r)}y_{i}-x_{i}d\sigma_{y}\\&amp;amp;+2\sum_{i\ne j}\int_{\partial B(x,r)}(y_{i}-x_{i})(y_{j}-x_{j})\left[\int_{0}^{1}(1-t)\frac{\partial^{2}u}{\partial x_{i}\partial x_{j}}(x+t(y-x))dt\right]d\sigma_{y}\\&amp;amp;+\sum_{i=1}^{n}\int_{\partial B(x,r)}(y_{i}-x_{i})^{2}\left[\int_{0}^{1}(1-t)\frac{\partial^{2}u}{\partial x_{i}^{2}}(x+t(y-x))dt\right]d\sigma_{y}\end{aligned}$$ となります。 まず、$y_{i}-x_{i}$は$\partial B(x,r)$の奇関数なのでこの積分は$0$となります。 また、$y-x=rz,\ d\sigma_{y}=r^{n-1}d\sigma_{z}$と変数変換すると、$|\partial B(x,r)|=r^{n-1}|\partial B(0,1)|$より、 $$ \begin{aligned}&amp;amp;\frac{2n}{r^{2}|\partial B(x,r)|}\int_{\partial B(x,r)}(y_{i}-x_{i})(y_{j}-x_{j})\left[\int_{0}^{1}(1-t)\frac{\partial^{2}u}{\partial x_{i}\partial x_{j}}(x+t(y-x))\right]d\sigma_{y}\\=&amp;amp;\frac{2n}{|\partial B(0,1)|}\int_{\partial B(0,1)}z_{i}z_{j}\left[\int_{0}^{1}(1-t)\frac{\partial^{2}u}{\partial x_{i}\partial x_{j}}(x+trz)\right]d\sigma_{z}\\\to&amp;amp;\frac{2n}{|\partial B(0,1)|}\int_{\partial B(0,1)}z_{i}z_{j}\left[\int_{0}^{1}(1-t)\frac{\partial^{2}u}{\partial x_{i}\partial x_{j}}(x)\right]d\sigma_{z}=\frac{n}{|\partial B(0,1)|}\frac{\partial^{2}u}{\partial x_{i}\partial x_{j}}(x)\int_{\partial B(0,1)}z_{i}z_{j}d\sigma_{z}\end{aligned} $$ と$r\to+0$の極限で求まります。ただし極限操作の交換は優収束定理から正当化されます。 さらに、$i\ne j$ならば$z_{i}z_{j}$の積分は$0$になり、$i=j$ならば、$z_{i}^{2}$の積分は$i$に依存しないので、 $$ \frac{n}{|\partial B(0,1)|}\frac{\partial^{2}u}{\partial x_{i}^{2}}(x)\int_{\partial B(0,1)}z_{i}^{2}d\sigma_{z}=\frac{1}{|\partial B(0,1)|}\frac{\partial^{2}u}{\partial x_{i}^{2}}(x)\int_{\partial B(0,1)}\sum_{i=1}^{n}z_{i}^{2}d\sigma_{z}=\frac{\partial^{2}u}{\partial x_{i}^{2}}(x) $$ と計算できます。よって、 $$ \frac{2n}{r^{2}|\partial B(x,r)|}\int_{\partial B(x,r)}u(y)-u(x)d\sigma_{y}\to\sum_{i=1}^{n}\frac{\partial^{2}u}{\partial x_{i}^{2}}(x)=\Delta u(x) $$ となり、示されました。</description>
    </item>
    
    <item>
      <title>Borwein integral</title>
      <link>https://yonesuke.github.io/posts/borwein/</link>
      <pubDate>Tue, 13 Jul 2021 02:20:20 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/borwein/</guid>
      <description>Borwein積分は$\sin x/x$に関する興味深い性質を持った積分のことです。 例えば $$ \int_{0}^{\infty}\frac{\sin x}{x}dx=\frac{\pi}{2} $$ となることはよく知られていますが、これに$\sin(3x)/3x$をかけたものについても $$ \int_{0}^{\infty}\frac{\sin x}{x}\frac{\sin (x/3)}{x/3}dx=\frac{\pi}{2} $$ が成り立ちます。同様のことは$\sin (x/5)/(x/5)$や$\sin(x/7)/(x/7)$をかけていっても成り立ち、 $$ \int_{0}^{\infty}\frac{\sin x}{x}\frac{\sin (x/3)}{x/3}\cdots\frac{\sin (x/13)}{x/13}dx=\frac{\pi}{2} $$ となります。しかし、次のステップではこの計算は崩れて $$ \int_{0}^{\infty}\frac{\sin x}{x}\frac{\sin (x/3)}{x/3}\cdots\frac{\sin (x/15)}{x/15}dx=\frac{467807924713440738696537864469}{935615849440640907310521750000}\pi&amp;lt;\frac{\pi}{2} $$ となってしまいます。一見するとこの値も$\pi/2$になりそうなのですが、何故か値がずれてしまいます。 このような積分のことをBorwein積分とよび、いくつかの計算がなされています。
公式 数列$a_{1},a_{2},\dots,a_{n}$に対して、Borwein積分$\tau_{n}$を $$ \tau_{n}=\int_{0}^{\infty}\prod_{k=1}^{n}\frac{\sin(a_{k}x)}{a_{k}x}dx $$ で定めます。ここでは各$a_{i}$は正だとしておきます。この積分は完全に計算することができて、 $$ \tau_{n}=\frac{\pi}{2^{n+1}(n-1)!\prod_{k=1}^{n}a_{k}}\sum_{p_{1},\dots,p_{n}=\pm1}p_{1}\cdots p_{n}\mathrm{sgn}\left(\bm{p}\cdot\bm{a}\right)(\bm{p}\cdot\bm{a})^{n-1} $$ となります。以下でこれを示していきます。元論文ではFourier変換による畳込み積分を用いて計算していますが、留数定理を用いた計算でも示すことができたのでそれを紹介していきます。
証明 はじめに$\sin(a_{k}x)$を展開していきます。 $$ \tau_{n}=\int_{0}^{\infty}\prod_{k=1}^{n}\frac{\sin(a_{k}x)}{a_{k}x}dx =\frac{1}{(2i)^{n}\prod_{k=1}^{n}a_{k}}\int_{0}^{\infty}\frac{\prod_{k=1}^{n}(e^{ia_{k}x}-e^{-ia_{k}x})}{x^{n}}dx $$ です。ここでパラメーター$p_{k}=\pm1$を導入して$e^{ia_{k}x}-e^{-ia_{k}x}=\sum_{p_{k}=\pm1}p_{k}e^{ip_{k}a_{k}x}$と書くと、 $$ \prod_{k=1}^{n}(e^{ia_{k}x}-e^{-ia_{k}x})=\sum_{p_{1},\dots,p_{n}=\pm1}p_{1}\cdots p_{n}e^{i\bm{p}\cdot\bm{a}x} $$ となるので、 $$ \tau_{n}=\frac{1}{(2i)^{n}\prod_{k=1}^{n}a_{k}}\sum_{p_{1},\dots,p_{n}=\pm1}p_{1}\cdots p_{n}\int_{0}^{\infty}\frac{e^{i\bm{p}\cdot\bm{a}x}}{x^{n}}dx $$ となります。
よって、$\int_{0}^{\infty}e^{iax}/x^{n}dx$が計算できれば良いです。 $a&amp;gt;0$の場合を考えます。 被積分関数を複素平面に持ち上げて $$ f(z)=\frac{e^{iaz}}{z^{n}} $$ とします。このとき、次のような積分経路$C$を考えます。
 $\gamma_{\varepsilon}$は半径$\varepsilon$の円の上半面を時計回りに回る経路で、$\Gamma_{R}$は半径$R$の上半面を反時計回りに回る経路です。 $f(z)$は$C$内で正則なので$\oint_{C}f(z)dz=0$です。 また、 $$ \oint_{C}=\int_{-R}^{-\varepsilon}+\int_{\gamma_{\varepsilon}}+\int_{\varepsilon}^{R}+\int_{\Gamma_{R}} $$ です。</description>
    </item>
    
    <item>
      <title>特異値分解</title>
      <link>https://yonesuke.github.io/posts/svd/</link>
      <pubDate>Mon, 31 May 2021 16:22:52 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/svd/</guid>
      <description>行列$A$を$m\times n$の実行列とします。 このときある直交行列$U\in\mathbb{R}^{m\times m},V\in\mathbb{R}^{n\times n}$が存在して、 $$U^{\mathsf{T}}AV=\Sigma=\begin{pmatrix}\mathrm{diag}(\sigma_{1},\dots,\sigma_{r}) &amp;amp; O_{r\times(n-r)} \\ O_{(m-r)\times r} &amp;amp; O_{(m-r)\times (n-r)}\end{pmatrix}\in\mathbb{R}^{m\times n}$$ となるようにできます。このような分解を特異値分解と言います。
証明 $A^{\mathsf{T}}A$は実対称行列なので固有ベクトル${v_{1},\dots,v_{n}}$と固有値${\xi_{1},\dots,\xi_{n}}$が存在して $$ A^{\mathsf{T}}Av_{i}=\xi_{i}v_{i},\quad (v_{i},v_{j})=\delta_{ij} $$ となるようにできます。 また、 $$ \xi_{i}=(v_{i},\xi_{i}v_{i})=(v_{i},A^{\mathsf{T}}Av_{i})=(Av_{i},Av_{i})\geq0 $$ なので固有値は常に0以上です。特に、$i=1,\dots,r$で$\xi_{i}&amp;gt;0$かつ$i&amp;gt;r$で$\xi_{i}=0$となるようにしておきます。 実はこの固有ベクトル$v_{i}$たちが直交行列$V$に対応します。
次に$i=1,2,\dots,r$に対して$u_{i}=Av_{i}/\sqrt{\xi_{i}}$としましょう。すると、 $$ (u_{i}, u_{j})=\frac{1}{\sqrt{\xi_{i}\xi_{j}}}(Av_{i},Av_{j})=\frac{1}{\sqrt{\xi_{i}\xi_{j}}}(v_{i},A^{\mathsf{T}}Av_{j})=\frac{\xi_{j}\delta_{ij}}{\sqrt{\xi_{i}\xi_{j}}}=\delta_{ij} $$ となり、$u_{i}$たちは互いに直交します。このとき$m-r$個のベクトル${u_{r+1},\dots,u_{m}}$を持ってきて正規直交基底${u_{1},\dots,u_{m}}$を構成することができます。 実はこのベクトル$u_{i}$たちが直交行列$U$に対応します。
上で得られたベクトルを用いて行列 $$ V=(v_{1},\dots,v_{n}),\quad U=(u_{1},\dots,u_{m}) $$ を構成します。これが直交行列なのは明らかです。 $U^{\mathsf{T}}AV$という行列を計算してみると、 $$ U^{\mathsf{T}}AV[i,j]=(u_{i},Av_{j}) $$ となります。
 $j=r+1,\dots,n$においては $$ (Av_{j},Av_{j})=(v_{j},A^{\mathsf{T}}Av_{j})=(v_{j},0)=0 $$ なので$(u_{i},Av_{j})=0$となります。 $i=r+1,\dots,m$かつ$j=1,\dots,r$においても $$ (u_{i},Av_{j})=(u_{i},\sqrt{\xi_{j}}u_{j})=0 $$ となります。 $1\leq i,j\leq r$のときには、 $$ (u_{i},Av_{j})=(u_{i},\sqrt{\xi_{j}}u_{j})=\sqrt{\xi_{j}}\delta_{ij} $$ となります。  よって、$\sqrt{\xi_{i}}=\sigma_{i}$と置くことで $$U^{\mathsf{T}}AV=\Sigma=\begin{pmatrix}\mathrm{diag}(\sigma_{1},\dots,\sigma_{r}) &amp;amp; O_{r\times(n-r)} \\ O_{(m-r)\times r} &amp;amp; O_{(m-r)\times (n-r)}\end{pmatrix}$$ となることが示されました。 特に$A=U\Sigma V^{\mathsf{T}}$もわかります。</description>
    </item>
    
    <item>
      <title>Good Will Hunting Problem</title>
      <link>https://yonesuke.github.io/posts/good-will-hunting/</link>
      <pubDate>Mon, 31 May 2021 13:01:31 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/good-will-hunting/</guid>
      <description>マット・デイモンとロビン・ウィリアムズ主演の映画『グッド・ウィル・ハンティング/旅立ち』(Good Will Hunting)の中で、MITの廊下に掲示されたグラフ理論の問題を清掃をしていたマット・デイモンが解いてしまうシーンがあります。 中学生とかのときに初めてこの映画を見たときにはよっぽど難しい問題なんだろうな、と思ったのですが、最近見返してみると定義に従って素直に計算すれば解ける問題だということがわかったのでまとめておきます。
 Given the graph $G$, find
 The adjacency matrix, $A$ The matrix giving the number of 3 step walks The generating function for walks from $i\to j$ The generating function for walks from $1\to3$    問1 グラフの隣接行列$A$の$(i,j)$成分は$i$から$j$に向かう枝があれば$1$、そうでなければ$0$となるように定義されています。 今の場合、頂点$3$と頂点$4$の間には2本枝があるのでその場合は重み付きの枝だと思って$2$とします。 そうすると隣接行列は、 $$ A= \begin{pmatrix} 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 1\\1 &amp;amp; 0 &amp;amp; 2 &amp;amp; 1\\0 &amp;amp; 2 &amp;amp; 0 &amp;amp; 0\\1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \end{pmatrix} $$ となります。</description>
    </item>
    
    <item>
      <title>至るところ微分不可能な連続関数: 初等的な構成方法</title>
      <link>https://yonesuke.github.io/posts/nowherediff/</link>
      <pubDate>Sun, 30 May 2021 09:56:32 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/nowherediff/</guid>
      <description>$[-1,1]$上の関数 $$ \varphi(x)=|x| $$ を考え、 これを$\varphi(x+2)=\varphi(x)$として$\mathbb{R}$上へ拡張します。 このとき、 $$ f(x)=\sum_{n=0}^{\infty}\left(\frac{3}{4}\right)^{n}\varphi(4^{n}x) $$ は$\mathbb{R}$上の連続関数ですが至るところ微分不可能であることが知られています。 以下でこれを示していきましょう。
連続であること 連続であることの証明はかんたんです。 $$ f_{k}(x)=\sum_{n=0}^{k}\left(\frac{3}{4}\right)^{n}\varphi(4^{n}x) $$ とおくと、$f_{k}$は連続関数です。 $$ |f(x)-f_{k}(x)|\leq\sum_{n=k+1}^{\infty}\left(\frac{3}{4}\right)^{n}=4\left(\frac{3}{4}\right)^{k+1} $$ となるので、$x$によらずに一様に抑えることができます。 これは連続関数$f_{k}$が$f$に一様収束していることを表します。 連続関数の一様収束極限は連続関数なので$f$は連続関数です。
微分不可能であること 任意の$x_{0}\in\mathbb{R}$を一つ固定しましょう。 $m\in\mathbb{N}$に対して、$\delta_{m}=\pm\frac{1}{2}4^{-m}$とおき、 符号は$4^{m}x_{0}$と$4^{m}(x_{0}+\delta_{m})$の間に整数が来ないようにします。 $$ \gamma_{n}=\frac{\varphi(4^{n}(x_{0}+\delta_{m}))-\varphi(4^{n}x_{0})}{\delta_{m}} $$ という値を計算してみましょう。
 $n&amp;gt;m$のときには$4^{n}\delta_{m}$は$2$の倍数となります。 $\varphi(x)$が周期$2$の関数であることを思い出すと、$\gamma_{n}=0$がわかります。 $n=m$のときには$4^{m}x_{0}$と$4^{m}(x_{0}+\delta_{m})$の間に整数が来ないので、 $|\gamma_{m}|=|4^{m}\delta_{m}/\delta_{m}|=4^{m}$となります。 $n &amp;lt; m$のときには、一般に$|\varphi(x)-\varphi(y)|\leq|x-y|$なので、 $|\gamma_{n}|\leq4^{n}$がわかります。  以上の準備のもと$f$が微分不可能であることを示しましょう。 具体的には、上で定義した$\delta_{m}$を用いて、$x_{0}+\delta_{m}\to x_{0}$の極限で微分が発散することを確認します。 $$ \left|\frac{f(x_{0}+\delta_{m})-f(x_{0})}{\delta_{m}}\right| =\left|\sum_{n=0}^{\infty}\left(\frac{3}{4}\right)^{n}\frac{\varphi(4^{n}(x_{0}+\delta_{m}))-\varphi(4^{n}x_{0})}{\delta_{m}}\right| =\left|\sum_{n=0}^{\infty}\left(\frac{3}{4}\right)^{n}\gamma_{n}\right| $$ となりますが、$\gamma_{n}$が$n&amp;gt;m$で消えること、また$|x+y|\geq|x|-|y|$であることを用いると、 $$\begin{aligned}\left|\sum_{n=0}^{\infty}\left(\frac{3}{4}\right)^{n}\gamma_{n}\right|=&amp;amp;\left|\sum_{n=0}^{m}\left(\frac{3}{4}\right)^{n}\gamma_{n}\right|\\=&amp;amp;\left|\left(\frac{3}{4}\right)^{m}\gamma_{m}+\sum_{n=0}^{m-1}\left(\frac{3}{4}\right)^{n}\gamma_{n}\right|\\\geq&amp;amp;\left(\frac{3}{4}\right)^{m}|\gamma_{m}|-\left|\sum_{n=0}^{m-1}\left(\frac{3}{4}\right)^{n}\gamma_{n}\right|\\\geq&amp;amp;3^{m}-\sum_{n=0}^{m-1}3^{m}=\frac{1}{2}(3^{m}+1)\to\infty\end{aligned}$$ となり、$\delta_{m}$を用いた点列の収束だと微分が発散することがわかります。 よって$f$が微分不可能であることを示すことができました。
グラフの概形 $f$自身は無限和で定義されているためプロットすることはできません。 その代わりに$f_{k}$をプロットすることにしました。 そのときのpythonファイルです。
 $ k = 0,1,3,10 $の場合の$ f _ {k} $をプロットしたものが次のようになります。
 $f_{0}$は$\varphi$にほかなりません。 $f_{1}$は$\varphi(4x)$によって引き伸ばされたものを足し込むことによって微分不可能な点が新たに増えていることがわかります。 このように$\varphi(4^{m}x)$によって微分不可能点が$\mathbb{R}$全体に伝播していって最終的に$f$が微分不可能になる様子が確認できます。</description>
    </item>
    
    <item>
      <title>坂口-蔵本モデルのダイナミクス</title>
      <link>https://yonesuke.github.io/posts/sakaguchi-kuramoto/</link>
      <pubDate>Fri, 28 May 2021 13:32:31 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/sakaguchi-kuramoto/</guid>
      <description>坂口-蔵本モデルは蔵本モデルにphase lagを導入したモデルで、次の微分方程式で表されます。 $$ \frac{d\theta_{i}}{dt}=\omega_{i}+\frac{K}{N}\sum_{j=1}^{N}\sin(\theta_{j}-\theta_{i}+\alpha),\quad i=1,\dots,N. $$ $\alpha$が位相差に対応していて、$\alpha=0$のときは蔵本モデルに戻ります。 結合強度$K$が変化したときに振動子が同期するかどうかを調べていきましょう。
自然振動数分布 各振動子は固有の角速度である$\omega_{i}$を持っています。 蔵本モデル関連の論文では$\omega_{i}$は分布に従っていると仮定しています。 この分布のことを自然振動数分布と呼びます。 分布としてはGauss分布を選ぶのかと思いきや、蔵本モデルにおいては解析のしやすさからCauchy分布が選ばれています。 $$ g(\omega)=\frac{\Delta}{\pi}\frac{1}{(\omega-\omega_{0})^{2}+\Delta^{2}} $$ $\omega_{0}$は分布の中心で、$\Delta$は分布の幅を表します。 このページでもCauchy分布を用いることにします。
 秩序変数 同期の具合をはかるパラメーターとして秩序変数(order parameter)を導入します。 $$ z=re^{i\phi}=\frac{1}{N}\sum_{j=1}^{N}e^{i\theta_{j}} $$
 $r\approx0$のときは振動子たちは円周上をバラバラに分布しているため、同期していないことがわかります。 $r=1$のときには振動子たちは円周上の1点に集中しているため、完全同期しています。  連続極限 振動子が無限個ある極限$N\to\infty$において、微分方程式は連続の式に従います。 $$ \frac{\partial f}{\partial t}+\frac{\partial}{\partial\theta}(V[f]f)=0 $$ ここで$V[f]$は速度場で微分方程式を無限に飛ばした式になります。 $$ V[f]=\omega+K\int_{\mathbb{R}}d\omega&#39;\int_{\mathbb{S}^{1}}d\theta&#39; f(\theta&#39;,\omega&#39;,t)\sin(\theta&#39;-\theta+\alpha) $$
 $f(\theta,\omega,t)$は時刻$t$における$\theta,\omega$の密度関数になります。 すなわち、$[\theta,\theta+\delta\theta)\times[\omega,\omega+\delta\omega)$に存在する振動子の割合が $f(\theta,\omega,t)\delta\theta\delta\omega$になります。 $\omega$は$g(\omega)$に従うので、 $$ \int_{\mathbb{S}^{1}}d\theta f(\theta,\omega,t)=g(\omega) $$ が成り立ちます。 連続極限において秩序変数は $$ z=re^{i\phi}=\int_{\mathbb{R}}d\omega\int_{\mathbb{S}^{1}}d\theta f(\theta,\omega,t)e^{i\theta} $$ となります。  Fourier級数展開 分布関数$f(\theta,\omega,t)$について、$\theta$方向は$\mathbb{S}^{1}$に乗っているのでFourier級数展開ができます。 $$ f(\theta,\omega,t)=\frac{1}{2\pi}\sum_{k\in\mathbb{Z}}\hat{f}_{k}(\omega,t)e^{-ik\theta} $$ これをもとに連続の式もFourier級数展開しましょう。 かんたんのために$h(\omega)$の$\omega$積分を$\langle h\rangle$と書くことにします。 $$ \begin{aligned}V[f]=&amp;amp;\omega+\frac{Ke^{-i(\theta-\alpha)}}{2i}\int_{\mathbb{R}}d\omega&#39;\int_{\mathbb{S}^{1}}d\theta&#39; f(\theta&#39;,\omega&#39;,t)e^{i\theta&#39;}-\frac{Ke^{i(\theta-\alpha)}}{2i}\int_{\mathbb{R}}d\omega&#39;\int_{\mathbb{S}^{1}}d\theta&#39; f(\theta&#39;,\omega&#39;,t)e^{-i\theta&#39;}\\=&amp;amp;\omega+\frac{Ke^{-i(\theta-\alpha)}}{2i}\langle\hat{f}_{1}\rangle-\frac{Ke^{i(\theta-\alpha)}}{2i}\langle\hat{f}_{-1}\rangle \end{aligned}$$ であるので、 $$ \widehat{V[f]}_{0}=2\pi\omega,\widehat{V[f]}_{-1}=i\pi Ke^{-i\alpha}\langle\hat{f}_{-1}\rangle,\widehat{V[f]}_{1}=-i\pi Ke^{i\alpha}\langle\hat{f}_{1}\rangle $$ がわかります。 これを連続の式に代入することで、 $$ \begin{aligned} \frac{\partial\hat{f}_{k}}{\partial t}=&amp;amp;-\widehat{\frac{\partial}{\partial\theta}(V[f]f)}_{k}=ik\widehat{V[f]f}_{k}=\frac{ik}{2\pi}\left(\sum_{l\in\mathbb{Z}}\widehat{V[f]}_{l}\hat{f}_{k-l}\right)=\frac{ik}{2\pi}\left(\sum_{l=0,\pm1}\widehat{V[f]}_{l}\hat{f}_{k-l}\right)\\=&amp;amp;ik\omega\hat{f}_{k}+\frac{kKe^{i\alpha}}{2}\langle\hat{f}_{1}\rangle\hat{f}_{k-1}-\frac{kKe^{-i\alpha}}{2}\langle\hat{f}_{-1}\rangle\hat{f}_{k+1} \end{aligned} $$ となります。特に秩序変数が$z=\langle\hat{f}_{1}\rangle$で書けるので、 $$ \frac{\partial\hat{f}_{k}}{\partial t}=ik\omega\hat{f}_{k}+\frac{kKe^{i\alpha}z}{2}\hat{f}_{k-1}-\frac{kKe^{-i\alpha}\overline{z}}{2}\hat{f}_{k+1} $$ とさらに簡略化して書くことができます。 この偏微分方程式を解くことができればダイナミクスの理解が進むのですが、 このままでは難しいです。 この難しさを乗り越えたのがOtt-Antonsen縮約と呼ばれるものがあって、この偏微分方程式の特解を表します。</description>
    </item>
    
    <item>
      <title>包除原理</title>
      <link>https://yonesuke.github.io/posts/inclusion_exclusion/</link>
      <pubDate>Sun, 16 May 2021 12:18:18 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/inclusion_exclusion/</guid>
      <description>測度空間$(X,\mathcal{B},\mu)$の有限測度集合$A_{i}(i=1,\dots,n)$に対して $$ \mu\left(\bigcup_{i=1}^{n}A_{i}\right)=\sum_{J\subset[n];J\ne\emptyset}(-1)^{|J|-1}\mu\left(\bigcap_{i\in J}A_{i}\right) $$ が成り立ちます。これを包除原理(Inclusion-exclusion principle)と呼びます。
証明 証明は $$ \int_{X}\left(1-\prod_{i=1}^{n}(1-1_{A_{i}})\right)d\mu $$ を二通りに計算することにより求まります。
はじめに愚直に展開してみることにします。 $$ \prod_{i=1}^{n}(1-x_{i})=\sum_{J\subset[n]}(-1)^{|J|}\prod_{i\in J}x_{i} $$ は展開すればわかるので、この$x_{i}$に$1_{A_{i}}$を代入すると、 $$ 1-\prod_{i=1}^{n}(1-1_{A_{i}})=1-\sum_{J\subset[n]}(-1)^{|J|}\prod_{i\in J}1_{A_{i}} =\sum_{J\subset[n];J\ne\emptyset}(-1)^{|J|-1}1_{\bigcap_{i\in J}A_{i}} $$ となります。ここで$1_{A}1_{B}=1_{A\cap B}$を用いました。以上より、 $$ \int_{X}\left(1-\prod_{i=1}^{n}(1-1_{A_{i}})\right)d\mu = \sum_{J\subset[n];J\ne\emptyset}(-1)^{|J|-1}\mu\left(\bigcap_{i\in J}A_{i}\right) $$ がわかります。これで包除原理の右辺が示されました。
次に賢い計算をしましょう。$1-1_{A}=1_{\overline{A}}$なので、 $$ 1-\prod_{i=1}^{n}(1-1_{A_{i}})=1-\prod_{i=1}^{n}1_{\overline{A_{i}}}=1-1_{\bigcap_{i=1}^{n}\overline{A_{i}}} =1_{\overline{\bigcap_{i=1}^{n}\overline{A_{i}}}} =1_{\bigcup_{i=1}^{n}A_{i}}$$ となります。最後にドモルガンの法則を用いました。 よって、 $$ \int_{X}\left(1-\prod_{i=1}^{n}(1-1_{A_{i}})\right)d\mu =\int_{X} 1_{\bigcup_{i=1}^{n}A_{i}}d\mu = \mu\left(\bigcup_{i=1}^{n}A_{i}\right) $$ となります。 これで包除原理の左辺が導かれ、包除原理が示されました。</description>
    </item>
    
    <item>
      <title>ガウス過程に関する文献</title>
      <link>https://yonesuke.github.io/posts/gp-articles/</link>
      <pubDate>Thu, 06 May 2021 12:53:12 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/gp-articles/</guid>
      <description>ガウス過程とその機械学習への応用に関する文献はたくさんあります。ここで特に有用だと思ったものを紹介していきます。
ガウス過程全般   ガウス過程と機械学習
ガウス過程と機械学習に関する和書です。非常にわかりやすい本でこの本を読めば一通りの実装はできるようになると思います。一冊目としてはこの本で間違いない気がします。
  Gaussian Processes for Machine Learning
ガウス過程と機械学習への適用に関する代表的な本だと思います。PDFはオンラインで読むことができます。
  ガウス過程の数学的な基礎に関する文献   Gaussian Processes and Kernel Methods: A Review on Connections and Equivalences
ガウス過程はカーネル法と切っても切り離せない関係にあります。この論文ではカーネル法との関係に関する結果を簡潔にまとめていて非常に参考になります。例えばRBFカーネルによって生成されるサンプルの関数が確率1で$C^{\infty}$級のなめらかな関数になることはよく知られていますが、その証明の流れもこの論文を読めばわかります。
  Reproducing Kernel Hilbert Spaces in Probability and Statistics
再生核ヒルベルト空間(RKHS)に関する本です。ガウス過程によって出力される関数が属する空間について議論する際にRKHSが出てきます。この本ではRKHSに関する議論やそのガウス過程との関係についても議論されています。この本の一番最後にカーネル関数とその対応するRKHSの具体例が27個載っていて圧巻です。
  補助変数法 データが$N$個あるときにガウス過程回帰を行うと、データ数に応じた逆行列計算が必要で$O(N^{3})$の計算量がかかってしまいます。 データ数が非常に多くなったときにはこの計算量は現実的ではないので補助変数法と呼ばれるものを用いて計算量を削減する試みが行われています。
  A Unifying View of Sparse Approximate Gaussian Process Regression
2005年に出た論文ですが、その時までに出ていた補助変数法の様々な手法をまとめた論文になっています。上で紹介した「ガウス過程と機械学習」ではこの中のFITCと呼ばれる方法が紹介されています。
  Gaussian processes for Big data
変分ベイズ法を用いてガウス過程回帰を行う手法についてまとまった論文です。補助変数の配置などを含めたハイパーパラメーターの最適化が可能になります。
  深層学習との関係   Deep Neural Networks as Gaussian Processes</description>
    </item>
    
    <item>
      <title>LaTeXの設定</title>
      <link>https://yonesuke.github.io/posts/latex-setup/</link>
      <pubDate>Wed, 05 May 2021 18:46:29 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/latex-setup/</guid>
      <description>$\LaTeX$で文章を書くときによく行う設定をまとめておきます。
Overleaf  Overleafで日本語で文章を書く場合は次のlatexmkrcというファイルをおいておきます。 $latex = &amp;#39;platex&amp;#39;; $bibtex = &amp;#39;pbibtex&amp;#39;; $dvipdf = &amp;#39;dvipdfmx %O -o %D %S&amp;#39;; $makeindex = &amp;#39;mendex -U %O -o %D %S&amp;#39;; $pdf_mode = 3; さらにCompilerはLaTeXにしておきましょう。
  Beamer  LaTeXでスライド生成する際に使われるbeamerです。テーマがたくさんあるので好みのものを選ぶと良いと思います。特にmetropolisとfocusというテーマが気に入っています。 \documentclass[dvipdfmx]{beamer} \usepackage{bxdpx-beamer} \usepackage{pxjahyper} \usepackage{minijs} \usetheme[numbering=fraction,block=fill,progressbar=frametitle]{metropolis} \usefonttheme{professionalfonts} \usepackage{appendixnumberbeamer} Appendixに移行する際の区切りのページには次を書くと便利です。
% スライド終わり \appendix \begin{frame}[standout] APPENDIX \end{frame} % アペンディクスはじまり % \begin{frame} % ...   便利Package   hyperref 次のオプションとともに呼ぶと便利です。
\usepackage[colorlinks=true,linkcolor=magenta,citecolor=magenta,breaklinks=true]{hyperref} 特にbreaklinks=trueはリンクを改行してくれるので重宝します。リンクの色は好みに合わせて変えてください。
  biblatex 次のオプションとともに呼ぶとPR系の論文のように参考文献を表示してくれます。
\usepackage[style=phys,articletitle=true,biblabel=brackets,chaptertitle=false,pageranges=false,doi=false]{biblatex} % bibファイルの読み込み \addbibresource{main.</description>
    </item>
    
    <item>
      <title>安定分布に従うノイズの生成方法</title>
      <link>https://yonesuke.github.io/posts/stable_simulations/</link>
      <pubDate>Wed, 21 Apr 2021 20:58:21 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/stable_simulations/</guid>
      <description>安定分布に従うノイズの生成方法について簡単にまとめておきます。 ここでは代表的なWeronの方法を用います。
Weronの方法 はじめに区間$(-\pi/2,\pi/2)$上の一様分布から乱数$V$を、平均$1$の指数分布に従う乱数$W$をそれぞれ生成します。
  $\alpha\ne1$の場合、 $$ \begin{aligned} X=S_{\alpha,\beta}\times\frac{\sin(\alpha(V+B_{\alpha,\beta}))}{(\cos V)^{1/\alpha}}\times\left(\frac{\cos(V-\alpha(V+B_{\alpha,\beta}))}{W}\right)^{(1-\alpha)/\alpha} \end{aligned} $$ を計算します。ここで$B_{\alpha,\beta},S_{\alpha,\beta}$はそれぞれ、 $$\begin{aligned}B_{\alpha,\beta}&amp;amp;=\frac{\arctan(\beta\tan(\pi\alpha/2))}{\alpha},\\S_{\alpha,\beta}&amp;amp;=\left(1+\beta^{2}\tan^{2}(\pi\alpha/2)\right)^{1/(2\alpha)}\end{aligned}$$ です。
  $\alpha=1$の場合には、 $$ X = \frac{2}{\pi}\left[(\frac{\pi}{2}+\beta V)\tan V - \beta\log\left(\frac{\frac{\pi}{2}W\cos V}{\frac{\pi}{2}+\beta V}\right)\right] $$ を計算します。
  これより、$X\sim S(\alpha,\beta,1,0)$なる安定分布ノイズが得られます。 より一般に$S(\alpha,\beta,\gamma,\delta)$に従うノイズがほしければ$\gamma X+\delta$とすれば良いです。
補足 $\beta=0$の場合には$S(\alpha,0,1,0)$に従うノイズの生成式は一つにまとめられて、 $$ X=\frac{\sin(\alpha V)}{(\cos V)^{1/\alpha}}\times\left(\frac{\cos((1-\alpha)V)}{W}\right)^{(1-\alpha)/\alpha} $$ とすればよいです。
この式において更に$\alpha=1$にすれば$X=\tan V$となり、これは平均$0$、$\gamma=1$のCauchy分布に従うノイズの生成方法に一致します。
また、$\alpha=2$の場合には$P,Q\sim U(0,1)$として $$ X=-2\cos\pi P\sqrt{-\log Q} $$ となり、これは平均$0$、分散$2$の正規分布に従うノイズの生成方法であるBox–Muller法に一致します。</description>
    </item>
    
    <item>
      <title>多変量正規分布間のKL距離</title>
      <link>https://yonesuke.github.io/posts/kl-gaussian/</link>
      <pubDate>Mon, 19 Apr 2021 00:32:17 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/kl-gaussian/</guid>
      <description>確率分布の間の&amp;quot;近さ&amp;quot;を測る代表的なものとしてKL距離(Kullback–Leibler divergence)があります。特に多変量正規分布間のKL距離は変分下界を計算する際に登場することもあったりして応用上も重要です。その導出を行います。
準備 平均$\mu$、分散共分散行列$\Sigma$の$N$次元正規分布$\mathcal{N}(u\mid\mu,\Sigma)$の確率密度関数は $$ p(u)=\frac{1}{\sqrt{2\pi}^{N}\sqrt{| \Sigma |}}\exp\left[-\frac{1}{2}(u-\mu)^{\mathsf{T}}\Sigma^{-1}(u-\mu)\right] $$ で与えられます。 また、確率分布$q$に対する確率分布$p$のKL距離は $$ \text{KL}[p || q]=\int p(u)\log\frac{p(u)}{q(u)}du $$ で定義されます。
導出 2つの$N$次元正規分布$p(u)=\mathcal{N}(u \mid \mu _ {1},\Sigma _ {1}),q(u)=\mathcal{N}(u \mid \mu _ {2},\Sigma _ {2})$に対して$\text{KL}[p || q]$を計算します。 $\log$部分を展開すると、 $$ \log\frac{p(u)}{q(u)}=\frac{1}{2}\log\frac{|\Sigma _ {2}|}{|\Sigma _ {1}|} -\frac{1}{2}(u-\mu _ {1})^{\mathsf{T}}\Sigma _ {1}^{-1}(u-\mu _ {1}) + \frac{1}{2}(u-\mu _ {2})^{\mathsf{T}}\Sigma _ {2}^{-1}(u-\mu _ {2}) $$ と3つにわかれるので $$ \begin{aligned} \text{KL}[p || q] = &amp;amp;\frac{1}{2}\log\frac{|\Sigma _ {2}|}{|\Sigma _ {1}|}\int_{\mathbb{R}^{N}}p(u)du \\ &amp;amp; - \frac{1}{2}\int_{\mathbb{R}^{N}}(u-\mu _ {1})^{\mathsf{T}}\Sigma _ {1}^{-1}(u-\mu _ {1})p(u)du \\ &amp;amp; + \frac{1}{2}\int_{\mathbb{R}^{N}}(u-\mu _ {2})^{\mathsf{T}}\Sigma _ {2}^{-1}(u-\mu _ {2}) p(u)du \end{aligned} $$ とできます。各行を計算していきます。</description>
    </item>
    
    <item>
      <title>博士課程1年目を振り返る</title>
      <link>https://yonesuke.github.io/posts/d1/</link>
      <pubDate>Mon, 22 Mar 2021 00:14:18 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/d1/</guid>
      <description>2020年の4月から京都大学大学院情報学研究科に博士課程として入学してから1年が経ちました。 ここに博士課程1年目に起こったことをまとめておきます。 将来博士課程に進学する誰かの参考になれば良いなと思います。
僕は京都大学大学院情報学研究科先端数理科学専攻の非線形物理学講座という研究室に所属しています。 修士までは同じ情報学研究科の数理工学専攻の研究室に所属していて、博士課程から移ってきました。 主にリズム現象に関する研究を行っています。 特にリズム現象を記述する結合振動子系が示す同期転移を、力学系の手法を用いて理論解析をしたり、統計力学の手法を用いて数値計算をしたりしてきました。 最近は流行りに乗っかって機械学習の勉強も少ししようと思っています。
4月 新しい研究室に所属して頑張って研究をするぞ！！と思っていた矢先に新型コロナウイルスの感染拡大とそれに伴う緊急事態宣言が発令されました。 そのため、ずっと家に篭りっきりでした(みなさんもそうだったと思いますが)。 研究室のセミナー等はすべてzoomで行われることになりました。 これまで研究室で研究するタイプの人間で、家では勉強していなかったので、家で勉強とか研究をするのに慣れなくてあまり集中できませんでしたね。。。 あとしょぼい椅子に長時間座ることになって体がバキバキになりました。 椅子を奮発して買ったのですがこれが良かったです。こういうものには投資していくべきだな、と気付かされました。
研究の面では修論の結果を論文にまとめる作業をずっとしていました。 あとガウス過程の勉強をはじめました。 また、TAの業務がちょくちょくありました。貴重な収入源なのでありがたいです。
5月 この頃はずっと学振でしたね、、、辛かったです。 申請書を書くのがまじで向いていないなあ、と実感しました。 研究者たちは科研費を取るためにああいった申請書を書き続けているのだ、と思うと頭が上がりません。
5月の終わり頃に緊急事態宣言が解除されて、この頃からちょくちょく研究室に行くようになりました。
6月 6月のはじめに学振の締め切りがあって肩の荷が下りた感じがしました。 ただ、申請書でいうと民間奨学金の申請と学振を持っていない人対象のRA雇用の申請もあって、 6月の前半も申請書で潰れていた気がします。 (本当は気合を入れたらすぐできることだったと思うんですが、 頭の片隅に申請書がずっとちらついていて研究をしていてもあまり集中ができない感じでした。)
7月 修論の結果を論文にまとめる作業もだいぶ収束してきてarxivにアップロードしました。 (arxivにアップロードするとき、texファイルやらepsファイルやらを一個ずつポチポチしないといけないのはなんとかならないんですかね。。) また、論文をPREに投稿しました。投稿してから毎日のようにstatusのところを眺めていました。
学振も論文も終わったのでずっと積んでいたデスストランディングというゲームを一気に終わらせました。 めっちゃおもろかったです。
8月 色々なことが片付いたので、研究のことを一旦忘れて勉強に全振りしていた気がします。 特に関数解析の勉強を重点的に行いました。 この時期はやるべきことも特になくて平和だった気がします。
9月 9月は日本物理学会とRIMSの力学系研究集会での発表がありました。 あと7月に投稿した論文の査読も帰ってきて8月とは一転急に忙しくなりました。
9月の終わり頃に学振の結果が帰ってきて不採用でした。 もちろん残念だったんですが、思ったほど悲観的ではなかった気がします。 落ちるだろうなあ、と思いながら結果を待つようにしていたのと、 TA・RAやJASSOや民間の奨学金のおかげで死なない程度には生きていけていたからなのかなあ、と自己分析をしていました(笑)
10月 10月からは後期の授業が始まりました。様々なことが引き続きzoomで行われていました。 8月からちょくちょく関数解析を勉強していたおかげで読める(理解できる)論文が少し増えたような気がします。勉強は大事。
この後期から京都外国語大学で非常勤講師として授業を受け持つことになりました。張り切ってnotionでホームページ的なのを作ってみました。 こちらも授業はすべてオンラインでした。学生とのやり取りにはmicrosoftのteamsというサービスが使われていて、 それでオンラインの授業も行いました。 授業をオンラインでするのは初めてだったのですが、やはり学生の顔が見えないのでみんながどれくらい理解しているのかが全然わからなくて 結構難しかったです。「手を挙げる」機能を使って適宜わからないところがないかを学生に聞いてみたりしていました。 授業の準備(授業スライド、演習問題、解説などなど)で半日から一日近く潰れていた気がします。 研究者の方々も毎年のように授業の準備をしているのだ、と思うと頭が上がりません(2回目)。
11月 9月に帰ってきた論文の直しがようやく終わり、PREに投稿し直しました。 今度はめっちゃ返事が早くて、acceptが決まりました。 よく読んでいた雑誌なので通ったのは嬉しかったです。
関数解析の勉強をしたし、neural networkの万能近似定理の証明を読みました。元論文はCybenkoによるものです。 この論文はいい感じの活性化関数を使った3層neural netoworkにおいて、中間層のunit数が無限の極限では任意の連続関数に近づけることができることを証明しています。 せっかく読んだので研究室内で発表することにしました。5人くらい(?)参加してくれて、2週に分けて関数解析の基礎から万能近似定理の証明まで話すことにしました。 駆け足で説明したのでみんながわかってくれたかは分からないです(笑)
この時期はアメリカで大統領選挙が行われて、それに関連して(関連はしてないですが)Strogatz先生がtweetしたことが目に止まりました。 If anyone wants to distract themselves while the votes are being counted, or perhaps just catch up on some lost sleep, have a look at this lecture I gave to students at Cambridge University a few days ago: https://t.</description>
    </item>
    
    <item>
      <title>2020年度 数的理解</title>
      <link>https://yonesuke.github.io/teaching/2020-suutekirikai/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yonesuke.github.io/teaching/2020-suutekirikai/</guid>
      <description>授業内容・計画 問題演習を通して、基礎的な数学の知識を身につける演習を行う。特に、SPI試験などを受験する際に必要な数学的知識を身につけ、問題が解けるようになることを目指す。また、問題演習を通して、数学的な考え方や論理的な考え方を身につけることも目指す。
 教科書 『最新最強のSPIクリア問題集』 成美堂出版編集部　著 (成美堂出版)
 スケジュール 水曜4限 (15:40~17:20)
    日付 授業内容 授業スライド 問題演習     第1回 10月7日 オリエンテーション Google スライド      割合 Google スライド 割合 PDF   第2回 10月14日 損益算 Google スライド 損益算 PDF   第3回 10月21日 順序、組み合わせ Google スライド 順序、組み合わせ PDF   第4回 10月28日 確率 Google スライド 確率 PDF   第5回 11月11日 方程式 Google スライド 方程式 PDF   第6回 11月25日 $n$進数 Google スライド $n$進数 PDF   第7回 12月2日 距離と時間と速さ Google スライド 距離と時間と速さ PDF   第8回 12月9日 濃度算 Google スライド 濃度算 PDF   第9回 12月16日 数列 Google スライド 数列 PDF   第10回 12月23日 関数とグラフ Google スライド 関数とグラフ PDF   第11回 1月6日 集合 Google スライド 集合 PDF   第12回 1月13日 論理 Google スライド 論理 PDF   第13回 1月20日 テスト演習と解説1  解答 PDF   第14回 1月27日 テスト演習と解説2  解答 PDF     成績評価   平常点 平常点は60点満点になります。第1回から第12回までの各授業で問題演習を提出した回数に5をかけた数が平常点になります。例えば12回すべての問題演習を提出した方の平常点は60点です。一方で12回のうち9回しか提出出来なかった方の平常点は45点です。</description>
    </item>
    
    <item>
      <title>2021年度 数的理解</title>
      <link>https://yonesuke.github.io/teaching/2021-suutekirikai/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yonesuke.github.io/teaching/2021-suutekirikai/</guid>
      <description>授業内容・計画 問題演習を通して、基礎的な数学の知識を身につける演習を行う。特に、SPI試験などを受験する際に必要な数学的知識を身につけ、問題が解けるようになることを目指す。また、問題演習を通して、数学的な考え方や論理的な考え方を身につけることも目指す。
 教科書 『最新最強のSPIクリア問題集』 成美堂出版編集部　著 (成美堂出版)
 スケジュール  水曜4限 (15:40~17:20) 教室 R156      日付 授業内容 授業スライド 問題演習     第1回 9月29日 オリエンテーション オリエンテーション PDF      割合 割合 PDF 割合 PDF   第2回 10月6日 損益算 損益算 PDF 損益算 PDF   第3回 10月13日 順序、組み合わせ 順列、組み合わせ PDF 順序、組み合わせ PDF   第4回 10月20日 確率 確率 PDF 確率 PDF   第5回 10月27日 方程式 方程式 PDF    第6回 11月10日 $n$進数     第7回 11月17日 距離と時間と速さ     第8回 12月1日 濃度算     第9回 12月8日 数列     第10回 12月15日 関数とグラフ     第11回 12月22日 集合     第12回 1月5日 論理     第13回 1月12日 テスト演習と解説1     第14回 1月19日 テスト演習と解説2       成績評価   平常点 平常点は60点満点になります。第1回から第12回までの各授業で問題演習を提出した回数に5をかけた数が平常点になります。例えば12回すべての問題演習を提出した方の平常点は60点です。一方で12回のうち9回しか提出出来なかった方の平常点は45点です。</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://yonesuke.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yonesuke.github.io/about/</guid>
      <description>Education   Ph.D. student April 2020 -
Nonlinear Physics Division, Department of Applied Mathematical Sciences, Graduate School of Informatics, Kyoto University.
Supervised by Toshio Aoyagi
  M.S. April 2018 - March 2020
Dynamical Systems Group, Department of Applied Mathematics and Physics, Graduate School of Informatics, Kyoto University.
Supervised by Yoshiyuki Y. Yamaguchi
  B.S. April 2014 - March 2018
Department of Informatics and Mathematical Science, Faculty of Engineering, Kyoto University.</description>
    </item>
    
    <item>
      <title>Research</title>
      <link>https://yonesuke.github.io/research/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yonesuke.github.io/research/</guid>
      <description>Keywords Synchronization, Kuramoto model, Critical phenomena, Gaussian process
Publications Google Scholar
 R. Yoneda, T. Tatsukawa, and J. Teramae, The lower bound of the network connectivity guaranteeing in-phase synchronization, Chaos 31, 063124 (2021), arXiv: 2104.05954. R. Yoneda, K. Harada, and Y. Y. Yamaguchi, Critical exponents in coupled phase-oscillator models on small-world networks, Phys. Rev. E 102, 062212 (2020), arXiv: 2007.04539. R. Yoneda and Y. Y. Yamaguchi, Classification of bifurcation diagrams in coupled phase-oscillator models with asymmetric natural frequency distributions, J.</description>
    </item>
    
    <item>
      <title>Teaching</title>
      <link>https://yonesuke.github.io/teaching/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yonesuke.github.io/teaching/</guid>
      <description>2021  2021年10月-2022年2月, 数的理解(短) (京都外国語大学・非常勤講師)  ホームページ    2020  2020年10月-2021年2月, 数的理解(短) (京都外国語大学・非常勤講師)  ホームページ ここに作り直しました    Teaching Assistant 2021  2021年4月-8月, 物理学基礎論A (京都大学工学部) 2021年4月-8月, 非線形動力学 (京都大学工学部)  2020  2020年10月-2021年2月, 解析力学 (京都大学工学部) 2020年4月-8月, 基礎数理演習 (京都大学工学部) 2020年4月-8月, 非線形動力学 (京都大学工学部)  2019  2019年10月-2020年2月, 工業数学A1 (京都大学工学部) 2019年4月-8月, 基礎数理演習 (京都大学工学部)  2018   2018年10月-2019年3月, 微分積分学続論II (京都大学工学部)
  2018年4月-8月, 基礎数理演習 (京都大学工学部)
  2018年4月-8月, 工業数学A3 (京都大学工学部)
  </description>
    </item>
    
  </channel>
</rss>
