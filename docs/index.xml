<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Ryosuke Yoneda</title>
    <link>https://yonesuke.github.io/</link>
    <description>Recent content on Ryosuke Yoneda</description>
    <image>
      <url>https://yonesuke.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://yonesuke.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 20 Jul 2022 16:38:49 +0900</lastBuildDate><atom:link href="https://yonesuke.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ランダムグラフ上の最小全域木</title>
      <link>https://yonesuke.github.io/posts/random_mst/</link>
      <pubDate>Wed, 20 Jul 2022 16:38:49 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/random_mst/</guid>
      <description>集中不等式に関する勉強をしている中でランダムグラフの最小全域木の重み和の期待値が頂点数無限の極限で$\zeta(3)$に収束するという驚異的な定理を目にしました。 今回はその定理をかんたんに紹介したいと思います。
Info
Hugo上で定義・定理・証明環境を整えるのにこのページが参考になりました。ありがとうございます。 また、このinfoはこの拡張機能を用いました。
Minimum Spanning Tree はじめに最小全域木についてまとめておきます。まず、Wikipediaによると全域木は Definition &amp;nbsp; &amp;nbsp;[全域木] グラフ$G(V,E)$において$T\subseteq E$となる辺集合$T$があるとき、グラフ$S(V,T)$が木（閉路を持たないグラフ）であるなら、$S(V,T)$のことをグラフ$G(V,E)$の全域木であるとする。 で定義されます。 例えば左端のグラフに対して、真ん中と右端のグラフ(実線が選ばれた枝に対応しています)は全域木になっています。この例からわかるように全域木は一意ではありません。
グラフの各辺に重みが与えられているときに、最小の重み和となる全域木を最小全域木と言います。 左のグラフのように辺重みが与えられているとき、最小全域木は右のようになります。また、そのときの重み和は$2+3+1+3=9$になります。
Random Minimum Spanning Tree 今、各頂点が他のすべての頂点と繋がっている完全グラフを考えます。また、それぞれの枝重みは独立に$[0,1]$区間からランダムに取ることにします。 このように構成されたグラフに対して最小全域木の重み和を求めます。この重み和は頂点数が無限の極限で期待値としてどのような値を取るのでしょうか？極限では重み和は発散してしまう可能性もあります。
頂点数が$n$のときの最小全域木の重み和を$T_{n}$という確率変数で表すことにすると、実はその期待値の極限は次のように表せることが知られています。 Theorem &amp;nbsp; &amp;nbsp;[Alan M. Frieze 1985] $$ \lim_{n\to\infty}\mathbb{E}[T_{n}]=\zeta(3) $$ ここで、$\zeta(s)=\sum_{n=1}^{\infty}n^{-s}$は$\Re s&amp;gt;1$で定義されるリーマンゼータ関数です。
Numerical Simulation Conjecture &amp;nbsp; &amp;nbsp;[Riemann Hypothesis] This can be sometimes useful Proof [Of Theorem 1]: Insert your proof here
gnmatiohnioanhaet@qio
$\frac{1}{3}$</description>
    </item>
    
    <item>
      <title>M1 MacでのPython環境構築(tensorflowとか)</title>
      <link>https://yonesuke.github.io/posts/m1-mac-python/</link>
      <pubDate>Mon, 11 Jul 2022 10:26:10 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/m1-mac-python/</guid>
      <description>Apple Siliconが搭載されたMacが手元に何台かあって、その上で色々と研究をしていたのですが、pythonの環境構築に結構手こずってしまいました。何度か試してうまく行ったものを備忘録として記しておきます。 (ここではpyenvを用いた環境構築について記しています。もちろん他にも良い方法はあると思います。) インストールに手こずることがあれば随時この記事に書き足していきたいと思います。
特にtensorflowのインストールが難しかったのですが、以下の記事が参考になりました。ありがとうございます。
https://qiita.com/chipmunk-star/items/90931fb5180b8024adcb pyenv はじめにpyenvをインストールします。これはpyenvのGitHubページに従えばうまく動きます。
Apple Siliconではarmアーキテクチャが採用されているのでminiforgeを使わないといけない、という説明がネット上に無数にあるのでそれに従います。今ではminiforge以外も対応しているのかもしれないですが、よくわからないです。 この記事を書いている現時点(2022/07/11)ではminiforge3-4.10.3-10が最新のように見えるのでこれをインストールしてglobalの環境に設定しておきます。
1 2 pyenv install miniforge3-4.10.3-10 pyenv global miniforge3-4.10.3-10 TensorFlow tensorflowのインストールが一番大変で、いろんな記事を眺めてはインストールを試みましたが成功には至りませんでした。上のリンクに従うとすんなりとインストールが完了しました。
インストールに必要なのはtensorflow-deps, tensorflow-macos, tensorflow-metalの3つ(多い！！)らしいです。そのうちtensorflow-deps, tensorflow-macosをインストールするときのバージョンがtensorflowのバージョンに対応するらしいです。そのためにまずtensorflow-depsがインストール可能なバージョンを確認します。tensorflow-depsはcondaによるインストールを利用するらしいのでconda searchを行います。
1 2 3 4 5 6 7 8 9 -&amp;gt; % conda search -c apple tensorflow-deps Loading channels: done # Name Version Build Channel tensorflow-deps 2.5.0 0 apple tensorflow-deps 2.5.0 1 apple tensorflow-deps 2.6.0 0 apple tensorflow-deps 2.7.0 0 apple tensorflow-deps 2.8.0 0 apple tensorflow-deps 2.</description>
    </item>
    
    <item>
      <title>Kovacicのアルゴリズムを用いて調和振動子を解く</title>
      <link>https://yonesuke.github.io/posts/kovacic/</link>
      <pubDate>Thu, 09 Jun 2022 00:14:18 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/kovacic/</guid>
      <description>Kovacicのアルゴリズムは有利係数の2階線形常微分方程式を解くアルゴリズムです。与えられた微分方程式が解くことができる場合にはその解を出力し、解くことができない場合にはそうであることがわかるという非常に便利なアルゴリズムになっています。ここで言う&amp;quot;解ける&amp;quot;という言葉は微分ガロア理論の意味で用いられています。僕自身は微分ガロア理論には詳しくはないので細かいことはわかりませんが、細かいことがわからなくてもKovacicのアルゴリズムを使うことができるものになっています。
以前所属していた研究室には微分方程式の非可積分性に関する研究があり、その中でKovacicのアルゴリズムを知る機会がありました。Kovacicのアルゴリズムが適用できる微分方程式で僕が一番馴染み深かったのが調和振動子のシュレディンガー方程式でした。実際にその方程式に対してKovacicのアルゴリズムを適用するとたしかに固有エネルギーとその固有状態を得ることができたときはものすごく感動しました。物理の授業で習うような生成消滅演算子を用いた方法でなくとも、可積分なのかどうかという観点からその固有エネルギーを求めることができるのは非常に面白いと思います。というわけでここでその計算の流れを紹介したいと思います。
ここでの計算はすべて https://tetobourbaki.hatenablog.com/entry/2018/11/03/231445 とそこに添付のPDFを参考にしています。
調和振動子 調和振動子のシュレディンガー方程式は $$ i\hbar\frac{\partial}{\partial t}\phi(x,t)=\left(-\frac{\hbar^{2}}{2m}\frac{\partial^{2}}{\partial x^{2}}+\frac{m\omega^{2}}{2}x^{2}\right)\phi(x,t) $$ で与えられます。$\phi(x,t)$は波動関数です。特に時間依存しないシュレディンガー方程式は、 $$ \left(-\frac{\hbar^{2}}{2m}\frac{d^{2}}{dx^{2}}+\frac{m\omega^{2}}{2}x^{2}\right)\phi(x)=E\phi(x) $$ となります。この方程式は解くことができて、固有エネルギーと固有状態は$n\in\mathbb{Z}_ {\geq 0}$に対して $$\begin{aligned}&amp;amp;E=\left(n+\frac{1}{2}\right)\hbar\omega,\\&amp;amp;\phi(x)=H_ {n}\left(\sqrt{\frac{m\omega}{\hbar}}x\right)\exp\left(-\frac{m\omega}{2\hbar}x^{2}\right)\end{aligned}$$ で与えられます。規格化の定数は除いています。$H_{n}$は$n$次のエルミート多項式です。
ここでは微分方程式を解くことに注目するので係数は無次元化しておきます。$m=\hbar^{2},\hbar\omega=1$とすると微分方程式は $$ \left(-\frac{1}{2}\frac{d^{2}}{dx^{2}}+\frac{1}{2}x^{2}\right)\phi(x)=E\phi(x) $$ となります。
Kovacicのアルゴリズム Kovacicのアルゴリズムに従って調和振動子のシュレディンガー方程式を解いていきます。以下の計算はすべて上のリンクに従っていますので適宜そちらを参照しながら計算を追ってもらうと良いと思います。
前処理 はじめに微分方程式を$\ddot{\eta}=r\eta$の形に前処理をします。シュレディンガー方程式は$\ddot{\phi}=(x^{2}-2E)\phi$となります。複素の微分方程式としてこれを考えるので$x$を$z$にして、 $$ \ddot{\eta}=r\eta,\quad r=z^{2}-2E $$ となります。$r$は$\mathbb{C}$上に特異点はないので、$\Gamma=\emptyset$です。また、$r=(1/z)^{-2}-2E$であるので、$z=\infty$における位数は$-2$です。
Case 1 Step 0 $z=\infty$における位数は偶数なのでStep 1に行きます。
Step 1 $(\infty_{3})$を計算します。$z=\infty$における位数は$m=-2\nu$として、$\nu=1$となります。 $\sqrt{r}$の$z=\infty$周りの展開は、 $$ \sqrt{r}=\sqrt{(1/z)^{-2}-2E}=(1/z)^{-1}-E(1/z)^{1}-\frac{E^{2}}{2}(1/z)^{3}+\cdots $$ となります。よって、 $$ [\omega]_ {\infty}=[\sqrt{r}]_ {\infty}=z $$ となります。$a$は$\sqrt{r}$のローラン展開の$z^{\nu}=z^{1}$の係数、$b$は$r-[\sqrt{r}]_ {\infty}^{2}$のローラン展開の$z^{\nu-1}=z^{0}$の係数となるので、 $$ a=1,\quad b=z^{2}-2E-z^{2}=-2E $$ です。よって、 $$ \alpha_{\infty}^{\pm}=\frac{1}{2}(\pm\frac{b}{a}-\nu)=\mp E-\frac{1}{2} $$ と求まりました。Step 2に行きます。
Step 2 $s=(s(c))_ {c\in\Gamma\cup{\infty}}$を定めますが、今$\Gamma=\emptyset$なので$s=\pm$について考えれば良いです。このとき、$d_{\pm}=\alpha_ {\infty}^{\pm}=\mp E-1/2$となります。これらが非負の整数である必要があることから場合分けが生じます。</description>
    </item>
    
    <item>
      <title>Egorov&#39;s theorem</title>
      <link>https://yonesuke.github.io/posts/egorov/</link>
      <pubDate>Tue, 22 Mar 2022 13:05:46 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/egorov/</guid>
      <description>Egorovの定理は関数列の概収束と概一様収束の関係を述べたものになります。
Theorem &amp;nbsp; &amp;nbsp;[Egorovの定理] 有限測度空間$(X,\mathcal{F},\mu)$上の可測関数列$f_{n}\colon X\to\mathbb{C}$に対して、 $f_{n}$が可測関数$f$に概収束するならば、$f_{n}$は$f$に概一様収束する。 ここで、収束の定義についてそれぞれまとめておきます。
Definition &amp;nbsp; &amp;nbsp;[概収束] 関数列$f_{n}$が$f$に概収束するとは、ほとんどすべての$x\in X$に対して$f_{n}(x)$が$f(x)$に収束することである。すなわち、$f_{n}$はほとんど至るところ$f$に各点収束する。 Definition &amp;nbsp; &amp;nbsp;[概一様収束] 関数列$f_{n}$が$f$に概一様収束するとは、任意の$\varepsilon&amp;gt;0$に対して$\mu(E)&amp;lt;\varepsilon$なる可測集合$E\in\mathcal{F}$が存在して、$E^{\mathrm{c}}$上では$f_{n}$が$f$に一様収束するようにできることである。 ちなみにT. TaoのAn Introduction to Measure Theoryでは、概収束のことをpointwise almost everywhere convergence, 概一様収束のことをalmost uniform convergenceと呼んでいます。(Taoの英語の本をずっと英語で読んでいたので対応する日本語を調べる必要がありました。)
証明 Proof [Of Theorem 1]: 点$x_{0}$において$f_{n}$が$f$に収束することを集合を用いて表してみましょう。 $$\begin{aligned}&amp;amp;f_{n}(x_{0})\to f(x_{0})\\ \Leftrightarrow&amp;amp; \forall m&amp;gt;0, \exists N\in\mathbb{N}, \mathrm{s.t.}\ n\geq N \Rightarrow |f_{n}(x_{0})-f(x_{0})|\leq\frac{1}{m}\\ \Leftrightarrow&amp;amp; \forall m&amp;gt;0, \exists N\in\mathbb{N}, \mathrm{s.t.}\ n\geq N \Rightarrow x_{0}\in\left\{x\in X\mid |f_{n}(x)-f(x)|\leq\frac{1}{m}\right\}\\ \Leftrightarrow&amp;amp; \forall m&amp;gt;0, \exists N\in\mathbb{N}, \mathrm{s.t.}\ x_{0}\in\bigcap_{n\geq N}\left\{x\in X\mid |f_{n}(x)-f(x)|\leq\frac{1}{m}\right\}\\ \Leftrightarrow&amp;amp; \forall m&amp;gt;0, x_{0}\in\bigcup_{N\in\mathbb{N}}\bigcap_{n\geq N}\left\{x\in X\mid |f_{n}(x)-f(x)|\leq\frac{1}{m}\right\}\\ \Leftrightarrow&amp;amp; x_{0}\in\bigcap_{m\in\mathbb{N}}\bigcup_{N\in\mathbb{N}}\bigcap_{n\geq N}\left\{x\in X\mid |f_{n}(x)-f(x)|\leq\frac{1}{m}\right\}\\\end{aligned}$$ となります。これより、点$x_{0}$において$f_{n}$が$f$に収束しないことは $$ x_{0}\in\bigcup_{m\in\mathbb{N}}\bigcap_{N\in\mathbb{N}}\bigcup_{n\geq N}\left\{x\in X\mid |f_{n}(x)-f(x)|&amp;gt;\frac{1}{m}\right\} $$ で書くことができます。ここで、 $$ E_{N,m}=\bigcup_{n\geq N} \left\{ x\in X \mid |f_{n}(x)-f(x)| &amp;gt; \frac{1}{m} \right\} $$ を定義すると(これが可測集合であることは明らかです)、収束しない点の測度は0なので$\mu(\bigcup_{m\in\mathbb{N}}\bigcap_{N\in\mathbb{N}}E_{N,m})=0$がわかります。特に測度の劣加法性から、任意の$m\in\mathbb{N}$に対して $$ \mu\left(\bigcap_{N\in\mathbb{N}}E_{N,m}\right)=0 $$ がわかります。$E_{N,m}$が集合として$N$に関して単調減少であり、しかも$\mu(X)&amp;lt;\infty$であることから、$\lim_{N\to\infty}\mu\left(E_{N,m}\right)=0$とも書けます。これは$m$を決めるたびごとに定まる$N$に関する数列だと思うと、任意の$\varepsilon&amp;gt;0$と任意の$m$に対してある$N_{m}$があって、$\mu\left(E_{N_{m},m}\right)&amp;lt;\varepsilon/2^{m}$となるようにできます。このときに集合$E$を $$ E=\bigcup_{m\in\mathbb{N}}E_{N_{m},m} $$ で定めると、$\mu(E)&amp;lt;\varepsilon$は測度の加法性から明らかです。$E^{\mathrm{c}}$は、 $$ E^{\mathrm{c}}=\bigcap_{m\in\mathbb{N}}\bigcap_{n\geq N_{m}}\left\{ x\in X \mid |f_{n}(x)-f(x)| \leq \frac{1}{m} \right\} $$ と書けて、これは$E^{\mathrm{c}}$上では$x$の選び方によらずに関数の差が$1/m$で抑えられていることを意味します。よって、$E^{\mathrm{c}}$上で一様収束することが示せました。 有限でない場合の反例 有限でない場合の例として、測度空間$(\mathbb{R}, \mathcal{B}, \mu)$で、$f_{n}=1_{[n,n+1]}$を考えます。この関数が$f=0$に各点収束することは明らかですが、$\mu(E)&amp;lt;1$であれば$E^{\mathrm{c}}$上で$f_{n}$の値が1になるようなものを作ることができてしまうので、この場合には概一様収束にはなりえません。</description>
    </item>
    
    <item>
      <title>Uniform ratio distribution and pi</title>
      <link>https://yonesuke.github.io/posts/uniform-ratio-pi/</link>
      <pubDate>Tue, 15 Mar 2022 13:05:46 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/uniform-ratio-pi/</guid>
      <description>3月14日は円周率の日ということもあって次のツイートを見つけました。
Happy Pi Day, folks!
(Today = March 14 = 3/14. And pi ~= 3.14.)
Here&amp;#39;s a fun way to approximate pi using probability.
Take 2 random numbers X and Y between 0 and 1. What&amp;#39;s the probability that the integer nearest to X/Y is even?
Answer: (5 - pi)/4 pic.twitter.com/ZWviwSQ2VM
&amp;mdash; 10-K Diver (@10kdiver) March 14, 2022 この証明を行っていきます。
Uniform ratio distribution $[0,1]$区間の一様乱数$X,Y$に対して$X/Y$が取る分布はuniform ratio distributionという名前がついています。この分布は手で計算することができて、 $$ \begin{aligned} P_{X/Y}(u)\coloneqq&amp;amp; \int_{0}^{1}\int_{0}^{1}\delta\left(\frac{x}{y}-u\right)dxdy\\ =&amp;amp;\begin{cases} 1/2, &amp;amp; 0&amp;lt;u&amp;lt;1\\ 1/(2u^{2}), &amp;amp; u\geq1 \end{cases} \end{aligned} $$ となることが知られています。</description>
    </item>
    
    <item>
      <title>MNISTをMLPで推論(Julia/Flux実装)</title>
      <link>https://yonesuke.github.io/posts/mnist_mlp/</link>
      <pubDate>Mon, 23 Aug 2021 16:38:35 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/mnist_mlp/</guid>
      <description>Juliaで機械学習をするための有名なライブラリにFluxがあります。Fluxを使ってMNISTの手書き数字の推論を行ったのでその方法をまとめておきます。 コードは次のようになります。これを参考に書きました。
パッケージ 基本的にFluxさえあれば良いです。今回はMNISTデータを用いるのでMLDatasetsというパッケージを用いてデータを読み込みます。これらのパッケージは事前にインストールしておく必要があります。JuliaのREPLやnotebook上で次を入力してください。
1 julia&amp;gt; import Pkg; Pkg.add([&amp;#34;Flux&amp;#34;, &amp;#34;MLDatasets&amp;#34;]) これでパッケージを読み込むことができます。
1 2 3 4 5 using Flux using Flux.Data: DataLoader using Flux: onehotbatch, onecold using Flux.Losses: logitcrossentropy using MLDatasets データの読み込み MNISTデータを読み込みます。
1 2 x_train, y_train = MLDatasets.MNIST.traindata(Float32) x_test, y_test = MLDatasets.MNIST.testdata(Float32) MNISTの画像はサイズ(28,28,1)になっていますが、MLPには1次元の配列として渡したいのでflattenで各データを1次元に落とします。
1 2 x_train = Flux.flatten(x_train) # 784×60000 x_test = Flux.flatten(x_test) # 784×10000 また、各画像の数字(0~9)はone-hotにしておきたいのでそちらはonehotbatchという関数で変換しておきます。
1 2 y_train = onehotbatch(y_train, 0:9) # 10×60000 y_test = onehotbatch(y_test, 0:9) # 10×10000 モデルの定義 いよいよモデルの定義です。今回は一番簡単なMLPで実装していきます。</description>
    </item>
    
    <item>
      <title>Gram行列の固有値の数値計算</title>
      <link>https://yonesuke.github.io/posts/gram_eigen/</link>
      <pubDate>Mon, 02 Aug 2021 16:59:14 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/gram_eigen/</guid>
      <description>カーネル関数$k(\cdot,\cdot)$が与えられたとき、データ点$\{x_{i}\}_{i=1}^{n}$に対するGram行列(グラム行列)は $$ K=\begin{pmatrix}k(x_{1},x_{1}) &amp;amp; \cdots &amp;amp; k(x_{1},x_{n})\\\vdots &amp;amp; \ddots &amp;amp; \vdots\\ k(x_{n},x_{1}) &amp;amp; \cdots &amp;amp; k(x_{n},x_{n})\end{pmatrix} $$ で与えられます。色々な場面に登場するのですが、RBFカーネルからガウス過程を生成する際にその固有値計算で詰まったところがあったのでかんたんにまとめておきます。
Gram行列の性質 カーネル関数の定義は対称性$k(x,y)=k(y,x)$が成り立ってグラム行列が(半)正定値行列になることです。すなわち、任意のベクトル$\bm{c}$に対して、 $$ \bm{c}^{\top}K\bm{c}\geq0 $$ が成り立つことです。なので、カーネル関数から生成されたGram行列は常に半正定値行列になります。 半正定値性と固有値が非負であることは同値なのでGram行列の固有値は常に非負です。
ガウス過程に従う関数の生成 次にガウス過程から関数をサンプルすることを考えましょう。$f\sim\mathcal{GP}(m,k)$について、特にかんたんのために平均を$0$としておきましょう。 このとき、$\{x_{i}\}_{i=1}^{n}$上で関数$f$のベクトル$\bm{f}$は $$ \bm{f}\sim\mathcal{N}(\bm{0},K) $$ という多次元ガウス分布に従うものになります。これはGram行列を分散共分散行列とするガウス分布で、代表的なサンプル方法は$K$のコレスキー分解行列と$n$次元の独立標準正規分布に従う乱数との行列・ベクトル積を計算することで得られます。
固有値分布とガウス過程の関係 RBFカーネルからGram行列を生成し、コレスキー分解することを考えましょう。ここでRBFカーネルは $$ k(x,y)=\exp\left(-\frac{(x-y)^{2}}{2l^{2}}\right) $$ で与えられます。Julia実装をしてみると、
1 2 3 4 using LinearAlgebra xs = -2:0.02:2 K = [exp(-(x-y)^2) for x in xs, y in xs] cholesky(K) となりますが、これを実行すると、
1 ERROR: PosDefException: matrix is not positive definite; Cholesky factorization failed. と帰ってきて、Gram行列が正定値でないと言われてしまいます。理論ではGram行列は正定値であるはずなので、これは数値的な誤差に起因していると考えられます。 そこでGram行列の固有値分布を確認してみることにします。この際、RBFカーネルだけでなく周期カーネルとMatérnカーネルについても固有値分布と対応するガウス過程のサンプルをプロットしました。この結果が次のようになります。</description>
    </item>
    
    <item>
      <title>Laplacianの積分表現</title>
      <link>https://yonesuke.github.io/posts/laplacian_integral/</link>
      <pubDate>Fri, 30 Jul 2021 18:34:51 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/laplacian_integral/</guid>
      <description>領域$\Omega\subset\mathbb{R}^{n}$上で定義された関数$u\in C^{2}(\Omega)$についてLaplacian(ラプラシアン)は $$ \Delta u(x)=\sum_{i=1}^{n}\frac{\partial^{2}u}{\partial x_{i}^{2}}(x) $$ で表されます。このとき、$\partial B(x,r)=\{y\in\mathbb{R}^{n}\mid |x-y|=r\}$とおくと、 $$ \Delta u(x)=\lim_{r\to+0}\frac{2n}{r^{2}|\partial B(x,r)|}\int_{\partial B(x,r)}u(y)-u(x)d\sigma_{y} $$ が成り立ちます。$d\sigma_{y}$は$\partial B(x,r)$上の面積要素です。 この表現を得るには$u(y)$を$x$まわりでTaylor展開することが大事になるのですが、 その際、平均値の定理によって得られるTaylor展開だと剰余項の評価が難しくなります。 積分型のTaylor展開を用いることでこの問題を解決することができます。
積分型のTaylor展開 はじめに多重指数$\alpha\in\mathbb{N}_{\geq 0}^{n}$を導入します。 $$ |\alpha|=\alpha_{1}+\cdots+\alpha_{n},\quad x^{\alpha}=x_{1}^{\alpha_{1}}\cdots x_{n}^{\alpha_{n}} $$ とし、多重指数による微分を $$ D^{\alpha}f=\frac{\partial^{|\alpha|}f}{\partial x_{1}^{\alpha_{1}}\cdots\partial x_{n}^{\alpha_{n}}} $$ とします。 このとき、関数$f$が$C^{k+1}$級であるとき、点$x$まわりの積分型のTaylor展開は $$\begin{aligned}&amp;amp;f(y)=\sum_{|\alpha|\leq k}\frac{D^{\alpha}f(x)}{\alpha!}(y-x)^{\alpha}+\sum_{|\beta|=k+1}R_{\beta}(y)(y-x)^{\beta},\\&amp;amp;R_{\beta}(y)=\frac{k+1}{\beta!}\int_{0}^{1}(1-t)^{k}D^{\beta}f(x+t(y-x))dt\end{aligned}$$ で与えられます。このように剰余項$R_{\beta}$が積分の形で明示的に与えられるのが特徴です。 授業ではよく平均値の定理を用いた証明を習うと思うのですが、意外にこの積分型のTaylor展開が役に立つことがあるので覚えておいて損はないと思います。
証明 $u$を$x$周りで展開しましょう。$u\in C^{2}(\Omega)$なので、 $$\begin{aligned}u(y)-u(x)=&amp;amp;\sum_{i=1}^{n}\frac{\partial u}{\partial x_{i}}(x)(y_{i}-x_{i})+2\sum_{i\ne j}(y_{i}-x_{i})(y_{j}-x_{j})\int_{0}^{1}(1-t)\frac{\partial^{2}u}{\partial x_{i}\partial x_{j}}(x+t(y-x))dt\\&amp;amp;+\sum_{i=1}^{n}(y_{i}-x_{i})^{2}\int_{0}^{1}(1-t)\frac{\partial^{2}u}{\partial x_{i}^{2}}(x+t(y-x))dt\end{aligned}$$ です。これを積分すると、 $$\begin{aligned}\int_{\partial B(x,r)}u(y)-u(x)d\sigma_{y}=&amp;amp;\sum_{i=1}^{n}\frac{\partial u}{\partial x_{i}}(x)\int_{\partial B(x,r)}y_{i}-x_{i}d\sigma_{y}\\&amp;amp;+2\sum_{i\ne j}\int_{\partial B(x,r)}(y_{i}-x_{i})(y_{j}-x_{j})\left[\int_{0}^{1}(1-t)\frac{\partial^{2}u}{\partial x_{i}\partial x_{j}}(x+t(y-x))dt\right]d\sigma_{y}\\&amp;amp;+\sum_{i=1}^{n}\int_{\partial B(x,r)}(y_{i}-x_{i})^{2}\left[\int_{0}^{1}(1-t)\frac{\partial^{2}u}{\partial x_{i}^{2}}(x+t(y-x))dt\right]d\sigma_{y}\end{aligned}$$ となります。 まず、$y_{i}-x_{i}$は$\partial B(x,r)$の奇関数なのでこの積分は$0$となります。 また、$y-x=rz,\ d\sigma_{y}=r^{n-1}d\sigma_{z}$と変数変換すると、$|\partial B(x,r)|=r^{n-1}|\partial B(0,1)|$より、 $$ \begin{aligned}&amp;amp;\frac{2n}{r^{2}|\partial B(x,r)|}\int_{\partial B(x,r)}(y_{i}-x_{i})(y_{j}-x_{j})\left[\int_{0}^{1}(1-t)\frac{\partial^{2}u}{\partial x_{i}\partial x_{j}}(x+t(y-x))\right]d\sigma_{y}\\=&amp;amp;\frac{2n}{|\partial B(0,1)|}\int_{\partial B(0,1)}z_{i}z_{j}\left[\int_{0}^{1}(1-t)\frac{\partial^{2}u}{\partial x_{i}\partial x_{j}}(x+trz)\right]d\sigma_{z}\\\to&amp;amp;\frac{2n}{|\partial B(0,1)|}\int_{\partial B(0,1)}z_{i}z_{j}\left[\int_{0}^{1}(1-t)\frac{\partial^{2}u}{\partial x_{i}\partial x_{j}}(x)\right]d\sigma_{z}=\frac{n}{|\partial B(0,1)|}\frac{\partial^{2}u}{\partial x_{i}\partial x_{j}}(x)\int_{\partial B(0,1)}z_{i}z_{j}d\sigma_{z}\end{aligned} $$ と$r\to+0$の極限で求まります。ただし極限操作の交換は優収束定理から正当化されます。 さらに、$i\ne j$ならば$z_{i}z_{j}$の積分は$0$になり、$i=j$ならば、$z_{i}^{2}$の積分は$i$に依存しないので、 $$ \frac{n}{|\partial B(0,1)|}\frac{\partial^{2}u}{\partial x_{i}^{2}}(x)\int_{\partial B(0,1)}z_{i}^{2}d\sigma_{z}=\frac{1}{|\partial B(0,1)|}\frac{\partial^{2}u}{\partial x_{i}^{2}}(x)\int_{\partial B(0,1)}\sum_{i=1}^{n}z_{i}^{2}d\sigma_{z}=\frac{\partial^{2}u}{\partial x_{i}^{2}}(x) $$ と計算できます。よって、 $$ \frac{2n}{r^{2}|\partial B(x,r)|}\int_{\partial B(x,r)}u(y)-u(x)d\sigma_{y}\to\sum_{i=1}^{n}\frac{\partial^{2}u}{\partial x_{i}^{2}}(x)=\Delta u(x) $$ となり、示されました。</description>
    </item>
    
    <item>
      <title>Borwein integral</title>
      <link>https://yonesuke.github.io/posts/borwein/</link>
      <pubDate>Tue, 13 Jul 2021 02:20:20 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/borwein/</guid>
      <description>Borwein積分は$\sin x/x$に関する興味深い性質を持った積分のことです。 例えば $$ \int_{0}^{\infty}\frac{\sin x}{x}dx=\frac{\pi}{2} $$ となることはよく知られていますが、これに$\sin(3x)/3x$をかけたものについても $$ \int_{0}^{\infty}\frac{\sin x}{x}\frac{\sin (x/3)}{x/3}dx=\frac{\pi}{2} $$ が成り立ちます。同様のことは$\sin (x/5)/(x/5)$や$\sin(x/7)/(x/7)$をかけていっても成り立ち、 $$ \int_{0}^{\infty}\frac{\sin x}{x}\frac{\sin (x/3)}{x/3}\cdots\frac{\sin (x/13)}{x/13}dx=\frac{\pi}{2} $$ となります。しかし、次のステップではこの計算は崩れて $$ \int_{0}^{\infty}\frac{\sin x}{x}\frac{\sin (x/3)}{x/3}\cdots\frac{\sin (x/15)}{x/15}dx=\frac{467807924713440738696537864469}{935615849440640907310521750000}\pi&amp;lt;\frac{\pi}{2} $$ となってしまいます。一見するとこの値も$\pi/2$になりそうなのですが、何故か値がずれてしまいます。 このような積分のことをBorwein積分とよび、いくつかの計算がなされています。
公式 数列$a_{1},a_{2},\dots,a_{n}$に対して、Borwein積分$\tau_{n}$を $$ \tau_{n}=\int_{0}^{\infty}\prod_{k=1}^{n}\frac{\sin(a_{k}x)}{a_{k}x}dx $$ で定めます。ここでは各$a_{i}$は正だとしておきます。この積分は完全に計算することができて、 $$ \tau_{n}=\frac{\pi}{2^{n+1}(n-1)!\prod_{k=1}^{n}a_{k}}\sum_{p_{1},\dots,p_{n}=\pm1}p_{1}\cdots p_{n}\mathrm{sgn}\left(\bm{p}\cdot\bm{a}\right)(\bm{p}\cdot\bm{a})^{n-1} $$ となります。以下でこれを示していきます。元論文ではFourier変換による畳込み積分を用いて計算していますが、留数定理を用いた計算でも示すことができたのでそれを紹介していきます。
証明 はじめに$\sin(a_{k}x)$を展開していきます。 $$ \tau_{n}=\int_{0}^{\infty}\prod_{k=1}^{n}\frac{\sin(a_{k}x)}{a_{k}x}dx =\frac{1}{(2i)^{n}\prod_{k=1}^{n}a_{k}}\int_{0}^{\infty}\frac{\prod_{k=1}^{n}(e^{ia_{k}x}-e^{-ia_{k}x})}{x^{n}}dx $$ です。ここでパラメーター$p_{k}=\pm1$を導入して$e^{ia_{k}x}-e^{-ia_{k}x}=\sum_{p_{k}=\pm1}p_{k}e^{ip_{k}a_{k}x}$と書くと、 $$ \prod_{k=1}^{n}(e^{ia_{k}x}-e^{-ia_{k}x})=\sum_{p_{1},\dots,p_{n}=\pm1}p_{1}\cdots p_{n}e^{i\bm{p}\cdot\bm{a}x} $$ となるので、 $$ \tau_{n}=\frac{1}{(2i)^{n}\prod_{k=1}^{n}a_{k}}\sum_{p_{1},\dots,p_{n}=\pm1}p_{1}\cdots p_{n}\int_{0}^{\infty}\frac{e^{i\bm{p}\cdot\bm{a}x}}{x^{n}}dx $$ となります。
よって、$\int_{0}^{\infty}e^{iax}/x^{n}dx$が計算できれば良いです。 $a&amp;gt;0$の場合を考えます。 被積分関数を複素平面に持ち上げて $$ f(z)=\frac{e^{iaz}}{z^{n}} $$ とします。このとき、次のような積分経路$C$を考えます。
$\gamma_{\varepsilon}$は半径$\varepsilon$の円の上半面を時計回りに回る経路で、$\Gamma_{R}$は半径$R$の上半面を反時計回りに回る経路です。 $f(z)$は$C$内で正則なので$\oint_{C}f(z)dz=0$です。 また、 $$ \oint_{C}=\int_{-R}^{-\varepsilon}+\int_{\gamma_{\varepsilon}}+\int_{\varepsilon}^{R}+\int_{\Gamma_{R}} $$ です。</description>
    </item>
    
    <item>
      <title>特異値分解</title>
      <link>https://yonesuke.github.io/posts/svd/</link>
      <pubDate>Mon, 31 May 2021 16:22:52 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/svd/</guid>
      <description>行列$A$を$m\times n$の実行列とします。 このときある直交行列$U\in\mathbb{R}^{m\times m},V\in\mathbb{R}^{n\times n}$が存在して、 $$U^{\mathsf{T}}AV=\Sigma=\begin{pmatrix}\mathrm{diag}(\sigma_{1},\dots,\sigma_{r}) &amp;amp; O_{r\times(n-r)} \\ O_{(m-r)\times r} &amp;amp; O_{(m-r)\times (n-r)}\end{pmatrix}\in\mathbb{R}^{m\times n}$$ となるようにできます。このような分解を特異値分解と言います。
証明 $A^{\mathsf{T}}A$は実対称行列なので固有ベクトル${v_{1},\dots,v_{n}}$と固有値${\xi_{1},\dots,\xi_{n}}$が存在して $$ A^{\mathsf{T}}Av_{i}=\xi_{i}v_{i},\quad (v_{i},v_{j})=\delta_{ij} $$ となるようにできます。 また、 $$ \xi_{i}=(v_{i},\xi_{i}v_{i})=(v_{i},A^{\mathsf{T}}Av_{i})=(Av_{i},Av_{i})\geq0 $$ なので固有値は常に0以上です。特に、$i=1,\dots,r$で$\xi_{i}&amp;gt;0$かつ$i&amp;gt;r$で$\xi_{i}=0$となるようにしておきます。 実はこの固有ベクトル$v_{i}$たちが直交行列$V$に対応します。
次に$i=1,2,\dots,r$に対して$u_{i}=Av_{i}/\sqrt{\xi_{i}}$としましょう。すると、 $$ (u_{i}, u_{j})=\frac{1}{\sqrt{\xi_{i}\xi_{j}}}(Av_{i},Av_{j})=\frac{1}{\sqrt{\xi_{i}\xi_{j}}}(v_{i},A^{\mathsf{T}}Av_{j})=\frac{\xi_{j}\delta_{ij}}{\sqrt{\xi_{i}\xi_{j}}}=\delta_{ij} $$ となり、$u_{i}$たちは互いに直交します。このとき$m-r$個のベクトル${u_{r+1},\dots,u_{m}}$を持ってきて正規直交基底${u_{1},\dots,u_{m}}$を構成することができます。 実はこのベクトル$u_{i}$たちが直交行列$U$に対応します。
上で得られたベクトルを用いて行列 $$ V=(v_{1},\dots,v_{n}),\quad U=(u_{1},\dots,u_{m}) $$ を構成します。これが直交行列なのは明らかです。 $U^{\mathsf{T}}AV$という行列を計算してみると、 $$ U^{\mathsf{T}}AV[i,j]=(u_{i},Av_{j}) $$ となります。
$j=r+1,\dots,n$においては $$ (Av_{j},Av_{j})=(v_{j},A^{\mathsf{T}}Av_{j})=(v_{j},0)=0 $$ なので$(u_{i},Av_{j})=0$となります。 $i=r+1,\dots,m$かつ$j=1,\dots,r$においても $$ (u_{i},Av_{j})=(u_{i},\sqrt{\xi_{j}}u_{j})=0 $$ となります。 $1\leq i,j\leq r$のときには、 $$ (u_{i},Av_{j})=(u_{i},\sqrt{\xi_{j}}u_{j})=\sqrt{\xi_{j}}\delta_{ij} $$ となります。 よって、$\sqrt{\xi_{i}}=\sigma_{i}$と置くことで $$U^{\mathsf{T}}AV=\Sigma=\begin{pmatrix}\mathrm{diag}(\sigma_{1},\dots,\sigma_{r}) &amp;amp; O_{r\times(n-r)} \\ O_{(m-r)\times r} &amp;amp; O_{(m-r)\times (n-r)}\end{pmatrix}$$ となることが示されました。 特に$A=U\Sigma V^{\mathsf{T}}$もわかります。</description>
    </item>
    
    <item>
      <title>Good Will Hunting Problem</title>
      <link>https://yonesuke.github.io/posts/good-will-hunting/</link>
      <pubDate>Mon, 31 May 2021 13:01:31 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/good-will-hunting/</guid>
      <description>マット・デイモンとロビン・ウィリアムズ主演の映画『グッド・ウィル・ハンティング/旅立ち』(Good Will Hunting)の中で、MITの廊下に掲示されたグラフ理論の問題を清掃をしていたマット・デイモンが解いてしまうシーンがあります。 中学生とかのときに初めてこの映画を見たときにはよっぽど難しい問題なんだろうな、と思ったのですが、最近見返してみると定義に従って素直に計算すれば解ける問題だということがわかったのでまとめておきます。
Given the graph $G$, find
The adjacency matrix, $A$ The matrix giving the number of 3 step walks The generating function for walks from $i\to j$ The generating function for walks from $1\to3$ 問1 グラフの隣接行列$A$の$(i,j)$成分は$i$から$j$に向かう枝があれば$1$、そうでなければ$0$となるように定義されています。 今の場合、頂点$3$と頂点$4$の間には2本枝があるのでその場合は重み付きの枝だと思って$2$とします。 そうすると隣接行列は、 $$ A= \begin{pmatrix} 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 1\\1 &amp;amp; 0 &amp;amp; 2 &amp;amp; 1\\0 &amp;amp; 2 &amp;amp; 0 &amp;amp; 0\\1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \end{pmatrix} $$ となります。</description>
    </item>
    
    <item>
      <title>至るところ微分不可能な連続関数: 初等的な構成方法</title>
      <link>https://yonesuke.github.io/posts/nowherediff/</link>
      <pubDate>Sun, 30 May 2021 09:56:32 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/nowherediff/</guid>
      <description>$[-1,1]$上の関数 $$ \varphi(x)=|x| $$ を考え、 これを$\varphi(x+2)=\varphi(x)$として$\mathbb{R}$上へ拡張します。 このとき、 $$ f(x)=\sum_{n=0}^{\infty}\left(\frac{3}{4}\right)^{n}\varphi(4^{n}x) $$ は$\mathbb{R}$上の連続関数ですが至るところ微分不可能であることが知られています。 以下でこれを示していきましょう。
連続であること 連続であることの証明はかんたんです。 $$ f_{k}(x)=\sum_{n=0}^{k}\left(\frac{3}{4}\right)^{n}\varphi(4^{n}x) $$ とおくと、$f_{k}$は連続関数です。 $$ |f(x)-f_{k}(x)|\leq\sum_{n=k+1}^{\infty}\left(\frac{3}{4}\right)^{n}=4\left(\frac{3}{4}\right)^{k+1} $$ となるので、$x$によらずに一様に抑えることができます。 これは連続関数$f_{k}$が$f$に一様収束していることを表します。 連続関数の一様収束極限は連続関数なので$f$は連続関数です。
微分不可能であること 任意の$x_{0}\in\mathbb{R}$を一つ固定しましょう。 $m\in\mathbb{N}$に対して、$\delta_{m}=\pm\frac{1}{2}4^{-m}$とおき、 符号は$4^{m}x_{0}$と$4^{m}(x_{0}+\delta_{m})$の間に整数が来ないようにします。 $$ \gamma_{n}=\frac{\varphi(4^{n}(x_{0}+\delta_{m}))-\varphi(4^{n}x_{0})}{\delta_{m}} $$ という値を計算してみましょう。
$n&amp;gt;m$のときには$4^{n}\delta_{m}$は$2$の倍数となります。 $\varphi(x)$が周期$2$の関数であることを思い出すと、$\gamma_{n}=0$がわかります。 $n=m$のときには$4^{m}x_{0}$と$4^{m}(x_{0}+\delta_{m})$の間に整数が来ないので、 $|\gamma_{m}|=|4^{m}\delta_{m}/\delta_{m}|=4^{m}$となります。 $n &amp;lt; m$のときには、一般に$|\varphi(x)-\varphi(y)|\leq|x-y|$なので、 $|\gamma_{n}|\leq4^{n}$がわかります。 以上の準備のもと$f$が微分不可能であることを示しましょう。 具体的には、上で定義した$\delta_{m}$を用いて、$x_{0}+\delta_{m}\to x_{0}$の極限で微分が発散することを確認します。 $$ \left|\frac{f(x_{0}+\delta_{m})-f(x_{0})}{\delta_{m}}\right| =\left|\sum_{n=0}^{\infty}\left(\frac{3}{4}\right)^{n}\frac{\varphi(4^{n}(x_{0}+\delta_{m}))-\varphi(4^{n}x_{0})}{\delta_{m}}\right| =\left|\sum_{n=0}^{\infty}\left(\frac{3}{4}\right)^{n}\gamma_{n}\right| $$ となりますが、$\gamma_{n}$が$n&amp;gt;m$で消えること、また$|x+y|\geq|x|-|y|$であることを用いると、 $$\begin{aligned}\left|\sum_{n=0}^{\infty}\left(\frac{3}{4}\right)^{n}\gamma_{n}\right|=&amp;amp;\left|\sum_{n=0}^{m}\left(\frac{3}{4}\right)^{n}\gamma_{n}\right|\\=&amp;amp;\left|\left(\frac{3}{4}\right)^{m}\gamma_{m}+\sum_{n=0}^{m-1}\left(\frac{3}{4}\right)^{n}\gamma_{n}\right|\\\geq&amp;amp;\left(\frac{3}{4}\right)^{m}|\gamma_{m}|-\left|\sum_{n=0}^{m-1}\left(\frac{3}{4}\right)^{n}\gamma_{n}\right|\\\geq&amp;amp;3^{m}-\sum_{n=0}^{m-1}3^{m}=\frac{1}{2}(3^{m}+1)\to\infty\end{aligned}$$ となり、$\delta_{m}$を用いた点列の収束だと微分が発散することがわかります。 よって$f$が微分不可能であることを示すことができました。
グラフの概形 $f$自身は無限和で定義されているためプロットすることはできません。 その代わりに$f_{k}$をプロットすることにしました。 そのときのpythonファイルです。
$ k = 0,1,3,10 $の場合の$ f _ {k} $をプロットしたものが次のようになります。
$f_{0}$は$\varphi$にほかなりません。 $f_{1}$は$\varphi(4x)$によって引き伸ばされたものを足し込むことによって微分不可能な点が新たに増えていることがわかります。 このように$\varphi(4^{m}x)$によって微分不可能点が$\mathbb{R}$全体に伝播していって最終的に$f$が微分不可能になる様子が確認できます。</description>
    </item>
    
    <item>
      <title>坂口-蔵本モデルのダイナミクス</title>
      <link>https://yonesuke.github.io/posts/sakaguchi-kuramoto/</link>
      <pubDate>Fri, 28 May 2021 13:32:31 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/sakaguchi-kuramoto/</guid>
      <description>坂口-蔵本モデルは蔵本モデルにphase lagを導入したモデルで、次の微分方程式で表されます。 $$ \frac{d\theta_{i}}{dt}=\omega_{i}+\frac{K}{N}\sum_{j=1}^{N}\sin(\theta_{j}-\theta_{i}+\alpha),\quad i=1,\dots,N. $$ $\alpha$が位相差に対応していて、$\alpha=0$のときは蔵本モデルに戻ります。 結合強度$K$が変化したときに振動子が同期するかどうかを調べていきましょう。
自然振動数分布 各振動子は固有の角速度である$\omega_{i}$を持っています。 蔵本モデル関連の論文では$\omega_{i}$は分布に従っていると仮定しています。 この分布のことを自然振動数分布と呼びます。 分布としてはGauss分布を選ぶのかと思いきや、蔵本モデルにおいては解析のしやすさからCauchy分布が選ばれています。 $$ g(\omega)=\frac{\Delta}{\pi}\frac{1}{(\omega-\omega_{0})^{2}+\Delta^{2}} $$ $\omega_{0}$は分布の中心で、$\Delta$は分布の幅を表します。 このページでもCauchy分布を用いることにします。
秩序変数 同期の具合をはかるパラメーターとして秩序変数(order parameter)を導入します。 $$ z=re^{i\phi}=\frac{1}{N}\sum_{j=1}^{N}e^{i\theta_{j}} $$
$r\approx0$のときは振動子たちは円周上をバラバラに分布しているため、同期していないことがわかります。 $r=1$のときには振動子たちは円周上の1点に集中しているため、完全同期しています。 連続極限 振動子が無限個ある極限$N\to\infty$において、微分方程式は連続の式に従います。 $$ \frac{\partial f}{\partial t}+\frac{\partial}{\partial\theta}(V[f]f)=0 $$ ここで$V[f]$は速度場で微分方程式を無限に飛ばした式になります。 $$ V[f]=\omega+K\int_{\mathbb{R}}d\omega&amp;rsquo;\int_{\mathbb{S}^{1}}d\theta&amp;rsquo; f(\theta&amp;rsquo;,\omega&amp;rsquo;,t)\sin(\theta&amp;rsquo;-\theta+\alpha) $$
$f(\theta,\omega,t)$は時刻$t$における$\theta,\omega$の密度関数になります。 すなわち、$[\theta,\theta+\delta\theta)\times[\omega,\omega+\delta\omega)$に存在する振動子の割合が $f(\theta,\omega,t)\delta\theta\delta\omega$になります。 $\omega$は$g(\omega)$に従うので、 $$ \int_{\mathbb{S}^{1}}d\theta f(\theta,\omega,t)=g(\omega) $$ が成り立ちます。 連続極限において秩序変数は $$ z=re^{i\phi}=\int_{\mathbb{R}}d\omega\int_{\mathbb{S}^{1}}d\theta f(\theta,\omega,t)e^{i\theta} $$ となります。 Fourier級数展開 分布関数$f(\theta,\omega,t)$について、$\theta$方向は$\mathbb{S}^{1}$に乗っているのでFourier級数展開ができます。 $$ f(\theta,\omega,t)=\frac{1}{2\pi}\sum_{k\in\mathbb{Z}}\hat{f} _ {k}(\omega,t)e^{-ik\theta} $$ これをもとに連続の式もFourier級数展開しましょう。 かんたんのために$h(\omega)$の$\omega$積分を$\langle h\rangle$と書くことにします。 $$ \begin{aligned}V[f]=&amp;amp;\omega+\frac{Ke^{-i(\theta-\alpha)}}{2i}\int_{\mathbb{R}}d\omega&amp;rsquo;\int_{\mathbb{S}^{1}}d\theta&amp;rsquo; f(\theta&amp;rsquo;,\omega&amp;rsquo;,t)e^{i\theta&amp;rsquo;}-\frac{Ke^{i(\theta-\alpha)}}{2i}\int_{\mathbb{R}}d\omega&amp;rsquo;\int_{\mathbb{S}^{1}}d\theta&amp;rsquo; f(\theta&amp;rsquo;,\omega&amp;rsquo;,t)e^{-i\theta&amp;rsquo;}\\=&amp;amp;\omega+\frac{Ke^{-i(\theta-\alpha)}}{2i}\langle\hat{f} _ {1}\rangle-\frac{Ke^{i(\theta-\alpha)}}{2i}\langle\hat{f} _ {-1}\rangle \end{aligned}$$ であるので、 $$ \widehat{V[f]} _ {0}=2\pi\omega,\widehat{V[f]} _ {-1}=i\pi Ke^{-i\alpha}\langle\hat{f} _ {-1}\rangle,\widehat{V[f]} _ {1}=-i\pi Ke^{i\alpha}\langle\hat{f} _ {1}\rangle $$ がわかります。 これを連続の式に代入することで、 $$ \begin{aligned} \frac{\partial\hat{f} _ {k}}{\partial t}=&amp;amp;-\widehat{\frac{\partial}{\partial\theta}(V[f]f)} _ {k}=ik\widehat{V[f]f} _ {k}=\frac{ik}{2\pi}\left(\sum_{l\in\mathbb{Z}}\widehat{V[f]} _ {l}\hat{f} _ {k-l}\right)=\frac{ik}{2\pi}\left(\sum_{l=0,\pm1}\widehat{V[f]} _ {l}\hat{f} _ {k-l}\right)\\=&amp;amp;ik\omega\hat{f} _ {k}+\frac{kKe^{i\alpha}}{2}\langle\hat{f} _ {1}\rangle\hat{f} _ {k-1}-\frac{kKe^{-i\alpha}}{2}\langle\hat{f} _ {-1}\rangle\hat{f} _ {k+1} \end{aligned} $$ となります。特に秩序変数が$z=\langle\hat{f} _ {1}\rangle$で書けるので、 $$ \frac{\partial\hat{f} _ {k}}{\partial t}=ik\omega\hat{f} _ {k}+\frac{kKe^{i\alpha}z}{2}\hat{f} _ {k-1}-\frac{kKe^{-i\alpha}\overline{z}}{2}\hat{f} _ {k+1} $$ とさらに簡略化して書くことができます。 この偏微分方程式を解くことができればダイナミクスの理解が進むのですが、 このままでは難しいです。 この難しさを乗り越えたのがOtt-Antonsen縮約と呼ばれるものがあって、この偏微分方程式の特解を表します。</description>
    </item>
    
    <item>
      <title>包除原理</title>
      <link>https://yonesuke.github.io/posts/inclusion_exclusion/</link>
      <pubDate>Sun, 16 May 2021 12:18:18 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/inclusion_exclusion/</guid>
      <description>測度空間$(X,\mathcal{B},\mu)$の有限測度集合$A_{i}(i=1,\dots,n)$に対して $$ \mu\left(\bigcup_{i=1}^{n}A_{i}\right)=\sum_{J\subset[n];J\ne\emptyset}(-1)^{|J|-1}\mu\left(\bigcap_{i\in J}A_{i}\right) $$ が成り立ちます。これを包除原理(Inclusion-exclusion principle)と呼びます。
証明 証明は $$ \int_{X}\left(1-\prod_{i=1}^{n}(1-1_{A_{i}})\right)d\mu $$ を二通りに計算することにより求まります。
はじめに愚直に展開してみることにします。 $$ \prod_{i=1}^{n}(1-x_{i})=\sum_{J\subset[n]}(-1)^{|J|}\prod_{i\in J}x_{i} $$ は展開すればわかるので、この$x_{i}$に$1_{A_{i}}$を代入すると、 $$ 1-\prod_{i=1}^{n}(1-1_{A_{i}})=1-\sum_{J\subset[n]}(-1)^{|J|}\prod_{i\in J}1_{A_{i}} =\sum_{J\subset[n];J\ne\emptyset}(-1)^{|J|-1}1_{\bigcap_{i\in J}A_{i}} $$ となります。ここで$1_{A}1_{B}=1_{A\cap B}$を用いました。以上より、 $$ \int_{X}\left(1-\prod_{i=1}^{n}(1-1_{A_{i}})\right)d\mu = \sum_{J\subset[n];J\ne\emptyset}(-1)^{|J|-1}\mu\left(\bigcap_{i\in J}A_{i}\right) $$ がわかります。これで包除原理の右辺が示されました。
次に賢い計算をしましょう。$1-1_{A}=1_{\overline{A}}$なので、 $$ 1-\prod_{i=1}^{n}(1-1_{A_{i}})=1-\prod_{i=1}^{n}1_{\overline{A_{i}}}=1-1_{\bigcap_{i=1}^{n}\overline{A_{i}}} =1_{\overline{\bigcap_{i=1}^{n}\overline{A_{i}}}} =1_{\bigcup_{i=1}^{n}A_{i}}$$ となります。最後にドモルガンの法則を用いました。 よって、 $$ \int_{X}\left(1-\prod_{i=1}^{n}(1-1_{A_{i}})\right)d\mu =\int_{X} 1_{\bigcup_{i=1}^{n}A_{i}}d\mu = \mu\left(\bigcup_{i=1}^{n}A_{i}\right) $$ となります。 これで包除原理の左辺が導かれ、包除原理が示されました。</description>
    </item>
    
    <item>
      <title>ガウス過程に関する文献</title>
      <link>https://yonesuke.github.io/posts/gp-articles/</link>
      <pubDate>Thu, 06 May 2021 12:53:12 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/gp-articles/</guid>
      <description>ガウス過程とその機械学習への応用に関する文献はたくさんあります。ここで特に有用だと思ったものを紹介していきます。
ガウス過程全般 ガウス過程と機械学習
ガウス過程と機械学習に関する和書です。非常にわかりやすい本でこの本を読めば一通りの実装はできるようになると思います。一冊目としてはこの本で間違いない気がします。
Gaussian Processes for Machine Learning
ガウス過程と機械学習への適用に関する代表的な本だと思います。PDFはオンラインで読むことができます。
ガウス過程の数学的な基礎に関する文献 Gaussian Processes and Kernel Methods: A Review on Connections and Equivalences
ガウス過程はカーネル法と切っても切り離せない関係にあります。この論文ではカーネル法との関係に関する結果を簡潔にまとめていて非常に参考になります。例えばRBFカーネルによって生成されるサンプルの関数が確率1で$C^{\infty}$級のなめらかな関数になることはよく知られていますが、その証明の流れもこの論文を読めばわかります。
Reproducing Kernel Hilbert Spaces in Probability and Statistics
再生核ヒルベルト空間(RKHS)に関する本です。ガウス過程によって出力される関数が属する空間について議論する際にRKHSが出てきます。この本ではRKHSに関する議論やそのガウス過程との関係についても議論されています。この本の一番最後にカーネル関数とその対応するRKHSの具体例が27個載っていて圧巻です。
補助変数法 データが$N$個あるときにガウス過程回帰を行うと、データ数に応じた逆行列計算が必要で$O(N^{3})$の計算量がかかってしまいます。 データ数が非常に多くなったときにはこの計算量は現実的ではないので補助変数法と呼ばれるものを用いて計算量を削減する試みが行われています。
A Unifying View of Sparse Approximate Gaussian Process Regression
2005年に出た論文ですが、その時までに出ていた補助変数法の様々な手法をまとめた論文になっています。上で紹介した「ガウス過程と機械学習」ではこの中のFITCと呼ばれる方法が紹介されています。
Gaussian processes for Big data
変分ベイズ法を用いてガウス過程回帰を行う手法についてまとまった論文です。補助変数の配置などを含めたハイパーパラメーターの最適化が可能になります。
深層学習との関係 Deep Neural Networks as Gaussian Processes
各層のunit数を無限に飛ばしたニューラルネットワークがガウス過程に対応する、という話です。中間層が一層の場合の話は昔に知られていましたが、多層の場合もそうだそうです。紹介しておきながら腰を据えて読んだことはないのでちゃんと読みたいです。</description>
    </item>
    
    <item>
      <title>LaTeXの設定</title>
      <link>https://yonesuke.github.io/posts/latex-setup/</link>
      <pubDate>Wed, 05 May 2021 18:46:29 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/latex-setup/</guid>
      <description>$\LaTeX$で文章を書くときによく行う設定をまとめておきます。
Overleaf Overleafで日本語で文章を書く場合は次のlatexmkrcというファイルをおいておきます。 1 2 3 4 5 $latex = &amp;#39;platex&amp;#39;; $bibtex = &amp;#39;pbibtex&amp;#39;; $dvipdf = &amp;#39;dvipdfmx %O -o %D %S&amp;#39;; $makeindex = &amp;#39;mendex -U %O -o %D %S&amp;#39;; $pdf_mode = 3; さらにCompilerはLaTeXにしておきましょう。 Beamer LaTeXでスライド生成する際に使われるbeamerです。テーマがたくさんあるので好みのものを選ぶと良いと思います。特にmetropolisとfocusというテーマが気に入っています。 1 2 3 4 5 6 7 \documentclass[dvipdfmx]{beamer} \usepackage{bxdpx-beamer} \usepackage{pxjahyper} \usepackage{minijs} \usetheme[numbering=fraction,block=fill,progressbar=frametitle]{metropolis} \usefonttheme{professionalfonts} \usepackage{appendixnumberbeamer} Appendixに移行する際の区切りのページには次を書くと便利です。 1 2 3 4 5 6 7 8 % スライド終わり \appendix \begin{frame}[standout] APPENDIX \end{frame} % アペンディクスはじまり % \begin{frame} % ... 便利Package hyperref 次のオプションとともに呼ぶと便利です。</description>
    </item>
    
    <item>
      <title>安定分布に従うノイズの生成方法</title>
      <link>https://yonesuke.github.io/posts/stable_simulations/</link>
      <pubDate>Wed, 21 Apr 2021 20:58:21 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/stable_simulations/</guid>
      <description>安定分布に従うノイズの生成方法について簡単にまとめておきます。 ここでは代表的なWeronの方法を用います。
Weronの方法 はじめに区間$(-\pi/2,\pi/2)$上の一様分布から乱数$V$を、平均$1$の指数分布に従う乱数$W$をそれぞれ生成します。
$\alpha\ne1$の場合、 $$ \begin{aligned} X=S_{\alpha,\beta}\times\frac{\sin(\alpha(V+B_{\alpha,\beta}))}{(\cos V)^{1/\alpha}}\times\left(\frac{\cos(V-\alpha(V+B_{\alpha,\beta}))}{W}\right)^{(1-\alpha)/\alpha} \end{aligned} $$ を計算します。ここで$B_{\alpha,\beta},S_{\alpha,\beta}$はそれぞれ、 $$\begin{aligned}B_{\alpha,\beta}&amp;amp;=\frac{\arctan(\beta\tan(\pi\alpha/2))}{\alpha},\\S_{\alpha,\beta}&amp;amp;=\left(1+\beta^{2}\tan^{2}(\pi\alpha/2)\right)^{1/(2\alpha)}\end{aligned}$$ です。
$\alpha=1$の場合には、 $$ X = \frac{2}{\pi}\left[(\frac{\pi}{2}+\beta V)\tan V - \beta\log\left(\frac{\frac{\pi}{2}W\cos V}{\frac{\pi}{2}+\beta V}\right)\right] $$ を計算します。
これより、$X\sim S(\alpha,\beta,1,0)$なる安定分布ノイズが得られます。 より一般に$S(\alpha,\beta,\gamma,\delta)$に従うノイズがほしければ$\gamma X+\delta$とすれば良いです。
補足 $\beta=0$の場合には$S(\alpha,0,1,0)$に従うノイズの生成式は一つにまとめられて、 $$ X=\frac{\sin(\alpha V)}{(\cos V)^{1/\alpha}}\times\left(\frac{\cos((1-\alpha)V)}{W}\right)^{(1-\alpha)/\alpha} $$ とすればよいです。
この式において更に$\alpha=1$にすれば$X=\tan V$となり、これは平均$0$、$\gamma=1$のCauchy分布に従うノイズの生成方法に一致します。
また、$\alpha=2$の場合には$P,Q\sim U(0,1)$として $$ X=-2\cos\pi P\sqrt{-\log Q} $$ となり、これは平均$0$、分散$2$の正規分布に従うノイズの生成方法であるBox–Muller法に一致します。</description>
    </item>
    
    <item>
      <title>多変量正規分布間のKL距離</title>
      <link>https://yonesuke.github.io/posts/kl-gaussian/</link>
      <pubDate>Mon, 19 Apr 2021 00:32:17 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/kl-gaussian/</guid>
      <description>確率分布の間の&amp;quot;近さ&amp;quot;を測る代表的なものとしてKL距離(Kullback–Leibler divergence)があります。特に多変量正規分布間のKL距離は変分下界を計算する際に登場することもあったりして応用上も重要です。その導出を行います。
準備 平均$\mu$、分散共分散行列$\Sigma$の$N$次元正規分布$\mathcal{N}(u\mid\mu,\Sigma)$の確率密度関数は $$ p(u)=\frac{1}{\sqrt{2\pi}^{N}\sqrt{| \Sigma |}}\exp\left[-\frac{1}{2}(u-\mu)^{\mathsf{T}}\Sigma^{-1}(u-\mu)\right] $$ で与えられます。 また、確率分布$q$に対する確率分布$p$のKL距離は $$ \text{KL}[p || q]=\int p(u)\log\frac{p(u)}{q(u)}du $$ で定義されます。
導出 2つの$N$次元正規分布$p(u)=\mathcal{N}(u \mid \mu _ {1},\Sigma _ {1}),q(u)=\mathcal{N}(u \mid \mu _ {2},\Sigma _ {2})$に対して$\text{KL}[p || q]$を計算します。 $\log$部分を展開すると、 $$ \log\frac{p(u)}{q(u)}=\frac{1}{2}\log\frac{|\Sigma _ {2}|}{|\Sigma _ {1}|} -\frac{1}{2}(u-\mu _ {1})^{\mathsf{T}}\Sigma _ {1}^{-1}(u-\mu _ {1}) + \frac{1}{2}(u-\mu _ {2})^{\mathsf{T}}\Sigma _ {2}^{-1}(u-\mu _ {2}) $$ と3つにわかれるので $$ \begin{aligned} \text{KL}[p || q] = &amp;amp;\frac{1}{2}\log\frac{|\Sigma _ {2}|}{|\Sigma _ {1}|}\int_{\mathbb{R}^{N}}p(u)du \\ &amp;amp; - \frac{1}{2}\int_{\mathbb{R}^{N}}(u-\mu _ {1})^{\mathsf{T}}\Sigma _ {1}^{-1}(u-\mu _ {1})p(u)du \\ &amp;amp; + \frac{1}{2}\int_{\mathbb{R}^{N}}(u-\mu _ {2})^{\mathsf{T}}\Sigma _ {2}^{-1}(u-\mu _ {2}) p(u)du \end{aligned} $$ とできます。各行を計算していきます。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://yonesuke.github.io/cactus_index/</link>
      <pubDate>Sun, 18 Apr 2021 21:20:05 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/cactus_index/</guid>
      <description>Ryosuke Yoneda Currently a Ph.D. student at the Department of Advanced Mathematical Sciences, Graduate School of Informatics, Kyoto University, Japan, from April 2020.
米田 亮介 Email: yoneda@acs.i.kyoto-u.ac.jp GitHub: yonesuke Twitter: @yonesuke1729 </description>
    </item>
    
    <item>
      <title>博士課程1年目を振り返る</title>
      <link>https://yonesuke.github.io/posts/d1/</link>
      <pubDate>Mon, 22 Mar 2021 00:14:18 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/d1/</guid>
      <description>2020年の4月から京都大学大学院情報学研究科に博士課程として入学してから1年が経ちました。 ここに博士課程1年目に起こったことをまとめておきます。 将来博士課程に進学する誰かの参考になれば良いなと思います。
僕は京都大学大学院情報学研究科先端数理科学専攻の非線形物理学講座という研究室に所属しています。 修士までは同じ情報学研究科の数理工学専攻の研究室に所属していて、博士課程から移ってきました。 主にリズム現象に関する研究を行っています。 特にリズム現象を記述する結合振動子系が示す同期転移を、力学系の手法を用いて理論解析をしたり、統計力学の手法を用いて数値計算をしたりしてきました。 最近は流行りに乗っかって機械学習の勉強も少ししようと思っています。
4月 新しい研究室に所属して頑張って研究をするぞ！！と思っていた矢先に新型コロナウイルスの感染拡大とそれに伴う緊急事態宣言が発令されました。 そのため、ずっと家に篭りっきりでした(みなさんもそうだったと思いますが)。 研究室のセミナー等はすべてzoomで行われることになりました。 これまで研究室で研究するタイプの人間で、家では勉強していなかったので、家で勉強とか研究をするのに慣れなくてあまり集中できませんでしたね。。。 あとしょぼい椅子に長時間座ることになって体がバキバキになりました。 椅子を奮発して買ったのですがこれが良かったです。こういうものには投資していくべきだな、と気付かされました。
研究の面では修論の結果を論文にまとめる作業をずっとしていました。 あとガウス過程の勉強をはじめました。 また、TAの業務がちょくちょくありました。貴重な収入源なのでありがたいです。
5月 この頃はずっと学振でしたね、、、辛かったです。 申請書を書くのがまじで向いていないなあ、と実感しました。 研究者たちは科研費を取るためにああいった申請書を書き続けているのだ、と思うと頭が上がりません。
5月の終わり頃に緊急事態宣言が解除されて、この頃からちょくちょく研究室に行くようになりました。
6月 6月のはじめに学振の締め切りがあって肩の荷が下りた感じがしました。 ただ、申請書でいうと民間奨学金の申請と学振を持っていない人対象のRA雇用の申請もあって、 6月の前半も申請書で潰れていた気がします。 (本当は気合を入れたらすぐできることだったと思うんですが、 頭の片隅に申請書がずっとちらついていて研究をしていてもあまり集中ができない感じでした。)
7月 修論の結果を論文にまとめる作業もだいぶ収束してきてarxivにアップロードしました。 (arxivにアップロードするとき、texファイルやらepsファイルやらを一個ずつポチポチしないといけないのはなんとかならないんですかね。。) また、論文をPREに投稿しました。投稿してから毎日のようにstatusのところを眺めていました。
学振も論文も終わったのでずっと積んでいたデスストランディングというゲームを一気に終わらせました。 めっちゃおもろかったです。
8月 色々なことが片付いたので、研究のことを一旦忘れて勉強に全振りしていた気がします。 特に関数解析の勉強を重点的に行いました。 この時期はやるべきことも特になくて平和だった気がします。
9月 9月は日本物理学会とRIMSの力学系研究集会での発表がありました。 あと7月に投稿した論文の査読も帰ってきて8月とは一転急に忙しくなりました。
9月の終わり頃に学振の結果が帰ってきて不採用でした。 もちろん残念だったんですが、思ったほど悲観的ではなかった気がします。 落ちるだろうなあ、と思いながら結果を待つようにしていたのと、 TA・RAやJASSOや民間の奨学金のおかげで死なない程度には生きていけていたからなのかなあ、と自己分析をしていました(笑)
10月 10月からは後期の授業が始まりました。様々なことが引き続きzoomで行われていました。 8月からちょくちょく関数解析を勉強していたおかげで読める(理解できる)論文が少し増えたような気がします。勉強は大事。
この後期から京都外国語大学で非常勤講師として授業を受け持つことになりました。張り切ってnotionでホームページ的なのを作ってみました。 こちらも授業はすべてオンラインでした。学生とのやり取りにはmicrosoftのteamsというサービスが使われていて、 それでオンラインの授業も行いました。 授業をオンラインでするのは初めてだったのですが、やはり学生の顔が見えないのでみんながどれくらい理解しているのかが全然わからなくて 結構難しかったです。「手を挙げる」機能を使って適宜わからないところがないかを学生に聞いてみたりしていました。 授業の準備(授業スライド、演習問題、解説などなど)で半日から一日近く潰れていた気がします。 研究者の方々も毎年のように授業の準備をしているのだ、と思うと頭が上がりません(2回目)。
11月 9月に帰ってきた論文の直しがようやく終わり、PREに投稿し直しました。 今度はめっちゃ返事が早くて、acceptが決まりました。 よく読んでいた雑誌なので通ったのは嬉しかったです。
関数解析の勉強をしたし、neural networkの万能近似定理の証明を読みました。元論文はCybenkoによるものです。 この論文はいい感じの活性化関数を使った3層neural netoworkにおいて、中間層のunit数が無限の極限では任意の連続関数に近づけることができることを証明しています。 せっかく読んだので研究室内で発表することにしました。5人くらい(?)参加してくれて、2週に分けて関数解析の基礎から万能近似定理の証明まで話すことにしました。 駆け足で説明したのでみんながわかってくれたかは分からないです(笑)
この時期はアメリカで大統領選挙が行われて、それに関連して(関連はしてないですが)Strogatz先生がtweetしたことが目に止まりました。 If anyone wants to distract themselves while the votes are being counted, or perhaps just catch up on some lost sleep, have a look at this lecture I gave to students at Cambridge University a few days ago: https://t.</description>
    </item>
    
    <item>
      <title>2020年度 数的理解</title>
      <link>https://yonesuke.github.io/teaching/2020-suutekirikai/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yonesuke.github.io/teaching/2020-suutekirikai/</guid>
      <description>授業内容・計画 問題演習を通して、基礎的な数学の知識を身につける演習を行う。特に、SPI試験などを受験する際に必要な数学的知識を身につけ、問題が解けるようになることを目指す。また、問題演習を通して、数学的な考え方や論理的な考え方を身につけることも目指す。
教科書 『最新最強のSPIクリア問題集』 成美堂出版編集部　著 (成美堂出版)
スケジュール 水曜4限 (15:40~17:20)
日付 授業内容 授業スライド 問題演習 第1回 10月7日 オリエンテーション Google スライド 割合 Google スライド 割合 PDF 第2回 10月14日 損益算 Google スライド 損益算 PDF 第3回 10月21日 順序、組み合わせ Google スライド 順序、組み合わせ PDF 第4回 10月28日 確率 Google スライド 確率 PDF 第5回 11月11日 方程式 Google スライド 方程式 PDF 第6回 11月25日 $n$進数 Google スライド $n$進数 PDF 第7回 12月2日 距離と時間と速さ Google スライド 距離と時間と速さ PDF 第8回 12月9日 濃度算 Google スライド 濃度算 PDF 第9回 12月16日 数列 Google スライド 数列 PDF 第10回 12月23日 関数とグラフ Google スライド 関数とグラフ PDF 第11回 1月6日 集合 Google スライド 集合 PDF 第12回 1月13日 論理 Google スライド 論理 PDF 第13回 1月20日 テスト演習と解説1 解答 PDF 第14回 1月27日 テスト演習と解説2 解答 PDF 成績評価 平常点 平常点は60点満点になります。第1回から第12回までの各授業で問題演習を提出した回数に5をかけた数が平常点になります。例えば12回すべての問題演習を提出した方の平常点は60点です。一方で12回のうち9回しか提出出来なかった方の平常点は45点です。</description>
    </item>
    
    <item>
      <title>2021年度 数的理解</title>
      <link>https://yonesuke.github.io/teaching/2021-suutekirikai/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yonesuke.github.io/teaching/2021-suutekirikai/</guid>
      <description>授業内容・計画 問題演習を通して、基礎的な数学の知識を身につける演習を行う。特に、SPI試験などを受験する際に必要な数学的知識を身につけ、問題が解けるようになることを目指す。また、問題演習を通して、数学的な考え方や論理的な考え方を身につけることも目指す。
教科書 『最新最強のSPIクリア問題集』 成美堂出版編集部 著 (成美堂出版)
講義動画 第5回以降の講義動画をYouTubeにアップロードしているので、復習に活用してください。
📹 2021年度 数的理解再生リスト
スケジュール 水曜4限 (15:40~17:20) 教室 R156 日付 授業内容 授業スライド 問題演習 講義動画 第1回 9月29日 オリエンテーション オリエンテーション PDF 割合 割合 PDF 割合 PDF 第2回 10月6日 損益算 損益算 PDF 損益算 PDF 第3回 10月13日 順序、組み合わせ 順列、組み合わせ PDF 順序、組み合わせ PDF 第4回 10月20日 確率 確率 PDF 確率 PDF 第5回 10月27日 方程式 方程式 PDF 方程式 PDF YouTube 第6回 11月10日 $n$進数 $n$進数 PDF $n$進数 PDF 第7回 11月17日 距離と時間と速さ 距離と時間と速さ PDF 距離と時間と速さ PDF YouTube 第8回 12月1日 濃度算 濃度算 PDF 濃度算 PDF YouTube 第9回 12月8日 数列 数列 PDF 数列 PDF YouTube 第10回 12月15日 関数とグラフ 関数とグラフ PDF 関数とグラフ PDF YouTube 第11回 12月22日 集合 集合 PDF 集合 PDF YouTube 第12回 1月5日 論理 論理 PDF 論理 PDF YouTube 第13回 1月12日 テスト演習と解説1 テスト問題 PDF 第14回 1月19日 テスト演習と解説2 テスト問題 PDF 成績評価 平常点 平常点は60点満点になります。第1回から第12回までの各授業で問題演習を提出した回数に5をかけた数が平常点になります。例えば12回すべての問題演習を提出した方の平常点は60点です。一方で12回のうち9回しか提出出来なかった方の平常点は45点です。 第6回を休講にしたため、11回分の授業で60点をつけることになりました。これに伴って、一回の授業で問題演習を提出すると$\frac{60}{11}=5.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://yonesuke.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yonesuke.github.io/about/</guid>
      <description>Education Ph.D. student April 2020 -
Nonlinear Physics Division, Department of Applied Mathematical Sciences, Graduate School of Informatics, Kyoto University.
Supervised by Toshio Aoyagi
M.S. April 2018 - March 2020
Dynamical Systems Group, Department of Applied Mathematics and Physics, Graduate School of Informatics, Kyoto University.
Supervised by Yoshiyuki Y. Yamaguchi
B.S. April 2014 - March 2018
Department of Informatics and Mathematical Science, Faculty of Engineering, Kyoto University.
Supervised by Yoshiyuki Y.</description>
    </item>
    
    <item>
      <title>Research</title>
      <link>https://yonesuke.github.io/research/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yonesuke.github.io/research/</guid>
      <description>Keywords Synchronization, Kuramoto model, Critical phenomena, Gaussian process
Publications Presentations 2022 米田亮介, 原田健自, &amp;ldquo;ニューラルネットワークを用いたスケーリング解析手法&amp;rdquo;, 日本物理学会 2022年年次大会, 2022年3月 (オンライン). 📂 PDF 樋口智英, 米田亮介, 小林望, 藤田政文, &amp;ldquo;デジタルおよびバリアオプションに対する Deep Hedging の学習性能の評価&amp;rdquo;, 日本金融・証券計量・工学学会 2021年冬季大会, 2022年2月 (オンライン). 2021 米田亮介, &amp;ldquo;結合振動子系において同期しない密なネットワークの探索&amp;rdquo;, RIMS研究集会「数理科学の諸問題と力学系理論の新展開」, 2021年6月 (オンライン). 米田亮介, 立川剛至, 寺前順之介, &amp;ldquo;結合振動子系において完全同期以外の安定平衡点を持つ密なネットワークの探索&amp;rdquo;, 日本物理学会 2021年年次大会, 2021年3月 (オンライン). 📂 PDF 米田亮介, &amp;ldquo;Lévyノイズを受けた大域結合振動子系の同期転移&amp;rdquo;, 第17回数学総合若手研究集会 ~数学の交叉点~, 北海道大学, 2021年3月 (オンライン). 2020 米田亮介, &amp;ldquo;ノイズを受けた大域結合振動子系の同期転移&amp;rdquo;, RIMS研究集会「数理科学の諸問題と力学系理論の新展開」, 2020年9月 (オンライン). 米田亮介, 原田健二, 山口義幸, &amp;ldquo;スモールワールドネットワーク上の結合位相振動子系における同期転移の臨界指数&amp;rdquo;, 日本物理学会 2020年秋季大会, 2020年9月 (オンライン). 米田亮介, “蔵本モデルにおける臨界指数”, 第16回数学総合若手研究集会 ~数学の交叉点~, 北海道大学, 2020年3月 (新型コロナウイルスの影響で中止).</description>
    </item>
    
    <item>
      <title>Teaching</title>
      <link>https://yonesuke.github.io/teaching/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yonesuke.github.io/teaching/</guid>
      <description>2022 2022年10月-2023年2月, 数的理解(短) (京都外国語大学・非常勤講師) 2021 2021年10月-2022年2月, 数的理解(短) (京都外国語大学・非常勤講師) ホームページ 2020 2020年10月-2021年2月, 数的理解(短) (京都外国語大学・非常勤講師) ホームページ ここに作り直しました Teaching Assistant 2021 2021年4月-8月, 物理学基礎論A (京都大学工学部) 2021年4月-8月, 非線形動力学 (京都大学工学部) 2020 2020年10月-2021年2月, 解析力学 (京都大学工学部) 2020年4月-8月, 基礎数理演習 (京都大学工学部) 2020年4月-8月, 非線形動力学 (京都大学工学部) 2019 2019年10月-2020年2月, 工業数学A1 (京都大学工学部) 2019年4月-8月, 基礎数理演習 (京都大学工学部) 2018 2018年10月-2019年3月, 微分積分学続論II (京都大学工学部) 2018年4月-8月, 基礎数理演習 (京都大学工学部) 2018年4月-8月, 工業数学A3 (京都大学工学部) </description>
    </item>
    
  </channel>
</rss>
