<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Ryosuke Yoneda&#39;s Homepage</title>
    <link>https://yonesuke.github.io/posts/</link>
    <description>Recent content in Posts on Ryosuke Yoneda&#39;s Homepage</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Ryosuke Yoneda</copyright>
    <lastBuildDate>Wed, 21 Apr 2021 20:58:21 +0900</lastBuildDate><atom:link href="https://yonesuke.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>安定分布に従うノイズの生成方法</title>
      <link>https://yonesuke.github.io/posts/stable_simulations/</link>
      <pubDate>Wed, 21 Apr 2021 20:58:21 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/stable_simulations/</guid>
      <description>安定分布に従うノイズの生成方法について簡単にまとめておきます。 ここでは代表的なWeronの方法を用います。
Weronの方法 はじめに区間$(-\pi/2,\pi/2)$上の一様分布から乱数$V$を、平均$1$の指数分布に従う乱数$W$をそれぞれ生成します。
  $\alpha\ne1$の場合、 $$ \begin{aligned} X=S_{\alpha,\beta}\times\frac{\sin(\alpha(V+B_{\alpha,\beta}))}{(\cos V)^{1/\alpha}}\times\left(\frac{\cos(V-\alpha(V+B_{\alpha,\beta}))}{W}\right)^{(1-\alpha)/\alpha} \end{aligned} $$ を計算します。ここで$B_{\alpha,\beta},S_{\alpha,\beta}$はそれぞれ、 $$\begin{aligned}B_{\alpha,\beta}&amp;amp;=\frac{\arctan(\beta\tan(\pi\alpha/2))}{\alpha},\\S_{\alpha,\beta}&amp;amp;=\left(1+\beta^{2}\tan^{2}(\pi\alpha/2)\right)^{1/(2\alpha)}\end{aligned}$$ です。
  $\alpha=1$の場合には、 $$ X = \frac{2}{\pi}\left[(\frac{\pi}{2}+\beta V)\tan V - \beta\log\left(\frac{\frac{\pi}{2}W\cos V}{\frac{\pi}{2}+\beta V}\right)\right] $$ を計算します。
  これより、$X\sim S(\alpha,\beta,1,0)$なる安定分布ノイズが得られます。 より一般に$S(\alpha,\beta,\gamma,\delta)$に従うノイズがほしければ$\gamma X+\delta$とすれば良いです。
補足 $\beta=0$の場合には$S(\alpha,0,1,0)$に従うノイズの生成式は一つにまとめられて、 $$ X=\frac{\sin(\alpha V)}{(\cos V)^{1/\alpha}}\times\left(\frac{\cos((1-\alpha)V)}{W}\right)^{(1-\alpha)/\alpha} $$ とすればよいです。
この式において更に$\alpha=1$にすれば$X=\tan V$となり、これは平均$0$、$\gamma=1$のCauchy分布に従うノイズの生成方法に一致します。
また、$\alpha=2$の場合には$P,Q\sim U(0,1)$として $$ X=-2\cos\pi P\sqrt{-\log Q} $$ となり、これは平均$0$、分散$2$の正規分布に従うノイズの生成方法であるBox–Muller法に一致します。</description>
    </item>
    
    <item>
      <title>多変量正規分布間のKL距離</title>
      <link>https://yonesuke.github.io/posts/kl-gaussian/</link>
      <pubDate>Mon, 19 Apr 2021 00:32:17 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/kl-gaussian/</guid>
      <description>確率分布の間の&amp;quot;近さ&amp;quot;を測る代表的なものとしてKL距離(Kullback–Leibler divergence)があります。特に多変量正規分布間のKL距離は変分下界を計算する際に登場することもあったりして応用上も重要です。その導出を行います。
準備 平均$\mu$、分散共分散行列$\Sigma$の$N$次元正規分布$\mathcal{N}(u\mid\mu,\Sigma)$の確率密度関数は $$ p(u)=\frac{1}{\sqrt{2\pi}^{N}\sqrt{| \Sigma |}}\exp\left[-\frac{1}{2}(u-\mu)^{\mathsf{T}}\Sigma^{-1}(u-\mu)\right] $$ で与えられます。 また、確率分布$q$に対する確率分布$p$のKL距離は $$ \text{KL}[p || q]=\int p(u)\log\frac{p(u)}{q(u)}du $$ で定義されます。
導出 2つの$N$次元正規分布$p(u)=\mathcal{N}(u \mid \mu _ {1},\Sigma _ {1}),q(u)=\mathcal{N}(u \mid \mu _ {2},\Sigma _ {2})$に対して$\text{KL}[p || q]$を計算します。 $\log$部分を展開すると、 $$ \log\frac{p(u)}{q(u)}=\frac{1}{2}\log\frac{|\Sigma _ {2}|}{|\Sigma _ {1}|} -\frac{1}{2}(u-\mu _ {1})^{\mathsf{T}}\Sigma _ {1}^{-1}(u-\mu _ {1}) + \frac{1}{2}(u-\mu _ {2})^{\mathsf{T}}\Sigma _ {2}^{-1}(u-\mu _ {2}) $$ と3つにわかれるので $$ \begin{aligned} \text{KL}[p || q] = &amp;amp;\frac{1}{2}\log\frac{|\Sigma _ {2}|}{|\Sigma _ {1}|}\int_{\mathbb{R}^{N}}p(u)du \\ &amp;amp; - \frac{1}{2}\int_{\mathbb{R}^{N}}(u-\mu _ {1})^{\mathsf{T}}\Sigma _ {1}^{-1}(u-\mu _ {1})p(u)du \\ &amp;amp; + \frac{1}{2}\int_{\mathbb{R}^{N}}(u-\mu _ {2})^{\mathsf{T}}\Sigma _ {2}^{-1}(u-\mu _ {2}) p(u)du \end{aligned} $$ とできます。各行を計算していきます。</description>
    </item>
    
    <item>
      <title>博士課程1年目を振り返る</title>
      <link>https://yonesuke.github.io/posts/d1/</link>
      <pubDate>Mon, 22 Mar 2021 00:14:18 +0900</pubDate>
      
      <guid>https://yonesuke.github.io/posts/d1/</guid>
      <description>2020年の4月から京都大学大学院情報学研究科に博士課程として入学してから1年が経ちました。 ここに博士課程1年目に起こったことをまとめておきます。 将来博士課程に進学する誰かの参考になれば良いなと思います。
僕は京都大学大学院情報学研究科先端数理科学専攻の非線形物理学講座という研究室に所属しています。 修士までは同じ情報学研究科の数理工学専攻の研究室に所属していて、博士課程から移ってきました。 主にリズム現象に関する研究を行っています。 特にリズム現象を記述する結合振動子系が示す同期転移を、力学系の手法を用いて理論解析をしたり、統計力学の手法を用いて数値計算をしたりしてきました。 最近は流行りに乗っかって機械学習の勉強も少ししようと思っています。
4月 新しい研究室に所属して頑張って研究をするぞ！！と思っていた矢先に新型コロナウイルスの感染拡大とそれに伴う緊急事態宣言が発令されました。 そのため、ずっと家に篭りっきりでした(みなさんもそうだったと思いますが)。 研究室のセミナー等はすべてzoomで行われることになりました。 これまで研究室で研究するタイプの人間で、家では勉強していなかったので、家で勉強とか研究をするのに慣れなくてあまり集中できませんでしたね。。。 あとしょぼい椅子に長時間座ることになって体がバキバキになりました。 椅子を奮発して買ったのですがこれが良かったです。こういうものには投資していくべきだな、と気付かされました。
研究の面では修論の結果を論文にまとめる作業をずっとしていました。 あとガウス過程の勉強をはじめました。 また、TAの業務がちょくちょくありました。貴重な収入源なのでありがたいです。
5月 この頃はずっと学振でしたね、、、辛かったです。 申請書を書くのがまじで向いていないなあ、と実感しました。 研究者たちは科研費を取るためにああいった申請書を書き続けているのだ、と思うと頭が上がりません。
5月の終わり頃に緊急事態宣言が解除されて、この頃からちょくちょく研究室に行くようになりました。
6月 6月のはじめに学振の締め切りがあって肩の荷が下りた感じがしました。 ただ、申請書でいうと民間奨学金の申請と学振を持っていない人対象のRA雇用の申請もあって、 6月の前半も申請書で潰れていた気がします。 (本当は気合を入れたらすぐできることだったと思うんですが、 頭の片隅に申請書がずっとちらついていて研究をしていてもあまり集中ができない感じでした。)
7月 修論の結果を論文にまとめる作業もだいぶ収束してきてarxivにアップロードしました。 (arxivにアップロードするとき、texファイルやらepsファイルやらを一個ずつポチポチしないといけないのはなんとかならないんですかね。。) また、論文をPREに投稿しました。投稿してから毎日のようにstatusのところを眺めていました。
学振も論文も終わったのでずっと積んでいたデスストランディングというゲームを一気に終わらせました。 めっちゃおもろかったです。
8月 色々なことが片付いたので、研究のことを一旦忘れて勉強に全振りしていた気がします。 特に関数解析の勉強を重点的に行いました。 この時期はやるべきことも特になくて平和だった気がします。
9月 9月は日本物理学会とRIMSの力学系研究集会での発表がありました。 あと7月に投稿した論文の査読も帰ってきて8月とは一転急に忙しくなりました。
9月の終わり頃に学振の結果が帰ってきて不採用でした。 もちろん残念だったんですが、思ったほど悲観的ではなかった気がします。 落ちるだろうなあ、と思いながら結果を待つようにしていたのと、 TA・RAやJASSOや民間の奨学金のおかげで死なない程度には生きていけていたからなのかなあ、と自己分析をしていました(笑)
10月 10月からは後期の授業が始まりました。様々なことが引き続きzoomで行われていました。 8月からちょくちょく関数解析を勉強していたおかげで読める(理解できる)論文が少し増えたような気がします。勉強は大事。
この後期から京都外国語大学で非常勤講師として授業を受け持つことになりました。張り切ってnotionでホームページ的なのを作ってみました。 こちらも授業はすべてオンラインでした。学生とのやり取りにはmicrosoftのteamsというサービスが使われていて、 それでオンラインの授業も行いました。 授業をオンラインでするのは初めてだったのですが、やはり学生の顔が見えないのでみんながどれくらい理解しているのかが全然わからなくて 結構難しかったです。「手を挙げる」機能を使って適宜わからないところがないかを学生に聞いてみたりしていました。 授業の準備(授業スライド、演習問題、解説などなど)で半日から一日近く潰れていた気がします。 研究者の方々も毎年のように授業の準備をしているのだ、と思うと頭が上がりません(2回目)。
11月 9月に帰ってきた論文の直しがようやく終わり、PREに投稿し直しました。 今度はめっちゃ返事が早くて、acceptが決まりました。 よく読んでいた雑誌なので通ったのは嬉しかったです。
関数解析の勉強をしたし、neural networkの万能近似定理の証明を読みました。元論文はCybenkoによるものです。 この論文はいい感じの活性化関数を使った3層neural netoworkにおいて、中間層のunit数が無限の極限では任意の連続関数に近づけることができることを証明しています。 せっかく読んだので研究室内で発表することにしました。5人くらい(?)参加してくれて、2週に分けて関数解析の基礎から万能近似定理の証明まで話すことにしました。 駆け足で説明したのでみんながわかってくれたかは分からないです(笑)
この時期はアメリカで大統領選挙が行われて、それに関連して(関連はしてないですが)Strogatz先生がtweetしたことが目に止まりました。 If anyone wants to distract themselves while the votes are being counted, or perhaps just catch up on some lost sleep, have a look at this lecture I gave to students at Cambridge University a few days ago: https://t.</description>
    </item>
    
  </channel>
</rss>
