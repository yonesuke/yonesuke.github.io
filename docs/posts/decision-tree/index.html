<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>決定木を1から実装する | Ryosuke Yoneda</title>
<meta name="keywords" content="">
<meta name="description" content="決定木って名前はよく聞くしscikit-learnで簡単に使えてしまうけど、中身を詳しく知っているわけではなかったのできちんと実装してみることにする。 from scratchでの実装にはこの記事が非常に参考になった。
https://towardsdatascience.com/implementing-a-decision-tree-from-scratch-f5358ff9c4bb https://darden.hatenablog.com/entry/2016/12/15/222447 決定木 まず決定木の仕組みについて説明する。 とは言っても懇切丁寧に説明している記事はほかにいくらでもあるので、ここではざっくりとした説明に留める。
アルゴリズム 決定木における分類問題においては分類を行いたい対象に対する特徴量がn_features個与えられており、各特徴量は数で表現されるとする。
例えば、sklearn内に格納されているirisデータセットには4個の特徴量が用意されている。具体的には、sepal length, sepal width, petal length, petal widthであり、いずれも単位はcmの数になっている。 このとき、決定木は与えられた特徴量たちに対して「特徴量Aは$x$より小さいか？」という質問を行う。この質問に対する答えがYesならば、次に「特徴量Bは$y$より小さいか？」という質問を、答えがNoならば、「特徴量Cは$z$より小さいか？」という質問を行う。 こういった質問を次々に行っていき、最後に「前の質問の答えがYesならば分類クラスはPである」といった具合に分類を行う。これが決定木の一つの大きな流れである。
質問によって分岐が起きていくので、最初の質問を根とする木とみなすことができる。 この木のもとでは各質問は木のノードと捉えられ、また分岐の果てに分類問題の答えを出力する所は葉とみなすことができる。
学習 上で説明した決定木アルゴリズムにおいて、各ノードに配置する「特徴量Aは$x$より小さいか？」という質問を作成する必要がある。これを学習データをもとに決定していくCARTアルゴリズムを紹介する。
学習の流れは各ノードを根から順に決定していく流れとなるので、再帰的な構造を持つ。 学習データが質問を経てあるノードに到着した段階でn_samples個になったとしよう。 このとき、学習データがそれぞれ持つ分類クラスが質問を通して最も別れるような質問が最適な質問であると考える。明日晴れるか雨が降るかを知りたいときに、質問にYesと答えれば必ず晴れ、Noと答えれば必ず雨、ということが分かれば、これは最適な質問である、ということである。 これを定量的に評価する方法として、情報利得を用いることを考える。 n_samples個の学習データのそれぞれのクラスが格納された$y\in\mathbb{R}^{n_\mathrm{samples}}$に対して、クラスの揃い具合を表す情報量$f\colon\mathbb{R}^{N}\to\mathbb{R}$があったとしよう。 この指標は値が小さいほど学習データの分類クラスは揃っていて、値が大きいほどに学習データの分類クラスは全くもってばらけている状況を想定する。 ある質問を通して、n_left個の学習データがYesと、n_right個の学習データがNoと答えたとしよう。また、これに応じて分類クラス$y$は$y_{\mathrm{left}}$と$y_{\mathrm{right}}$に分けられるとする。 学習データが入力された段階では、情報量は$f(y)$である。 また、質問にYesと答えたときの情報量は$f(y_{\mathrm{left}})$、Noと答えたときの情報量は$f(y_{\mathrm{right}})$となる。 確率的には質問に答えた後の情報量は$\frac{n_{\mathrm{left}}}{n_{\mathrm{samples}}}f(y_{\mathrm{left}}) &#43; \frac{n_{\mathrm{right}}}{n_{\mathrm{samples}}}f(y_{\mathrm{right}})$となる。よって、質問に答える前後で情報利得 $$ f(y) - \left(\frac{n_{\mathrm{left}}}{n_{\mathrm{samples}}}f(y_{\mathrm{left}}) &#43; \frac{n_{\mathrm{right}}}{n_{\mathrm{samples}}}f(y_{\mathrm{right}})\right) $$ が最大となるような質問が最適である、と考える。 具体的な実装においては、各ノードに対して学習データを入力するたびごとに、まず特徴量を一つ選ぶ。学習データにわたってこの特徴量の値を閾値として情報利得を計算する。これをすべての特徴量と閾値に対して行い、最大の情報利得を取る特徴量と閾値でもってそのノードの質問を確定する、 という流れを取る。
分類クラスの揃い具合を表す代表的な指標として、エントロピーとジニ不純度が ある。sklearnはデフォルトでジニ不純度を指定するようである。
決定木の実装 はじめに、ノードの実装を行う。 各ノードは質問を決定する特徴量(のインデックス)self.featureとその閾値self.thresholdを持つ。 これにより定まる質問に対して答えがYesであれば左側の子ノードself.leftを、 Noであれば右側の子ノードselft.rightを参照するようにする。 ただし、葉ノードである場合には、その特徴量(のインデックス)を保管するようにする。
1 2 3 4 5 6 7 8 9 10 class Node: def __init__(self, feature=None, threshold=None, left=None, right=None, value=None): self.">
<meta name="author" content="Ryosuke Yoneda">
<link rel="canonical" href="https://yonesuke.github.io/posts/decision-tree/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css" integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe&#43;FVUFzPh7U=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://yonesuke.github.io/favicon_io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://yonesuke.github.io/favicon_io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://yonesuke.github.io/favicon_io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://yonesuke.github.io/favicon_io/apple-touch-icon.png">
<link rel="mask-icon" href="https://yonesuke.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css" integrity="sha384-Xi8rHCmBmhbuyyhbI88391ZKP2dmfnOl4rT9ZfRI7mLTdk1wblIUnrIq35nqwEvC" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js" integrity="sha384-X/XCfMm41VSsqRNQgDerQczD69XqmjOOOwYQvr/uuC+j4OPoNhVgjdGFwhvN02Ja" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>


<link rel="stylesheet" href="https://yonesuke.github.io/css/theoremproof.css">
<meta property="og:title" content="決定木を1から実装する" />
<meta property="og:description" content="決定木って名前はよく聞くしscikit-learnで簡単に使えてしまうけど、中身を詳しく知っているわけではなかったのできちんと実装してみることにする。 from scratchでの実装にはこの記事が非常に参考になった。
https://towardsdatascience.com/implementing-a-decision-tree-from-scratch-f5358ff9c4bb https://darden.hatenablog.com/entry/2016/12/15/222447 決定木 まず決定木の仕組みについて説明する。 とは言っても懇切丁寧に説明している記事はほかにいくらでもあるので、ここではざっくりとした説明に留める。
アルゴリズム 決定木における分類問題においては分類を行いたい対象に対する特徴量がn_features個与えられており、各特徴量は数で表現されるとする。
例えば、sklearn内に格納されているirisデータセットには4個の特徴量が用意されている。具体的には、sepal length, sepal width, petal length, petal widthであり、いずれも単位はcmの数になっている。 このとき、決定木は与えられた特徴量たちに対して「特徴量Aは$x$より小さいか？」という質問を行う。この質問に対する答えがYesならば、次に「特徴量Bは$y$より小さいか？」という質問を、答えがNoならば、「特徴量Cは$z$より小さいか？」という質問を行う。 こういった質問を次々に行っていき、最後に「前の質問の答えがYesならば分類クラスはPである」といった具合に分類を行う。これが決定木の一つの大きな流れである。
質問によって分岐が起きていくので、最初の質問を根とする木とみなすことができる。 この木のもとでは各質問は木のノードと捉えられ、また分岐の果てに分類問題の答えを出力する所は葉とみなすことができる。
学習 上で説明した決定木アルゴリズムにおいて、各ノードに配置する「特徴量Aは$x$より小さいか？」という質問を作成する必要がある。これを学習データをもとに決定していくCARTアルゴリズムを紹介する。
学習の流れは各ノードを根から順に決定していく流れとなるので、再帰的な構造を持つ。 学習データが質問を経てあるノードに到着した段階でn_samples個になったとしよう。 このとき、学習データがそれぞれ持つ分類クラスが質問を通して最も別れるような質問が最適な質問であると考える。明日晴れるか雨が降るかを知りたいときに、質問にYesと答えれば必ず晴れ、Noと答えれば必ず雨、ということが分かれば、これは最適な質問である、ということである。 これを定量的に評価する方法として、情報利得を用いることを考える。 n_samples個の学習データのそれぞれのクラスが格納された$y\in\mathbb{R}^{n_\mathrm{samples}}$に対して、クラスの揃い具合を表す情報量$f\colon\mathbb{R}^{N}\to\mathbb{R}$があったとしよう。 この指標は値が小さいほど学習データの分類クラスは揃っていて、値が大きいほどに学習データの分類クラスは全くもってばらけている状況を想定する。 ある質問を通して、n_left個の学習データがYesと、n_right個の学習データがNoと答えたとしよう。また、これに応じて分類クラス$y$は$y_{\mathrm{left}}$と$y_{\mathrm{right}}$に分けられるとする。 学習データが入力された段階では、情報量は$f(y)$である。 また、質問にYesと答えたときの情報量は$f(y_{\mathrm{left}})$、Noと答えたときの情報量は$f(y_{\mathrm{right}})$となる。 確率的には質問に答えた後の情報量は$\frac{n_{\mathrm{left}}}{n_{\mathrm{samples}}}f(y_{\mathrm{left}}) &#43; \frac{n_{\mathrm{right}}}{n_{\mathrm{samples}}}f(y_{\mathrm{right}})$となる。よって、質問に答える前後で情報利得 $$ f(y) - \left(\frac{n_{\mathrm{left}}}{n_{\mathrm{samples}}}f(y_{\mathrm{left}}) &#43; \frac{n_{\mathrm{right}}}{n_{\mathrm{samples}}}f(y_{\mathrm{right}})\right) $$ が最大となるような質問が最適である、と考える。 具体的な実装においては、各ノードに対して学習データを入力するたびごとに、まず特徴量を一つ選ぶ。学習データにわたってこの特徴量の値を閾値として情報利得を計算する。これをすべての特徴量と閾値に対して行い、最大の情報利得を取る特徴量と閾値でもってそのノードの質問を確定する、 という流れを取る。
分類クラスの揃い具合を表す代表的な指標として、エントロピーとジニ不純度が ある。sklearnはデフォルトでジニ不純度を指定するようである。
決定木の実装 はじめに、ノードの実装を行う。 各ノードは質問を決定する特徴量(のインデックス)self.featureとその閾値self.thresholdを持つ。 これにより定まる質問に対して答えがYesであれば左側の子ノードself.leftを、 Noであれば右側の子ノードselft.rightを参照するようにする。 ただし、葉ノードである場合には、その特徴量(のインデックス)を保管するようにする。
1 2 3 4 5 6 7 8 9 10 class Node: def __init__(self, feature=None, threshold=None, left=None, right=None, value=None): self." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yonesuke.github.io/posts/decision-tree/" /><meta property="og:image" content="https://yonesuke.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-25T10:00:06+09:00" />
<meta property="article:modified_time" content="2023-06-25T10:00:06+09:00" /><meta property="og:site_name" content="Ryosuke Yoneda" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://yonesuke.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"/>

<meta name="twitter:title" content="決定木を1から実装する"/>
<meta name="twitter:description" content="決定木って名前はよく聞くしscikit-learnで簡単に使えてしまうけど、中身を詳しく知っているわけではなかったのできちんと実装してみることにする。 from scratchでの実装にはこの記事が非常に参考になった。
https://towardsdatascience.com/implementing-a-decision-tree-from-scratch-f5358ff9c4bb https://darden.hatenablog.com/entry/2016/12/15/222447 決定木 まず決定木の仕組みについて説明する。 とは言っても懇切丁寧に説明している記事はほかにいくらでもあるので、ここではざっくりとした説明に留める。
アルゴリズム 決定木における分類問題においては分類を行いたい対象に対する特徴量がn_features個与えられており、各特徴量は数で表現されるとする。
例えば、sklearn内に格納されているirisデータセットには4個の特徴量が用意されている。具体的には、sepal length, sepal width, petal length, petal widthであり、いずれも単位はcmの数になっている。 このとき、決定木は与えられた特徴量たちに対して「特徴量Aは$x$より小さいか？」という質問を行う。この質問に対する答えがYesならば、次に「特徴量Bは$y$より小さいか？」という質問を、答えがNoならば、「特徴量Cは$z$より小さいか？」という質問を行う。 こういった質問を次々に行っていき、最後に「前の質問の答えがYesならば分類クラスはPである」といった具合に分類を行う。これが決定木の一つの大きな流れである。
質問によって分岐が起きていくので、最初の質問を根とする木とみなすことができる。 この木のもとでは各質問は木のノードと捉えられ、また分岐の果てに分類問題の答えを出力する所は葉とみなすことができる。
学習 上で説明した決定木アルゴリズムにおいて、各ノードに配置する「特徴量Aは$x$より小さいか？」という質問を作成する必要がある。これを学習データをもとに決定していくCARTアルゴリズムを紹介する。
学習の流れは各ノードを根から順に決定していく流れとなるので、再帰的な構造を持つ。 学習データが質問を経てあるノードに到着した段階でn_samples個になったとしよう。 このとき、学習データがそれぞれ持つ分類クラスが質問を通して最も別れるような質問が最適な質問であると考える。明日晴れるか雨が降るかを知りたいときに、質問にYesと答えれば必ず晴れ、Noと答えれば必ず雨、ということが分かれば、これは最適な質問である、ということである。 これを定量的に評価する方法として、情報利得を用いることを考える。 n_samples個の学習データのそれぞれのクラスが格納された$y\in\mathbb{R}^{n_\mathrm{samples}}$に対して、クラスの揃い具合を表す情報量$f\colon\mathbb{R}^{N}\to\mathbb{R}$があったとしよう。 この指標は値が小さいほど学習データの分類クラスは揃っていて、値が大きいほどに学習データの分類クラスは全くもってばらけている状況を想定する。 ある質問を通して、n_left個の学習データがYesと、n_right個の学習データがNoと答えたとしよう。また、これに応じて分類クラス$y$は$y_{\mathrm{left}}$と$y_{\mathrm{right}}$に分けられるとする。 学習データが入力された段階では、情報量は$f(y)$である。 また、質問にYesと答えたときの情報量は$f(y_{\mathrm{left}})$、Noと答えたときの情報量は$f(y_{\mathrm{right}})$となる。 確率的には質問に答えた後の情報量は$\frac{n_{\mathrm{left}}}{n_{\mathrm{samples}}}f(y_{\mathrm{left}}) &#43; \frac{n_{\mathrm{right}}}{n_{\mathrm{samples}}}f(y_{\mathrm{right}})$となる。よって、質問に答える前後で情報利得 $$ f(y) - \left(\frac{n_{\mathrm{left}}}{n_{\mathrm{samples}}}f(y_{\mathrm{left}}) &#43; \frac{n_{\mathrm{right}}}{n_{\mathrm{samples}}}f(y_{\mathrm{right}})\right) $$ が最大となるような質問が最適である、と考える。 具体的な実装においては、各ノードに対して学習データを入力するたびごとに、まず特徴量を一つ選ぶ。学習データにわたってこの特徴量の値を閾値として情報利得を計算する。これをすべての特徴量と閾値に対して行い、最大の情報利得を取る特徴量と閾値でもってそのノードの質問を確定する、 という流れを取る。
分類クラスの揃い具合を表す代表的な指標として、エントロピーとジニ不純度が ある。sklearnはデフォルトでジニ不純度を指定するようである。
決定木の実装 はじめに、ノードの実装を行う。 各ノードは質問を決定する特徴量(のインデックス)self.featureとその閾値self.thresholdを持つ。 これにより定まる質問に対して答えがYesであれば左側の子ノードself.leftを、 Noであれば右側の子ノードselft.rightを参照するようにする。 ただし、葉ノードである場合には、その特徴量(のインデックス)を保管するようにする。
1 2 3 4 5 6 7 8 9 10 class Node: def __init__(self, feature=None, threshold=None, left=None, right=None, value=None): self."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://yonesuke.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "決定木を1から実装する",
      "item": "https://yonesuke.github.io/posts/decision-tree/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "決定木を1から実装する",
  "name": "決定木を1から実装する",
  "description": "決定木って名前はよく聞くしscikit-learnで簡単に使えてしまうけど、中身を詳しく知っているわけではなかったのできちんと実装してみることにする。 from scratchでの実装にはこの記事が非常に参考になった。\nhttps://towardsdatascience.com/implementing-a-decision-tree-from-scratch-f5358ff9c4bb https://darden.hatenablog.com/entry/2016/12/15/222447 決定木 まず決定木の仕組みについて説明する。 とは言っても懇切丁寧に説明している記事はほかにいくらでもあるので、ここではざっくりとした説明に留める。\nアルゴリズム 決定木における分類問題においては分類を行いたい対象に対する特徴量がn_features個与えられており、各特徴量は数で表現されるとする。\n例えば、sklearn内に格納されているirisデータセットには4個の特徴量が用意されている。具体的には、sepal length, sepal width, petal length, petal widthであり、いずれも単位はcmの数になっている。 このとき、決定木は与えられた特徴量たちに対して「特徴量Aは$x$より小さいか？」という質問を行う。この質問に対する答えがYesならば、次に「特徴量Bは$y$より小さいか？」という質問を、答えがNoならば、「特徴量Cは$z$より小さいか？」という質問を行う。 こういった質問を次々に行っていき、最後に「前の質問の答えがYesならば分類クラスはPである」といった具合に分類を行う。これが決定木の一つの大きな流れである。\n質問によって分岐が起きていくので、最初の質問を根とする木とみなすことができる。 この木のもとでは各質問は木のノードと捉えられ、また分岐の果てに分類問題の答えを出力する所は葉とみなすことができる。\n学習 上で説明した決定木アルゴリズムにおいて、各ノードに配置する「特徴量Aは$x$より小さいか？」という質問を作成する必要がある。これを学習データをもとに決定していくCARTアルゴリズムを紹介する。\n学習の流れは各ノードを根から順に決定していく流れとなるので、再帰的な構造を持つ。 学習データが質問を経てあるノードに到着した段階でn_samples個になったとしよう。 このとき、学習データがそれぞれ持つ分類クラスが質問を通して最も別れるような質問が最適な質問であると考える。明日晴れるか雨が降るかを知りたいときに、質問にYesと答えれば必ず晴れ、Noと答えれば必ず雨、ということが分かれば、これは最適な質問である、ということである。 これを定量的に評価する方法として、情報利得を用いることを考える。 n_samples個の学習データのそれぞれのクラスが格納された$y\\in\\mathbb{R}^{n_\\mathrm{samples}}$に対して、クラスの揃い具合を表す情報量$f\\colon\\mathbb{R}^{N}\\to\\mathbb{R}$があったとしよう。 この指標は値が小さいほど学習データの分類クラスは揃っていて、値が大きいほどに学習データの分類クラスは全くもってばらけている状況を想定する。 ある質問を通して、n_left個の学習データがYesと、n_right個の学習データがNoと答えたとしよう。また、これに応じて分類クラス$y$は$y_{\\mathrm{left}}$と$y_{\\mathrm{right}}$に分けられるとする。 学習データが入力された段階では、情報量は$f(y)$である。 また、質問にYesと答えたときの情報量は$f(y_{\\mathrm{left}})$、Noと答えたときの情報量は$f(y_{\\mathrm{right}})$となる。 確率的には質問に答えた後の情報量は$\\frac{n_{\\mathrm{left}}}{n_{\\mathrm{samples}}}f(y_{\\mathrm{left}}) + \\frac{n_{\\mathrm{right}}}{n_{\\mathrm{samples}}}f(y_{\\mathrm{right}})$となる。よって、質問に答える前後で情報利得 $$ f(y) - \\left(\\frac{n_{\\mathrm{left}}}{n_{\\mathrm{samples}}}f(y_{\\mathrm{left}}) + \\frac{n_{\\mathrm{right}}}{n_{\\mathrm{samples}}}f(y_{\\mathrm{right}})\\right) $$ が最大となるような質問が最適である、と考える。 具体的な実装においては、各ノードに対して学習データを入力するたびごとに、まず特徴量を一つ選ぶ。学習データにわたってこの特徴量の値を閾値として情報利得を計算する。これをすべての特徴量と閾値に対して行い、最大の情報利得を取る特徴量と閾値でもってそのノードの質問を確定する、 という流れを取る。\n分類クラスの揃い具合を表す代表的な指標として、エントロピーとジニ不純度が ある。sklearnはデフォルトでジニ不純度を指定するようである。\n決定木の実装 はじめに、ノードの実装を行う。 各ノードは質問を決定する特徴量(のインデックス)self.featureとその閾値self.thresholdを持つ。 これにより定まる質問に対して答えがYesであれば左側の子ノードself.leftを、 Noであれば右側の子ノードselft.rightを参照するようにする。 ただし、葉ノードである場合には、その特徴量(のインデックス)を保管するようにする。\n1 2 3 4 5 6 7 8 9 10 class Node: def __init__(self, feature=None, threshold=None, left=None, right=None, value=None): self.",
  "keywords": [
    
  ],
  "articleBody": "決定木って名前はよく聞くしscikit-learnで簡単に使えてしまうけど、中身を詳しく知っているわけではなかったのできちんと実装してみることにする。 from scratchでの実装にはこの記事が非常に参考になった。\nhttps://towardsdatascience.com/implementing-a-decision-tree-from-scratch-f5358ff9c4bb https://darden.hatenablog.com/entry/2016/12/15/222447 決定木 まず決定木の仕組みについて説明する。 とは言っても懇切丁寧に説明している記事はほかにいくらでもあるので、ここではざっくりとした説明に留める。\nアルゴリズム 決定木における分類問題においては分類を行いたい対象に対する特徴量がn_features個与えられており、各特徴量は数で表現されるとする。\n例えば、sklearn内に格納されているirisデータセットには4個の特徴量が用意されている。具体的には、sepal length, sepal width, petal length, petal widthであり、いずれも単位はcmの数になっている。 このとき、決定木は与えられた特徴量たちに対して「特徴量Aは$x$より小さいか？」という質問を行う。この質問に対する答えがYesならば、次に「特徴量Bは$y$より小さいか？」という質問を、答えがNoならば、「特徴量Cは$z$より小さいか？」という質問を行う。 こういった質問を次々に行っていき、最後に「前の質問の答えがYesならば分類クラスはPである」といった具合に分類を行う。これが決定木の一つの大きな流れである。\n質問によって分岐が起きていくので、最初の質問を根とする木とみなすことができる。 この木のもとでは各質問は木のノードと捉えられ、また分岐の果てに分類問題の答えを出力する所は葉とみなすことができる。\n学習 上で説明した決定木アルゴリズムにおいて、各ノードに配置する「特徴量Aは$x$より小さいか？」という質問を作成する必要がある。これを学習データをもとに決定していくCARTアルゴリズムを紹介する。\n学習の流れは各ノードを根から順に決定していく流れとなるので、再帰的な構造を持つ。 学習データが質問を経てあるノードに到着した段階でn_samples個になったとしよう。 このとき、学習データがそれぞれ持つ分類クラスが質問を通して最も別れるような質問が最適な質問であると考える。明日晴れるか雨が降るかを知りたいときに、質問にYesと答えれば必ず晴れ、Noと答えれば必ず雨、ということが分かれば、これは最適な質問である、ということである。 これを定量的に評価する方法として、情報利得を用いることを考える。 n_samples個の学習データのそれぞれのクラスが格納された$y\\in\\mathbb{R}^{n_\\mathrm{samples}}$に対して、クラスの揃い具合を表す情報量$f\\colon\\mathbb{R}^{N}\\to\\mathbb{R}$があったとしよう。 この指標は値が小さいほど学習データの分類クラスは揃っていて、値が大きいほどに学習データの分類クラスは全くもってばらけている状況を想定する。 ある質問を通して、n_left個の学習データがYesと、n_right個の学習データがNoと答えたとしよう。また、これに応じて分類クラス$y$は$y_{\\mathrm{left}}$と$y_{\\mathrm{right}}$に分けられるとする。 学習データが入力された段階では、情報量は$f(y)$である。 また、質問にYesと答えたときの情報量は$f(y_{\\mathrm{left}})$、Noと答えたときの情報量は$f(y_{\\mathrm{right}})$となる。 確率的には質問に答えた後の情報量は$\\frac{n_{\\mathrm{left}}}{n_{\\mathrm{samples}}}f(y_{\\mathrm{left}}) + \\frac{n_{\\mathrm{right}}}{n_{\\mathrm{samples}}}f(y_{\\mathrm{right}})$となる。よって、質問に答える前後で情報利得 $$ f(y) - \\left(\\frac{n_{\\mathrm{left}}}{n_{\\mathrm{samples}}}f(y_{\\mathrm{left}}) + \\frac{n_{\\mathrm{right}}}{n_{\\mathrm{samples}}}f(y_{\\mathrm{right}})\\right) $$ が最大となるような質問が最適である、と考える。 具体的な実装においては、各ノードに対して学習データを入力するたびごとに、まず特徴量を一つ選ぶ。学習データにわたってこの特徴量の値を閾値として情報利得を計算する。これをすべての特徴量と閾値に対して行い、最大の情報利得を取る特徴量と閾値でもってそのノードの質問を確定する、 という流れを取る。\n分類クラスの揃い具合を表す代表的な指標として、エントロピーとジニ不純度が ある。sklearnはデフォルトでジニ不純度を指定するようである。\n決定木の実装 はじめに、ノードの実装を行う。 各ノードは質問を決定する特徴量(のインデックス)self.featureとその閾値self.thresholdを持つ。 これにより定まる質問に対して答えがYesであれば左側の子ノードself.leftを、 Noであれば右側の子ノードselft.rightを参照するようにする。 ただし、葉ノードである場合には、その特徴量(のインデックス)を保管するようにする。\n1 2 3 4 5 6 7 8 9 10 class Node: def __init__(self, feature=None, threshold=None, left=None, right=None, value=None): self.feature = feature self.threshold = threshold self.left = left self.right = right self.value = value def is_leaf(self): return self.value is not None 次に決定木の実装である。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 import numpy as np class DecisionTree: def __init__(self, max_depth=10, min_samples_split=2, criterion='entropy', random_state=None): self.max_depth = max_depth self.min_samples_split = min_samples_split self.root = None if criterion == 'entropy': self.criterion = self._entropy elif criterion == 'gini': self.criterion = self._gini else: raise ValueError(f\"Invalid criterion: criterion '{criterion}' not implemented, use 'entropy' or 'gini'\") if random_state is not None: self.random_state = np.random.RandomState(random_state) else: self.random_state = np.random.RandomState() def _is_finished(self, depth, n_class_labels, n_samples): return (depth \u003e= self.max_depth or n_class_labels == 1 or n_samples \u003c self.min_samples_split) def _entropy(self, y): proportions = np.bincount(y) / len(y) entropy = -np.sum([p * np.log2(p) for p in proportions if p \u003e 0]) return entropy def _gini(self, y): proportions = np.bincount(y) / len(y) gini = 1 - (proportions ** 2).sum() return gini def _create_split(self, X, thresh): idx_left = X \u003c= thresh idx_right = np.logical_not(idx_left) return idx_left, idx_right def _information_gain(self, X, y, thresh): idx_left, idx_right = self._create_split(X, thresh) n, n_left, n_right = len(y), len(idx_left), len(idx_right) if n_left == 0 or n_right == 0: return 0 info_parent = self.criterion(y) info_child = (n_left / n) * self.criterion(y[idx_left]) + (n_right / n) * self.criterion(y[idx_right]) return info_parent - info_child def _best_split(self, X, y, features_candidate): gain_best = -1 for feat in features_candidate: X_feat = X[:, feat] thresholds = np.unique(X_feat) for thresh in thresholds: gain = self._information_gain(X_feat, y, thresh) if gain \u003e gain_best: gain_best, feat_best, thresh_best = gain, feat, thresh return feat_best, thresh_best def _build_tree(self, X, y, depth=0): n_samples = X.shape[0] n_class_labels = len(np.unique(y)) if self._is_finished(depth, n_class_labels, n_samples): most_common_labels = np.argmax(np.bincount(y)) return Node(value=most_common_labels) features_candidate = self.random_state.choice(self.n_features, self.n_features, replace=False) feat, thresh = self._best_split(X, y, features_candidate) idx_left, idx_right = self._create_split(X[:, feat], thresh) child_left = self._build_tree(X[idx_left, :], y[idx_left], depth + 1) child_right = self._build_tree(X[idx_right, :], y[idx_right], depth + 1) return Node(feat, thresh, child_left, child_right) def _traverse_tree(self, x, node): if node.is_leaf(): return node.value node_next = node.left if x[node.feature] \u003c= node.threshold else node.right return self._traverse_tree(x, node_next) def fit(self, X, y): self.n_features = X.shape[1] self.root = self._build_tree(X, y) def predict(self, X): predictions = [self._traverse_tree(x, self.root) for x in X] return np.array(predictions) Info\nsplitter='best'の場合は各ノードの質問を構成する特徴量を決めるためにすべての特徴量を試す必要がある。そのため、はじめはrandom_stateの指定は特に必要ないのではないか？と思っていた。具体的には45行目においてfor feat in features_candidate:を行う代わりにfor i in range(self.n_samples):を行えば良いのではないか？ということである。 自分の中での一つの答えは、毎回for i in range(self.n_samples):を行ってしまうと、複数の特徴量(と閾値)で同じ最善な情報利得を得た場合に、必ずインデックスが若いほうの特徴量を選んでしまう、という問題が発生してしまう。 そのため、インデックスをnp.random.choice(self.n_features, self.n_features, replace=False)で並び変えることによって複数の最適な情報利得を得た場合にもランダム性が作用して性能向上に繋がる可能性が出る。このIssueが参考になる。\n具体例 sklearn内のデータを利用して分類問題を解こう。sklearn.tree.DecisionTreeClassifierとの比較も行う。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier data = datasets.load_breast_cancer() X, y = data.data, data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) random_state = 20230625 clf_sklearn = DecisionTreeClassifier(max_depth=5, criterion='gini', splitter='best', random_state=random_state) clf_sklearn.fit(X_train, y_train) y_pred_sklearn = clf_sklearn.predict(X_test) acc_sklearn = (y_pred_sklearn == y_test).sum() / len(y_test) clf_myown = DecisionTree(max_depth=5, criterion='gini', random_state=random_state) clf_myown.fit(X_train, y_train) y_pred_myown = clf_myown.predict(X_test) acc_myown = (y_pred_myown == y_test).sum() / len(y_test) print(f'Accuracy(sklearn): {acc_sklearn:.5f}') print(f'Accuracy(my own): {acc_myown:.5f}') 出力は次のようになる。\n1 2 Accuracy(sklearn): 0.94737 Accuracy(my own): 0.92105 それなりに精度は出ているがsklearnには負けている。。。\nまとめ 決定木のアルゴリズムを概説し、1から実装を行った。それなりの精度は出ているがsklearnには負けている。splitterの作り方が間違っているのかもしれないが、これはsklearnの細かい実装を見る必要がある。\nこの実装をもとにrandom forestや勾配boosting等の実装も行ってみたい。\n",
  "wordCount" : "631",
  "inLanguage": "en",
  "datePublished": "2023-06-25T10:00:06+09:00",
  "dateModified": "2023-06-25T10:00:06+09:00",
  "author":{
    "@type": "Person",
    "name": "Ryosuke Yoneda"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://yonesuke.github.io/posts/decision-tree/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Ryosuke Yoneda",
    "logo": {
      "@type": "ImageObject",
      "url": "https://yonesuke.github.io/favicon_io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://yonesuke.github.io" accesskey="h" title="Ryosuke Yoneda&#39;s Homepage (Alt + H)">
                <img src="https://yonesuke.github.io/favicon_io/apple-touch-icon.png" alt="" aria-label="logo"
                    height="35">Ryosuke Yoneda&#39;s Homepage</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://yonesuke.github.io/about" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://yonesuke.github.io/research" title="Research">
                    <span>Research</span>
                </a>
            </li>
            <li>
                <a href="https://yonesuke.github.io/teaching" title="Teaching">
                    <span>Teaching</span>
                </a>
            </li>
            <li>
                <a href="https://yonesuke.github.io/posts" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://yonesuke.github.io">Home</a>&nbsp;»&nbsp;<a href="https://yonesuke.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      決定木を1から実装する
    </h1>
    <div class="post-meta"><span title='2023-06-25 10:00:06 +0900 JST'>June 25, 2023</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;631 words&nbsp;·&nbsp;Ryosuke Yoneda

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#決定木">決定木</a>
      <ul>
        <li><a href="#アルゴリズム">アルゴリズム</a></li>
        <li><a href="#学習">学習</a></li>
      </ul>
    </li>
    <li><a href="#決定木の実装">決定木の実装</a></li>
    <li><a href="#具体例">具体例</a></li>
    <li><a href="#まとめ">まとめ</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><p>決定木って名前はよく聞くし<code>scikit-learn</code>で簡単に使えてしまうけど、中身を詳しく知っているわけではなかったのできちんと実装してみることにする。
from scratchでの実装にはこの記事が非常に参考になった。</p>
<ul>
<li><a href="https://towardsdatascience.com/implementing-a-decision-tree-from-scratch-f5358ff9c4bb">https://towardsdatascience.com/implementing-a-decision-tree-from-scratch-f5358ff9c4bb</a></li>
<li><a href="https://darden.hatenablog.com/entry/2016/12/15/222447">https://darden.hatenablog.com/entry/2016/12/15/222447</a></li>
</ul>
<h2 id="決定木">決定木<a hidden class="anchor" aria-hidden="true" href="#決定木">#</a></h2>
<p>まず決定木の仕組みについて説明する。
とは言っても懇切丁寧に説明している記事はほかにいくらでもあるので、ここではざっくりとした説明に留める。</p>
<h3 id="アルゴリズム">アルゴリズム<a hidden class="anchor" aria-hidden="true" href="#アルゴリズム">#</a></h3>
<p>決定木における分類問題においては分類を行いたい対象に対する特徴量が<code>n_features</code>個与えられており、各特徴量は数で表現されるとする。</p>
<ul>
<li>例えば、<code>sklearn</code>内に格納されているirisデータセットには4個の特徴量が用意されている。具体的には、sepal length, sepal width, petal length, petal widthであり、いずれも単位はcmの数になっている。</li>
</ul>
<p>このとき、決定木は与えられた特徴量たちに対して「特徴量Aは$x$より小さいか？」という質問を行う。この質問に対する答えがYesならば、次に「特徴量Bは$y$より小さいか？」という質問を、答えがNoならば、「特徴量Cは$z$より小さいか？」という質問を行う。
こういった質問を次々に行っていき、最後に「前の質問の答えがYesならば分類クラスはPである」といった具合に分類を行う。これが決定木の一つの大きな流れである。</p>
<p>質問によって分岐が起きていくので、最初の質問を根とする<strong>木</strong>とみなすことができる。
この木のもとでは各質問は木の<strong>ノード</strong>と捉えられ、また分岐の果てに分類問題の答えを出力する所は<strong>葉</strong>とみなすことができる。</p>
<h3 id="学習">学習<a hidden class="anchor" aria-hidden="true" href="#学習">#</a></h3>
<p>上で説明した決定木アルゴリズムにおいて、各ノードに配置する「特徴量Aは$x$より小さいか？」という質問を作成する必要がある。これを学習データをもとに決定していく<strong>CART</strong>アルゴリズムを紹介する。</p>
<p>学習の流れは各ノードを根から順に決定していく流れとなるので、再帰的な構造を持つ。
学習データが質問を経てあるノードに到着した段階で<code>n_samples</code>個になったとしよう。
このとき、<strong>学習データがそれぞれ持つ分類クラスが質問を通して最も別れるような質問が最適な質問である</strong>と考える。明日晴れるか雨が降るかを知りたいときに、質問にYesと答えれば必ず晴れ、Noと答えれば必ず雨、ということが分かれば、これは最適な質問である、ということである。
これを定量的に評価する方法として、<strong>情報利得</strong>を用いることを考える。
<code>n_samples</code>個の学習データのそれぞれのクラスが格納された$y\in\mathbb{R}^{n_\mathrm{samples}}$に対して、クラスの揃い具合を表す情報量$f\colon\mathbb{R}^{N}\to\mathbb{R}$があったとしよう。
この指標は値が小さいほど学習データの分類クラスは揃っていて、値が大きいほどに学習データの分類クラスは全くもってばらけている状況を想定する。
ある質問を通して、<code>n_left</code>個の学習データがYesと、<code>n_right</code>個の学習データがNoと答えたとしよう。また、これに応じて分類クラス$y$は$y_{\mathrm{left}}$と$y_{\mathrm{right}}$に分けられるとする。
学習データが入力された段階では、情報量は$f(y)$である。
また、質問にYesと答えたときの情報量は$f(y_{\mathrm{left}})$、Noと答えたときの情報量は$f(y_{\mathrm{right}})$となる。
確率的には質問に答えた後の情報量は$\frac{n_{\mathrm{left}}}{n_{\mathrm{samples}}}f(y_{\mathrm{left}}) + \frac{n_{\mathrm{right}}}{n_{\mathrm{samples}}}f(y_{\mathrm{right}})$となる。よって、質問に答える前後で情報利得
$$
f(y) - \left(\frac{n_{\mathrm{left}}}{n_{\mathrm{samples}}}f(y_{\mathrm{left}}) + \frac{n_{\mathrm{right}}}{n_{\mathrm{samples}}}f(y_{\mathrm{right}})\right)
$$
が最大となるような質問が最適である、と考える。
具体的な実装においては、各ノードに対して学習データを入力するたびごとに、まず特徴量を一つ選ぶ。学習データにわたってこの特徴量の値を閾値として情報利得を計算する。これをすべての特徴量と閾値に対して行い、最大の情報利得を取る特徴量と閾値でもってそのノードの質問を確定する、
という流れを取る。</p>
<p>分類クラスの揃い具合を表す代表的な指標として、<strong>エントロピー</strong>と<strong>ジニ不純度</strong>が
ある。<code>sklearn</code>はデフォルトでジニ不純度を指定するようである。</p>
<h2 id="決定木の実装">決定木の実装<a hidden class="anchor" aria-hidden="true" href="#決定木の実装">#</a></h2>
<p>はじめに、ノードの実装を行う。
各ノードは質問を決定する特徴量(のインデックス)<code>self.feature</code>とその閾値<code>self.threshold</code>を持つ。
これにより定まる質問に対して答えがYesであれば左側の子ノード<code>self.left</code>を、
Noであれば右側の子ノード<code>selft.right</code>を参照するようにする。
ただし、葉ノードである場合には、その特徴量(のインデックス)を保管するようにする。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Node</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">feature</span> <span class="o">=</span> <span class="n">feature</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">threshold</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">left</span> <span class="o">=</span> <span class="n">left</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">right</span> <span class="o">=</span> <span class="n">right</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">is_leaf</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>次に決定木の実装である。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span><span class="lnt">78
</span><span class="lnt">79
</span><span class="lnt">80
</span><span class="lnt">81
</span><span class="lnt">82
</span><span class="lnt">83
</span><span class="lnt">84
</span><span class="lnt">85
</span><span class="lnt">86
</span><span class="lnt">87
</span><span class="lnt">88
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DecisionTree</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;entropy&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span> <span class="o">=</span> <span class="n">min_samples_split</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">root</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">criterion</span> <span class="o">==</span> <span class="s1">&#39;entropy&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_entropy</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">criterion</span> <span class="o">==</span> <span class="s1">&#39;gini&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gini</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Invalid criterion: criterion &#39;</span><span class="si">{</span><span class="n">criterion</span><span class="si">}</span><span class="s2">&#39; not implemented, use &#39;entropy&#39; or &#39;gini&#39;&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">random_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_is_finished</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">n_class_labels</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">(</span><span class="n">depth</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="ow">or</span> <span class="n">n_class_labels</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">n_samples</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_entropy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">proportions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">entropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">([</span><span class="n">p</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">proportions</span> <span class="k">if</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">entropy</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_gini</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">proportions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">gini</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">proportions</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">gini</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_create_split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">thresh</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">idx_left</span> <span class="o">=</span> <span class="n">X</span> <span class="o">&lt;=</span> <span class="n">thresh</span>
</span></span><span class="line"><span class="cl">        <span class="n">idx_right</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">idx_left</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">idx_left</span><span class="p">,</span> <span class="n">idx_right</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_information_gain</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">thresh</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">idx_left</span><span class="p">,</span> <span class="n">idx_right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">thresh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">n</span><span class="p">,</span> <span class="n">n_left</span><span class="p">,</span> <span class="n">n_right</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">idx_left</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">idx_right</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">n_left</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">n_right</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">         
</span></span><span class="line"><span class="cl">        <span class="n">info_parent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">info_child</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_left</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">idx_left</span><span class="p">])</span> <span class="o">+</span> <span class="p">(</span><span class="n">n_right</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">idx_right</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">info_parent</span> <span class="o">-</span> <span class="n">info_child</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_best_split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">features_candidate</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">gain_best</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">features_candidate</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">X_feat</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">feat</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">thresholds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">X_feat</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">thresh</span> <span class="ow">in</span> <span class="n">thresholds</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">gain</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_information_gain</span><span class="p">(</span><span class="n">X_feat</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">thresh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">gain</span> <span class="o">&gt;</span> <span class="n">gain_best</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">gain_best</span><span class="p">,</span> <span class="n">feat_best</span><span class="p">,</span> <span class="n">thresh_best</span> <span class="o">=</span> <span class="n">gain</span><span class="p">,</span> <span class="n">feat</span><span class="p">,</span> <span class="n">thresh</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">feat_best</span><span class="p">,</span> <span class="n">thresh_best</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_build_tree</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">n_class_labels</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_finished</span><span class="p">(</span><span class="n">depth</span><span class="p">,</span> <span class="n">n_class_labels</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">most_common_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">Node</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">most_common_labels</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">features_candidate</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">feat</span><span class="p">,</span> <span class="n">thresh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_best_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">features_candidate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">idx_left</span><span class="p">,</span> <span class="n">idx_right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_split</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="n">feat</span><span class="p">],</span> <span class="n">thresh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">child_left</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_tree</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">idx_left</span><span class="p">,</span> <span class="p">:],</span> <span class="n">y</span><span class="p">[</span><span class="n">idx_left</span><span class="p">],</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">child_right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_tree</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">idx_right</span><span class="p">,</span> <span class="p">:],</span> <span class="n">y</span><span class="p">[</span><span class="n">idx_right</span><span class="p">],</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">Node</span><span class="p">(</span><span class="n">feat</span><span class="p">,</span> <span class="n">thresh</span><span class="p">,</span> <span class="n">child_left</span><span class="p">,</span> <span class="n">child_right</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_traverse_tree</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">node</span><span class="o">.</span><span class="n">value</span>
</span></span><span class="line"><span class="cl">        <span class="n">node_next</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">left</span> <span class="k">if</span> <span class="n">x</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">feature</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">node</span><span class="o">.</span><span class="n">threshold</span> <span class="k">else</span> <span class="n">node</span><span class="o">.</span><span class="n">right</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_traverse_tree</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">node_next</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">root</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_tree</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_traverse_tree</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">root</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><style type="text/css">.notice{--root-color:#444;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#c33;--warning-content:#fee;--info-title:#fb7;--info-content:#fec;--note-title:#6be;--note-content:#e7f2fa;--tip-title:#5a5;--tip-content:#efe}@media (prefers-color-scheme:dark){.notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}}body.dark .notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}.notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:var(--root-color);background:var(--root-background)}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:var(--title-color);background:var(--title-background)}.notice.warning .notice-title{background:var(--warning-title)}.notice.warning{background:var(--warning-content)}.notice.info .notice-title{background:var(--info-title)}.notice.info{background:var(--info-content)}.notice.note .notice-title{background:var(--note-title)}.notice.note{background:var(--note-content)}.notice.tip .notice-title{background:var(--tip-title)}.notice.tip{background:var(--tip-content)}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline svg{top:.125em;position:relative}</style>
<div><svg width="0" height="0" display="none" xmlns="http://www.w3.org/2000/svg"><symbol id="tip-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"/></symbol><symbol id="note-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="warning-notice" viewBox="0 0 576 512" preserveAspectRatio="xMidYMid meet"><path d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="info-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z"/></symbol></svg></div><div class="notice info" >
<p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#info-notice"></use></svg></span>Info</p><p><code>splitter='best'</code>の場合は各ノードの質問を構成する特徴量を決めるためにすべての特徴量を試す必要がある。そのため、はじめは<code>random_state</code>の指定は特に必要ないのではないか？と思っていた。具体的には45行目において<code>for feat in features_candidate:</code>を行う代わりに<code>for i in range(self.n_samples):</code>を行えば良いのではないか？ということである。
自分の中での一つの答えは、毎回<code>for i in range(self.n_samples):</code>を行ってしまうと、複数の特徴量(と閾値)で同じ最善な情報利得を得た場合に、必ずインデックスが若いほうの特徴量を選んでしまう、という問題が発生してしまう。
そのため、インデックスを<code>np.random.choice(self.n_features, self.n_features, replace=False)</code>で並び変えることによって複数の最適な情報利得を得た場合にもランダム性が作用して性能向上に繋がる可能性が出る。<a href="https://github.com/scikit-learn/scikit-learn/issues/2386">このIssue</a>が参考になる。</p></div>

<h2 id="具体例">具体例<a hidden class="anchor" aria-hidden="true" href="#具体例">#</a></h2>
<p><code>sklearn</code>内のデータを利用して分類問題を解こう。<code>sklearn.tree.DecisionTreeClassifier</code>との比較も行う。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">random_state</span> <span class="o">=</span> <span class="mi">20230625</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">clf_sklearn</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;gini&#39;</span><span class="p">,</span> <span class="n">splitter</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">clf_sklearn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y_pred_sklearn</span> <span class="o">=</span> <span class="n">clf_sklearn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">acc_sklearn</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred_sklearn</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">clf_myown</span> <span class="o">=</span> <span class="n">DecisionTree</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;gini&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">clf_myown</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y_pred_myown</span> <span class="o">=</span> <span class="n">clf_myown</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">acc_myown</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred_myown</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Accuracy(sklearn): </span><span class="si">{</span><span class="n">acc_sklearn</span><span class="si">:</span><span class="s1">.5f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Accuracy(my own):  </span><span class="si">{</span><span class="n">acc_myown</span><span class="si">:</span><span class="s1">.5f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>出力は次のようになる。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="go">Accuracy(sklearn): 0.94737
</span></span></span><span class="line"><span class="cl"><span class="go">Accuracy(my own):  0.92105
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>それなりに精度は出ているが<code>sklearn</code>には負けている。。。</p>
<h2 id="まとめ">まとめ<a hidden class="anchor" aria-hidden="true" href="#まとめ">#</a></h2>
<p>決定木のアルゴリズムを概説し、1から実装を行った。それなりの精度は出ているが<code>sklearn</code>には負けている。splitterの作り方が間違っているのかもしれないが、これは<code>sklearn</code>の細かい実装を見る必要がある。</p>
<p>この実装をもとにrandom forestや勾配boosting等の実装も行ってみたい。</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://yonesuke.github.io/posts/benford/">
    <span class="title">« Prev</span>
    <br>
    <span>ベンフォードの法則</span>
  </a>
  <a class="next" href="https://yonesuke.github.io/posts/sha256-constants/">
    <span class="title">Next »</span>
    <br>
    <span>SHA-256に用いられる定数が微妙に合わない</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share 決定木を1から実装する on twitter"
        href="https://twitter.com/intent/tweet/?text=%e6%b1%ba%e5%ae%9a%e6%9c%a8%e3%82%921%e3%81%8b%e3%82%89%e5%ae%9f%e8%a3%85%e3%81%99%e3%82%8b&amp;url=https%3a%2f%2fyonesuke.github.io%2fposts%2fdecision-tree%2f&amp;hashtags=">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 決定木を1から実装する on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fyonesuke.github.io%2fposts%2fdecision-tree%2f&amp;title=%e6%b1%ba%e5%ae%9a%e6%9c%a8%e3%82%921%e3%81%8b%e3%82%89%e5%ae%9f%e8%a3%85%e3%81%99%e3%82%8b&amp;summary=%e6%b1%ba%e5%ae%9a%e6%9c%a8%e3%82%921%e3%81%8b%e3%82%89%e5%ae%9f%e8%a3%85%e3%81%99%e3%82%8b&amp;source=https%3a%2f%2fyonesuke.github.io%2fposts%2fdecision-tree%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 決定木を1から実装する on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fyonesuke.github.io%2fposts%2fdecision-tree%2f&title=%e6%b1%ba%e5%ae%9a%e6%9c%a8%e3%82%921%e3%81%8b%e3%82%89%e5%ae%9f%e8%a3%85%e3%81%99%e3%82%8b">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 決定木を1から実装する on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fyonesuke.github.io%2fposts%2fdecision-tree%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 決定木を1から実装する on whatsapp"
        href="https://api.whatsapp.com/send?text=%e6%b1%ba%e5%ae%9a%e6%9c%a8%e3%82%921%e3%81%8b%e3%82%89%e5%ae%9f%e8%a3%85%e3%81%99%e3%82%8b%20-%20https%3a%2f%2fyonesuke.github.io%2fposts%2fdecision-tree%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 決定木を1から実装する on telegram"
        href="https://telegram.me/share/url?text=%e6%b1%ba%e5%ae%9a%e6%9c%a8%e3%82%921%e3%81%8b%e3%82%89%e5%ae%9f%e8%a3%85%e3%81%99%e3%82%8b&amp;url=https%3a%2f%2fyonesuke.github.io%2fposts%2fdecision-tree%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer><script src="https://utteranc.es/client.js"
        repo="yonesuke/yonesuke.github.io"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://yonesuke.github.io">Ryosuke Yoneda</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
