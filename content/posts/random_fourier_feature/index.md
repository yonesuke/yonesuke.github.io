---
title: "Random Fourier Features"
date: 2023-01-06T23:34:02+09:00
draft: true
math: true
author: Ryosuke Yoneda
---

ã‚«ãƒ¼ãƒãƒ«æ³•ã«ã‚ˆã‚‹ãƒªãƒƒã‚¸å›å¸°ã¯è¡¨ç¾åŠ›ãŒé«˜ã„ã“ã¨ãŒçŸ¥ã‚‰ã‚Œã¦ãŠã‚Šã€ã¾ãŸãã®æ•°å­¦çš„èƒŒæ™¯ã®è±Šã‹ã•ã‹ã‚‰å¤šãã®ç ”ç©¶ãŒãªã•ã‚Œã¦ãã¾ã—ãŸã€‚
ã—ã‹ã—ã€$n$å€‹ã®ãƒ‡ãƒ¼ã‚¿æ•°ã«å¯¾ã—ã¦æ¨è«–ã«$\mathcal{O}(n^{3})$ã®è¨ˆç®—é‡ãŒå¿…è¦ã¨ã•ã‚Œã‚‹ãŸã‚ã€è¨ˆç®—é‡ã‚’ä½æ¸›ã•ã›ã‚‹æ–¹æ³•ã‚’æ¤œè¨ã™ã‚‹ã“ã¨ã¯éå¸¸ã«é‡è¦ã§ã™ã€‚
ã“ã“ã§ã¯ã€Random Fourier Features [^rff]ã¨å‘¼ã°ã‚Œã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚
å®Ÿè£…ã‚‚è¡Œã£ãŸãŒ[Gist](https://gist.github.com/yonesuke/ebc5a69d270cf2cbc559ce228370f910)ã«ã‚‚å…¬é–‹ã—ã¦ã„ã‚‹ã€‚

## Random Fourier Features
Random Fourier Featuresã¯ã‚«ãƒ¼ãƒãƒ«é–¢æ•°$k(x,y)\colon\mathbb{R}^{d}\times\mathbb{R}^{d}\to\mathbb{R}$ãŒ$x-y$ã®é–¢æ•°$\phi(x-y)$ã§è¡¨ç¾ã§ãã‚‹å ´åˆã«ã€ãã‚Œã‚’ãƒ©ãƒ³ãƒ€ãƒ ãªåŸºåº•ã§è¿‘ä¼¼ã™ã‚‹æ‰‹æ³•ã§ã‚ã‚‹ã€‚ã‚­ãƒ¢ã¨ãªã‚‹ã®ã¯Bochnerã®å®šç†ã§ã‚ã‚‹ã€‚

{{< thmlike type="Theorem" title="Bochnerã®å®šç†" >}}
$k(x,y)=\phi(x-y)$ãŒé€£ç¶šãªæ­£å®šå€¤ã‚«ãƒ¼ãƒãƒ«ã§ã‚ã‚‹ãŸã‚ã®å¿…è¦ååˆ†æ¡ä»¶ã¯$\mathbb{R}^{d}$ä¸Šã®æœ‰é™éè² Borelæ¸¬åº¦$\mu$ãŒã‚ã£ã¦ã€
$$
k(x,y)=\int_{\mathbb{R}^{d}}e^{i\omega^{\top}(x-y)}\mathrm{d}\mu(\omega)
$$
ã§è¡¨ã•ã‚Œã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
{{< /thmlike >}}

é©å½“ã«ã‚¹ã‚±ãƒ¼ãƒ«ã™ã‚Œã°$\mu$ã¯ç¢ºç‡ã«ãªã‚Šã€(å­˜åœ¨ã™ã‚Œã°)$\mathrm{d}\mu(\omega)=p(\omega)\mathrm{d}\omega$ã¨æ›¸ãã“ã¨ãŒå‡ºæ¥ã‚‹ã€‚
ã“ã®ã¨ãã€$k$ã®å€¤åŸŸã¯å®Ÿæ•°ã§ã‚ã‚‹ã®ã§ã€
$$
k(x,y)=\mathbb{E}_ {\omega}[\cos(\omega^{\top}(x-y))]
$$
å®Ÿã¯ã“ã‚Œã¯$b$ã‚’$[0,2\pi)$ä¸Šã®ä¸€æ§˜ä¹±æ•°ã¨ã—ã¦ã€
$$
2\mathbb{E}_ {\omega,b}[\cos(\omega^{\top}x+b)\cos(\omega^{\top}y+b)]
$$
ã¨ä¸€è‡´ã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚(åŠ æ³•å®šç†ã‚’ç”¨ã„ã‚ˆ)

{{< thmlike type="Proposition" title="Random Fourier Features" >}}
ã‚«ãƒ¼ãƒãƒ«é–¢æ•°$k(x,y)$ãŒ$x-y$ã®é–¢æ•°ã§ä¸ãˆã‚‰ã‚Œã‚‹ã¨ãã€
$$
k(x,y)=2\mathbb{E}_ {\omega,b}[\cos(\omega^{\top}x+b)\cos(\omega^{\top}y+b)]
$$
ãŒæˆç«‹ã™ã‚‹ã€‚ã“ã“ã§ã€$\omega$ã¯$k$ã®ç¢ºç‡ã«å¾“ã„ã€$b$ã¯$[0,2\pi)$ä¸Šã®ä¸€æ§˜åˆ†å¸ƒã«å¾“ã†ã€‚
{{< /thmlike >}}

ã“ã®æ€§è³ªã‚’ç”¨ã„ã¦ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã‚’è¿‘ä¼¼ã™ã‚‹ã“ã¨ã‚’è€ƒãˆã‚‹ã€‚$\omega_{i},b_{i}$ã‚’ãã‚Œãã‚Œã®åˆ†å¸ƒã«å¾“ã†ä¹±æ•°ã¨ã—ã¦$m$å€‹ç™ºç”Ÿã•ã›ã€é–¢æ•°$z_{i}(x)=\sqrt{2/m}\cos(\omega_{i}^{\top}x+b_{i})$ã‚’æ§‹æˆã—ãŸã¨ãã€
$$
\sum_{i=1}^{m}z_{i}(x)z_{i}(y)\to k(x,y)
$$
ãŒ$m\to\infty$ã®æ¥µé™ã§å¤§æ•°ã®æ³•å‰‡ã«ã‚ˆã‚ŠåæŸã—ã¦ã„ãã€‚

## Kernel Ridge Regression
Random Fourier Featuresã‚’ç”¨ã„ã¦ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã‚’è¡¨ç¾ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã‚«ãƒ¼ãƒãƒ«ãƒªãƒƒã‚¸å›å¸°ã®è¨ˆç®—é‡ãŒä½æ¸›ã•ã‚Œã‚‹ã€‚

ãƒ‡ãƒ¼ã‚¿$\mathcal{D}=\\{(x_{i},y_{i})\\}_ {i=1}^{n}$ãŒä¸ãˆã‚‰ã‚Œã‚‹å ´åˆã‚’è€ƒãˆã‚‹ã€‚å…¥åŠ›$x_{i}$ã‚’ç‰¹å¾´å†™åƒ$\Phi$ã§å†™ã—ã€å†™ã—ãŸå…ˆã®ç©ºé–“$H$ã§ãƒªãƒƒã‚¸å›å¸°ã‚’ã™ã‚‹ã€‚
æå¤±é–¢æ•°ã¯
$$
L=\sum_{i=1}^{n}[y_{i}-f(x_{i})]^{2}+\lambda\||f\||^{2}_ {H}
$$
ã¨ãªã‚Šã€ã“ã‚Œã‚’æœ€å°åŒ–ã™ã‚‹$f\in H$ã‚’æ¢ã™ã€‚$\lambda\||f\||^{2}_ {H}$ã¯æ­£å‰‡åŒ–ã®é …ã§ã‚ã‚‹ã€‚
Representationå®šç†ã«ã‚ˆã‚Š$f$ã¯$\Phi(x_{i})$ã§å±•é–‹ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã€è‰²ã€…è¨ˆç®—ã™ã‚‹ã¨æå¤±é–¢æ•°ã‚’æœ€å°åŒ–ã™ã‚‹$\hat{f}$ã¯
$$
\hat{f}(x)=\sum_{i=1}^{n}\hat{\alpha}\_{i}k(x_{i},x)
$$
ã¨ãªã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚ã“ã“ã§ã€$\hat{\alpha}=(K+\lambda I_{n})^{-1}y,[K]\_{ij}=k(x_{i},x_{j})$ã§ã‚ã‚‹ã€‚
$\alpha$ã®è¨ˆç®—ã«é€†è¡Œåˆ—ãŒå«ã¾ã‚Œã‚‹ãŸã‚$\mathcal{O}(n^{3})$ã®è¨ˆç®—é‡ãŒå¿…è¦ã¨ãªã£ã¦ã—ã¾ã†ã€‚

ã“ã“ã§ã€Random Fourier Featuresã‚’ç”¨ã„ã¦ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã‚’è¿‘ä¼¼ã™ã‚‹ã“ã¨ã‚’è€ƒãˆã‚‹ã€‚
å…±åˆ†æ•£è¡Œåˆ—ã¯$[Z]\_{ij}=z_{j}(x_{i})$ã«ã‚ˆã£ã¦$K=ZZ^{\top}$ã§å±•é–‹ã•ã‚Œã‚‹ã®ã§ã€
$$
\begin{align*}
\hat{f}(x)=&\sum_{i=1}^{n}\sum_{j=1}^{m}[(K+\lambda I_{n})^{-1}y]\_{i}z_{j}(x_{i})z_{j}(x)\\\\
=&\sum_{i=1}^{n}\sum_{j=1}^{m}\left[\left(ZZ^{\top}+\lambda I_{n}\right)^{-1}y\right]\_{i}[Z]\_{ij}z_{j}(x)\\\\
=&\sum_{j=1}^{m}\left[Z^{\top}\left(ZZ^{\top}+\lambda I_{n}\right)^{-1}y\right]\_{j}z_{j}(x)
\end{align*}
$$
ã“ã“ã§Woodburyã®å…¬å¼ã‹ã‚‰$Z^{\top}(ZZ^{\top}+\lambda I_{n})^{-1}=(Z^{\top}Z+\lambda I_{m})^{-1}Z^{\top}$ã¨ãªã‚‹ [^woodbury] ã®ã§ã€
$$
\hat{f}(x)=\sum_{j=1}^{m}\left[\left(Z^{\top}Z+\lambda I_{m}\right)^{-1}Z^{\top}y\right]\_{j}z_{j}(x)
$$
ã§å¾—ã‚‰ã‚Œã‚‹ã€‚$Z^{\top}Z$ã®è¨ˆç®—ã«$\mathcal{O}(m^{2}n)$ã€$Z^{\top}Z+\lambda I_{m}$ã®é€†è¡Œåˆ—è¨ˆç®—ã«$\mathcal{O}(m^{3})$ã«ãªã‚‹ã®ã§ã€$m\ll n$ãªã‚‰ã°è¨ˆç®—é‡ã¯$\mathcal{O}(m^{2}n)$ã«è»½æ¸›ã•ã‚Œã‚‹ã€‚

## Implementation
ã“ã“ã§ã¯JAXã‚’ç”¨ã„ãŸå®Ÿè£…ã‚’è¡Œã†ã€‚ã¯ã˜ã‚ã«ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã®ã‚¯ãƒ©ã‚¹ã‚’å®šç¾©ã™ã‚‹ã€‚
ãŸã ã—ã€$k\colon\mathbb{R}^{1}\times\mathbb{R}^{1}\to\mathbb{R}$ã®ã‚‚ã®ã‚’ä»®å®šã™ã‚‹ã€‚
`cov_mat`é–¢æ•°ã¯å…±åˆ†æ•£è¡Œåˆ—ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°ã§ã‚ã‚‹ã€‚`jax.vmap`ã‚’ä½¿ã£ã¦åŠ¹ç‡ã‚ˆãè¨ˆç®—ã—ã¦ã„ã‚‹ã€‚

```python
import jax.numpy as jnp
from jax import random, vmap, scipy
import matplotlib.pyplot as plt

class Kernel:
    def __init__(self):
        pass
    def covariance(self, x1, x2):
        raise NotImplementedError
    def cov_mat(self, xs, xs2=None):
        if xs2 is None:
            return vmap(lambda x: vmap(lambda y: self.covariance(x, y))(xs))(xs)
        else:
            return vmap(lambda x: vmap(lambda y: self.covariance(x, y))(xs2))(xs)
```

ã“ã‚Œã‚’ã‚‚ã¨ã«RBFã‚«ãƒ¼ãƒãƒ«ã¨Random Fourier Featuresã‚’ç”¨ã„ãŸè¿‘ä¼¼ã‚«ãƒ¼ãƒãƒ«ã‚’å®šç¾©ã™ã‚‹ã€‚
RBFã‚«ãƒ¼ãƒãƒ«ã¯
$$
k(x,y)=\exp\left(-\frac{(x-y)^{2}}{2\sigma^{2}}\right)
$$
ã§å®šç¾©ã•ã‚Œã€Random Fourier Featuresã®ç¢ºç‡ã¯$\mu\sim\mathcal{N}(0,1/\sigma^{2})$ã«ãªã‚‹ã€‚
Random Fourier Featuresã®ã‚¹ã‚±ãƒ¼ãƒ«ã¯$k(x,x)=1$ãªã‚‹ã‚ˆã†ã«ã™ã‚Œã°ã‚ˆã„ãŒã€RBFã‚«ãƒ¼ãƒãƒ«ã¯æœ€åˆã‹ã‚‰ã“ã‚Œã‚’æº€ãŸã—ã¦ã„ã‚‹ã“ã¨ã«æ³¨æ„ã™ã‚‹ã€‚

```python
class RadialBasisFunction(Kernel):
    def __init__(self, sigma):
        super().__init__()
        self.sigma = sigma

    def covariance(self, x1, x2):
        return jnp.exp(-jnp.sum((x1 - x2) ** 2) / (2 * self.sigma ** 2))

class RandomFourierFeature(Kernel):
    def __init__(self, n_feature, sigma, seed):
        super().__init__()
        self.n_feature = n_feature
        self.sigma = sigma
        key_w = random.PRNGKey(seed)
        self.w = random.normal(key_w, (n_feature,)) / sigma
        key_b = random.split(key_w, 1)
        self.b = random.uniform(key_b, (n_feature,)) * 2 * jnp.pi

    def z(self, x):
        return jnp.sqrt(2 / self.n_feature) * jnp.cos(self.w * x + self.b)

    def covariance(self, x1, x2):
        return jnp.dot(self.z(x1), self.z(x2))
```

ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã‚’æ¯”è¼ƒã—ã¦ã¿ã‚ˆã†ã€‚

```python
xs = jnp.arange(-2.0, 2.0, 0.01)
sigma = 0.5

plt.figure(figsize=(24, 6))
plt.rcParams["font.size"] = 20

plt.subplot(1, 3, 1)
rbf = RadialBasisFunction(sigma)
rbf_mat = rbf.cov_mat(xs)
plt.matshow(rbf_mat, fignum=0, extent=(-2, 2, -2, 2))
plt.title("RBF")
plt.colorbar()

plt.subplot(1, 3, 2)
n_feature = 100
rff = RandomFourierFeature(n_feature, sigma, 0)
rff_mat = rff.cov_mat(xs)
plt.matshow(rff_mat, fignum=0, extent=(-2, 2, -2, 2))
plt.title(f"RFF, n_feature={n_feature}")
plt.colorbar()

plt.subplot(1, 3, 3)
n_feature = 10000
rff = RandomFourierFeature(n_feature, sigma, 0)
rff_mat = rff.cov_mat(xs)
plt.matshow(rff_mat, fignum=0, extent=(-2, 2, -2, 2))
plt.title(f"RFF, n_feature={n_feature}")
plt.colorbar()
```

![](rbf_rff.png)

ç‰¹å¾´å†™åƒã‚’$10^4$å€‹ã‚‚ä½¿ã£ã¦ã¿ã‚‹ã¨ã€RBFã‚«ãƒ¼ãƒãƒ«ã¨ã»ã¼åŒã˜ã«ãªã£ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚

æœ€å¾Œã«ã‚«ãƒ¼ãƒãƒ«ãƒªãƒƒã‚¸å›å¸°ã®ã‚¯ãƒ©ã‚¹ã‚’å®šç¾©ã™ã‚‹ã€‚
Random Fourier Featuresã‹å¦ã‹ã§`predict`é–¢æ•°ã‚’åˆ†ã‘ã¦ã„ã‚‹ã€‚

```python
class KernelRidgeRegression:
    def __init__(self, kernel: Kernel, alpha):
        self.kernel = kernel
        self.alpha = alpha

    def fit(self, xs_data, ys_data):
        self.xs_data = xs_data
        self.ys_data = ys_data
        self.K_data = self.kernel.cov_mat(xs_data)
        if self.kernel.__class__.__name__ == "RandomFourierFeature":
            Z = vmap(self.kernel.z)(xs_data)
            self.coeffs_rff = scipy.linalg.solve(Z.T @ Z + self.alpha * jnp.eye(self.kernel.n_feature), Z.T @ ys_data)
        else:
            self.coeffs = scipy.linalg.solve(self.K_data + self.alpha * jnp.eye(len(xs_data)), ys_data)

    def predict(self, xs_infer):
        if self.kernel.__class__.__name__ == "RandomFourierFeature":
            Z = vmap(self.kernel.z)(xs_infer)
            return Z @ self.coeffs_rff
        else:
            K_infer = self.kernel.cov_mat(xs_infer, self.xs_data)
            return K_infer @ self.coeffs
```

å®Ÿéš›ã«å›å¸°ã‚’è¡Œã£ã¦ã¿ã‚ˆã†ã€‚$10^4$å€‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’$x\mapsto\sin(2\pi x)$ã®é–¢æ•°ã«ãƒã‚¤ã‚ºã‚’åŠ ãˆãŸã‚‚ã®ã§ç”Ÿæˆã™ã‚‹ã€‚

```python
n_data = 10**4
true_fn = lambda x: jnp.sin(2 * jnp.pi * x)
xs_data = random.uniform(random.PRNGKey(0), (n_data,))
ys_data = true_fn(xs_data) + random.normal(random.PRNGKey(1), (n_data,)) * 0.1
```

æ­£å‰‡åŒ–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯$\lambda=10^{-3}$ã€ã‚«ãƒ¼ãƒãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯$\sigma=0.5$ã¨ã™ã‚‹ã€‚
ã¾ãŸã€Random Fourier Featuresã®ç‰¹å¾´å†™åƒã¯$100$å€‹ã¨ã™ã‚‹ã€‚

```python
sigma = 0.5
alpha = 10**-3
n_feature = 100
xs_infer = jnp.arange(-0.1, 1.1, 0.01)
# rbf
rbf = RadialBasisFunction(sigma)
rbf_regression = KernelRidgeRegression(rbf, alpha)
rbf_regression.fit(xs_data, ys_data)
ys_infer_rbf = rbf_regression.predict(xs_infer)
# rff
rff = RandomFourierFeature(n_feature, sigma, 0)
rff_regression = KernelRidgeRegression(rff, alpha)
rff_regression.fit(xs_data, ys_data)
ys_infer_rff = rff_regression.predict(xs_infer)

# plot
plt.figure(figsize=(12, 6))
plt.rcParams["font.size"] = 20
plt.xlim(-0.1, 1.1)
plt.scatter(xs_data, ys_data, s=0.1, alpha=0.2)
plt.plot(xs_infer, true_fn(xs_infer), c="tab:blue", label="true", lw=1, ls="dashed")
plt.plot(xs_infer, ys_infer_rbf, c="tab:orange", label="RBF", lw=2)
plt.plot(xs_infer, ys_infer_rff, c="tab:green", label="RFF", lw=2)
plt.legend()
```

![](rbf_rff_regression.png)

ã„ãšã‚Œã®æ‰‹æ³•ã‚‚é–¢æ•°ã‚’å›å¸°ã§ãã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã§ããŸã€‚
æ¬¡ã«ã€ãƒ‡ãƒ¼ã‚¿æ•°ã«å¯¾ã™ã‚‹è¨ˆç®—æ™‚é–“ã®æ¯”è¼ƒã‚’è¡Œã£ã¦ã¿ã‚‹ã€‚

```python
import time
times_rbf, times_rff = [], []
for n_data in [10**2, 10**3, 10**4, 10**5]:
    xs_data = random.uniform(random.PRNGKey(0), (n_data,))
    ys_data = true_fn(xs_data) + random.normal(random.PRNGKey(1), (n_data,)) * 0.1
    # rbf
    start_rbf = time.perf_counter()
    rbf = RadialBasisFunction(sigma)
    rbf_regression = KernelRidgeRegression(rbf, alpha)
    rbf_regression.fit(xs_data, ys_data)
    ys_infer_rbf = rbf_regression.predict(xs_infer)
    end_rbf = time.perf_counter()
    times_rbf.append(end_rbf - start_rbf)
    # rff
    start_rff = time.perf_counter()
    rff = RandomFourierFeature(n_feature, sigma, 0)
    rff_regression = KernelRidgeRegression(rff, alpha)
    rff_regression.fit(xs_data, ys_data)
    ys_infer_rff = rff_regression.predict(xs_infer)
    end_rff = time.perf_counter()
    times_rff.append(end_rff - start_rff)
```

è¡¨ã«ã¾ã¨ã‚ã‚‹ã¨æ¬¡ã®ã‚ˆã†ã«ãªã‚‹ã€‚Random Fourier Featuresã®æ–¹ãŒè¨ˆç®—æ™‚é–“ãŒçŸ­ã„ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
ã“ã‚Œã¯ä¸€å›ã ã‘ã®è¨ˆæ¸¬æ™‚é–“ãªã®ã§æœ¬å½“ã¯è¤‡æ•°å›è¨ˆæ¸¬ã—ã¦å¹³å‡ã‚’å–ã£ãŸæ–¹ãŒè‰¯ã„ãŒã€ä»Šå›ã¯çœç•¥ã™ã‚‹ã€‚
ã‚ã¨ãƒ‡ãƒ¼ã‚¿æ•°ã‚’ã‚ˆã‚Šå¢—ã‚„ã—ã¦ç†è«–äºˆæ¸¬ã•ã‚Œã‚‹è¨ˆç®—é‡ã®ã‚¹ã‚±ãƒ¼ãƒ«ã«ä¸€è‡´ã™ã‚‹ã‹ã‚’æ¯”è¼ƒã™ã‚‹å¿…è¦ã‚‚ã‚ã‚‹ãŒã€ä»Šå›ã¯çœç•¥ã™ã‚‹ã€‚

| #data| RBF| RFF|
| ---: | --- | --- |
| $100$| 0.0351[s] | 0.0234[s] |
| $1000$| 0.0361[s] | 0.0059[s] |
| $10000$| 1.8547[s] | 0.0489[s] |
| $20000$| 11.0161[s] | 0.1733[s] |
| $30000$| 34.6534[s] | 0.4661[s] |


[^rff]: A. Rahimi, and B. Recht, "Random features for large-scale kernel machines." Advances in neural information processing systems 20 (2007). [[ğŸ“ PDF](https://proceedings.neurips.cc/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf)]

[^woodbury]: ã“ã®å¼è‡ªä½“ã®è¨¼æ˜ã¯Woodburyã®å…¬å¼ã‚’ç”¨ã„ã‚‹ã¾ã§ã‚‚ãªã$Z^{\top}(ZZ^{\top}+\lambda I_{n})=(Z^{\top}Z+\lambda I_{m})Z^{\top}$ã‹ã‚‰ã‚ã‹ã‚‹ã€‚